# TRAINER_SPEC.md

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**æ›´æ–°æ—¥**: 2025-10-21
**è²¬ä»»è€…**: core-team
**å‡¦ç†æ®µéš**: ç¬¬4æ®µéš: å­¦ç¿’

---

## ğŸ“‹ ç›®çš„

`src/trainer.py` ã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ»ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã®å®Ÿè£…ä»•æ§˜ã‚’å®šç¾©ã™ã‚‹ã€‚

---

## ğŸ¯ å…¥åŠ›å¥‘ç´„

```
features: Tensor[batch, time, channels]  # ãƒãƒ«ãƒTFçµ±åˆæ¸ˆã¿
time: æ­£è¦åŒ–å¾Œã®å›ºå®šé•·ï¼ˆä¾‹: N=64ï¼‰
aux_tf_streams: {M1, M5, M15, H1, H4} (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
```

---

## ğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### åŸºæœ¬æ§‹é€ ï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰

```
å…¥åŠ›: ãƒãƒ«ãƒTFç‰¹å¾´é‡ï¼ˆM1/M5/M15/H1/H4ï¼‰

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ M1 LSTM  â”‚â”€â”€â”€â”€â”  weight: 0.35 (ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¿ã‚¤ãƒŸãƒ³ã‚°)
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
    â”‚ M5 LSTM  â”‚â”€â”€â”€â”€â”¤  weight: 0.30 (çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰)
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
    â”‚ M15 LSTM â”‚â”€â”€â”€â”€â”¤â”€â”€â†’ Fusion (Attention) â”€â”€â”¬â†’ Direction Head
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚                          â”œâ†’ Magnitude_Scalp Head
    â”‚ H1 LSTM  â”‚â”€â”€â”€â”€â”¤  weight: 0.10 (å¤§å±€)     â”œâ†’ Magnitude_Swing Head
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚                          â””â†’ Trend_Strength Head
    â”‚ H4 LSTM  â”‚â”€â”€â”€â”€â”˜  weight: 0.05 (ãƒ¬ã‚¸ãƒ¼ãƒ )
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å‡ºåŠ›ï¼ˆãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ï¼‰:
- Direction: [UP, DOWN, NEUTRAL] ç¢ºç‡åˆ†å¸ƒ
- Magnitude_Scalp: çŸ­æœŸä¾¡æ ¼å¹… (0.5-2.0pipsæœªæº€, 1æ™‚é–“ä»¥å†…)
- Magnitude_Swing: å»¶é•·ä¾¡æ ¼å¹… (2.0pipsä»¥ä¸Š-5.0pips, ãƒˆãƒ¬ãƒ¼ãƒ«æ™‚)
- Trend_Strength: ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ (0-1, H1/H4ãƒ™ãƒ¼ã‚¹)

**ãƒ©ãƒ™ãƒ«å¢ƒç•Œæ¡ä»¶**:
```python
# ä¾¡æ ¼å¹…å®Ÿæ¸¬å€¤ã‹ã‚‰é©åˆ‡ãªãƒ©ãƒ™ãƒ«å‰²ã‚Šå½“ã¦
def assign_magnitude_label(actual_magnitude_pips: float, trend_strength: float) -> Dict[str, float]:
    """
    Scalp/Swingå¢ƒç•Œã§ã®ä¸€è²«ã—ãŸãƒ©ãƒ™ãƒ«å‰²ã‚Šå½“ã¦

    å¢ƒç•Œæ¡ä»¶:
    - actual_magnitude < 2.0 pips: Scalpãƒ©ãƒ™ãƒ«ã®ã¿ç”Ÿæˆã€Swingãƒ©ãƒ™ãƒ«ã¯ None
    - actual_magnitude >= 2.0 pips: Swingãƒ©ãƒ™ãƒ«ã®ã¿ç”Ÿæˆã€Scalpãƒ©ãƒ™ãƒ«ã¯ None
    - trend_strength ã¯å‚ç…§ã—ãªã„ï¼ˆå®Ÿæ¸¬å€¤ã®ã¿ã§åˆ¤å®šï¼‰

    Returns:
        {
            'magnitude_scalp': float or None,
            'magnitude_swing': float or None
        }
    """
    if actual_magnitude_pips < 2.0:
        return {
            'magnitude_scalp': actual_magnitude_pips,
            'magnitude_swing': None  # å­¦ç¿’æ™‚ã«ãƒã‚¹ã‚¯
        }
    else:  # >= 2.0 pips
        return {
            'magnitude_scalp': None,  # å­¦ç¿’æ™‚ã«ãƒã‚¹ã‚¯
            'magnitude_swing': actual_magnitude_pips
        }
```

**å­¦ç¿’æ™‚ã®ãƒã‚¹ã‚¯å‡¦ç†**:
```python
# None ãƒ©ãƒ™ãƒ«ã¯æå¤±è¨ˆç®—ã‹ã‚‰é™¤å¤–
def compute_masked_loss(pred, target, mask):
    """
    æœ‰åŠ¹ãªãƒ©ãƒ™ãƒ«ã®ã¿ã§æå¤±è¨ˆç®—

    Args:
        pred: äºˆæ¸¬å€¤ [batch_size]
        target: æ­£è§£å€¤ [batch_size]ï¼ˆNoneã¯0.0ã«å¤‰æ›æ¸ˆã¿ï¼‰
        mask: æœ‰åŠ¹ãƒ•ãƒ©ã‚° [batch_size]ï¼ˆFalse=é™¤å¤–ï¼‰
    """
    valid_pred = pred[mask]
    valid_target = target[mask]

    if len(valid_target) == 0:
        return torch.tensor(0.0)  # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ãªã—

    return F.huber_loss(valid_pred, valid_target, delta=0.3)

# ä½¿ç”¨ä¾‹
loss_scalp = compute_masked_loss(
    pred_magnitude_scalp,
    target_magnitude_scalp,
    mask_scalp  # actual_magnitude < 2.0 ã®ã‚µãƒ³ãƒ—ãƒ«
)

loss_swing = compute_masked_loss(
    pred_magnitude_swing,
    target_magnitude_swing,
    mask_swing  # actual_magnitude >= 2.0 ã®ã‚µãƒ³ãƒ—ãƒ«
)
```

**æ³¨è¨˜**:
- å¢ƒç•Œå€¤ `2.0 pips` ã¯Swingå´ã«å«ã‚ã‚‹ï¼ˆ`>=` åˆ¤å®šï¼‰
- ä¸¡ãƒ˜ãƒƒãƒ‰ã¯å¸¸ã«äºˆæ¸¬ã‚’å‡ºåŠ›ã™ã‚‹ãŒã€å­¦ç¿’æ™‚ã¯è©²å½“ç¯„å›²ã®ã¿ã§æå¤±è¨ˆç®—
- æ¨è«–æ™‚ã¯ `trend_strength` ã§ã©ã¡ã‚‰ã®äºˆæ¸¬ã‚’ä½¿ã†ã‹åˆ¤å®šï¼ˆå­¦ç¿’ã¯å®Ÿæ¸¬å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
```

### ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ãƒ­ã‚¸ãƒƒã‚¯

```python
# åŸºæœ¬ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆ70-80%ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼‰
if trend_strength < 0.7:
    mode = "scalp"
    primary_tf_weight = [0.35, 0.30, 0.20, 0.10, 0.05]  # M1å„ªå…ˆ
    target_magnitude = magnitude_scalp
    exit_strategy = "fixed_tp_sl"
    # å‹•çš„TP/SLï¼ˆDYNAMIC_EXIT_SPEC.mdå‚ç…§ï¼‰
    tp_base = 0.8 pips  # å›ºå®šå€ç‡ï¼ˆATRÃ—0.8ï¼‰ã¯å»ƒæ­¢
    sl_base = 0.5 pips

# æ‹¡å¼µãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆ20-30%ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼‰
else:  # trend_strength >= 0.7
    mode = "swing_trail"
    primary_tf_weight = [0.20, 0.20, 0.25, 0.20, 0.15]  # H1/H4é‡è¦–
    target_magnitude = magnitude_swing
    exit_strategy = "trailing_stop"
    trail_activation = +0.8 pips
    trail_distance = 0.3 pips
```

#### ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹

**ç›®çš„**: é–¾å€¤ï¼ˆ0.7ï¼‰ä»˜è¿‘ã§ã®é »ç¹ãªãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã«ã‚ˆã‚‹æ€§èƒ½åŠ£åŒ–ï¼ˆchurnï¼‰ã‚’æŠ‘åˆ¶ã™ã‚‹ã€‚

```python
class ModeStateMachine:
    """
    ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã®ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹å®Ÿè£…

    ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹å¹…ã«ã‚ˆã‚Šã€é »ç¹ãªåˆ‡æ›¿ã‚’æŠ‘åˆ¶
    """

    def __init__(
        self,
        enter_swing_threshold: float = 0.7,
        exit_swing_threshold: float = 0.6
    ):
        """
        Args:
            enter_swing_threshold: Swing ãƒ¢ãƒ¼ãƒ‰é€²å…¥é–¾å€¤
            exit_swing_threshold: Swing ãƒ¢ãƒ¼ãƒ‰é€€å‡ºé–¾å€¤
        """
        self.enter_swing_thr = enter_swing_threshold
        self.exit_swing_thr = exit_swing_threshold
        self.current_mode = "scalp"  # åˆæœŸãƒ¢ãƒ¼ãƒ‰

    def update(self, trend_strength: float) -> str:
        """
        ãƒ¢ãƒ¼ãƒ‰æ›´æ–°ï¼ˆãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹é©ç”¨ï¼‰

        Args:
            trend_strength: ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ [0, 1]

        Returns:
            str: 'scalp' ã¾ãŸã¯ 'swing_trail'
        """
        if self.current_mode == "scalp":
            # Scalp â†’ Swing é·ç§»: é«˜ã„é–¾å€¤
            if trend_strength >= self.enter_swing_thr:
                self.current_mode = "swing_trail"
                logger.info(f"Mode switch: scalp â†’ swing (strength={trend_strength:.3f})")

        elif self.current_mode == "swing_trail":
            # Swing â†’ Scalp é·ç§»: ä½ã„é–¾å€¤
            if trend_strength < self.exit_swing_thr:
                self.current_mode = "scalp"
                logger.info(f"Mode switch: swing â†’ scalp (strength={trend_strength:.3f})")

        return self.current_mode

    def get_mode_config(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’å–å¾—"""
        if self.current_mode == "scalp":
            return {
                'mode': 'scalp',
                'tf_weights': [0.35, 0.30, 0.20, 0.10, 0.05],
                'magnitude_head': 'magnitude_scalp',
                'exit_strategy': 'fixed_tp_sl'
            }
        else:  # swing_trail
            return {
                'mode': 'swing_trail',
                'tf_weights': [0.20, 0.20, 0.25, 0.20, 0.15],
                'magnitude_head': 'magnitude_swing',
                'exit_strategy': 'trailing_stop'
            }


# ä½¿ç”¨ä¾‹
mode_fsm = ModeStateMachine(
    enter_swing_threshold=0.7,
    exit_swing_threshold=0.6
)

# æ¨è«–æ™‚
for signal in signals:
    trend_strength = model.predict(signal)['trend_strength']

    # ãƒ¢ãƒ¼ãƒ‰æ›´æ–°ï¼ˆãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹é©ç”¨ï¼‰
    current_mode = mode_fsm.update(trend_strength)
    config = mode_fsm.get_mode_config()

    # ãƒ¢ãƒ¼ãƒ‰åˆ¥å‡¦ç†
    if current_mode == "scalp":
        magnitude = model.predict(signal)['magnitude_scalp']
    else:
        magnitude = model.predict(signal)['magnitude_swing']
```

**ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹å¹…ã®é¸æŠ**:
```python
# æ¨å¥¨è¨­å®š
default_config = {
    'enter_swing_threshold': 0.7,   # Swingé€²å…¥
    'exit_swing_threshold': 0.6,    # Swingé€€å‡º
    'hysteresis_width': 0.1         # å¹… = 0.7 - 0.6
}

# èª¿æ•´æŒ‡é‡
# - å¹…ãŒå¤§ãã„ï¼ˆä¾‹: 0.15ï¼‰: åˆ‡æ›¿é »åº¦â†“ã€ãƒ¢ãƒ¼ãƒ‰å›ºå®šå‚¾å‘â†‘
# - å¹…ãŒå°ã•ã„ï¼ˆä¾‹: 0.05ï¼‰: åˆ‡æ›¿é »åº¦â†‘ã€å¸‚å ´è¿½å¾“æ€§â†‘
```

**æˆåŠŸæŒ‡æ¨™**:
- ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿é »åº¦: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”ã§30%æ¸›å°‘
- åˆ‡æ›¿æ™‚ã®æœŸå¾…å€¤åŠ£åŒ– < 5%
- ä¸è¦ãªåˆ‡æ›¿ï¼ˆ10ãƒãƒ¼ä»¥å†…ã®å¾€å¾©ï¼‰< 3%

**æ¤œè¨¼**:
```python
def test_mode_hysteresis():
    """ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ã®å‹•ä½œæ¤œè¨¼"""
    fsm = ModeStateMachine(enter_swing_thr=0.7, exit_swing_thr=0.6)

    # Scalp â†’ Swing é·ç§»
    assert fsm.update(0.65) == "scalp"  # ã¾ã Scalp
    assert fsm.update(0.69) == "scalp"  # ã¾ã Scalp
    assert fsm.update(0.71) == "swing_trail"  # é·ç§»

    # Swing â†’ Scalp é·ç§»ï¼ˆä½ã„é–¾å€¤ï¼‰
    assert fsm.update(0.65) == "swing_trail"  # ã¾ã Swing
    assert fsm.update(0.61) == "swing_trail"  # ã¾ã Swing
    assert fsm.update(0.59) == "scalp"  # é·ç§»

    # ãƒãƒ£ã‚¿ãƒªãƒ³ã‚°é˜²æ­¢ç¢ºèª
    # 0.65ä»˜è¿‘ã®å¾€å¾©ã§ã‚‚åˆ‡æ›¿ã—ãªã„
    fsm = ModeStateMachine(enter_swing_thr=0.7, exit_swing_thr=0.6)
    modes = [fsm.update(s) for s in [0.65, 0.66, 0.65, 0.64, 0.65]]
    assert all(m == "scalp" for m in modes), "ä¸è¦ãªåˆ‡æ›¿ç™ºç”Ÿ"
```

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³ç´°

#### 1. TFã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
- å„TFç”¨LSTMï¼ˆhidden=128ï¼‰
- æœ€çµ‚éš ã‚ŒçŠ¶æ…‹ã‚’æ™‚ç³»åˆ—æ‹¡å¼µï¼ˆé•·ã•Næƒãˆï¼‰

#### 2. Fusionå±¤
- Attentionæ©Ÿæ§‹ã§TFé–“ã‚’çµ±åˆ
- Query: æœ€æ–°Kæœ¬ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«
- Keys/Values: å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—

**è©³ç´°**: [MULTI_TF_FUSION_SPEC.md](trainer/MULTI_TF_FUSION_SPEC.md) ã‚’å‚ç…§
- å„TFã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·å®šç¾©ï¼ˆæœ€å¤§å…¬ç´„æ•°çš„å›ºå®šçª“ï¼‰
- LSTM + Self-Attentionè¨­è¨ˆï¼ˆå‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºï¼‰
- Cross-TF Attentionå®Ÿè£…è©³ç´°

#### 3. å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
- **Direction Head**: 3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆUP/DOWN/NEUTRALï¼‰
- **Magnitude_Scalp Head**: å›å¸°ï¼ˆ0.5-2.0 pipsã€å›ºå®šTP/SLç”¨ï¼‰
- **Magnitude_Swing Head**: å›å¸°ï¼ˆ2.0-5.0 pipsã€ãƒˆãƒ¬ãƒ¼ãƒ«å»¶é•·ç”¨ï¼‰
- **Trend_Strength Head**: å›å¸°ï¼ˆ0-1ã€H1/H4ãƒˆãƒ¬ãƒ³ãƒ‰æŒç¶šæ€§ï¼‰

---

## ğŸ“Š ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯æå¤±é–¢æ•°ï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰

```python
L_total = Î± Ã— L_direction + Î² Ã— L_magnitude_scalp + Î³ Ã— L_magnitude_swing + Î´ Ã— L_trend_strength

# Direction: CrossEntropy
L_direction = CE(pred_direction, true_direction)

# Magnitude_Scalp: Huber Lossï¼ˆå¤–ã‚Œå€¤è€æ€§ã€0.5-2.0 pipsï¼‰
L_magnitude_scalp = HuberLoss(pred_scalp, true_scalp, delta=0.3)

# Magnitude_Swing: Huber Lossï¼ˆ2.0-5.0 pipsï¼‰
L_magnitude_swing = HuberLoss(pred_swing, true_swing, delta=0.5)

# Trend_Strength: MSEï¼ˆ0-1ï¼‰
L_trend_strength = MSE(pred_trend, true_trend)

# é‡ã¿ï¼ˆåˆæœŸå€¤ï¼‰
Î± = 0.40  # æ–¹å‘ï¼ˆæœ€é‡è¦ï¼‰
Î² = 0.35  # ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ä¾¡æ ¼å¹…
Î³ = 0.15  # ã‚¹ã‚¤ãƒ³ã‚°å»¶é•·ä¾¡æ ¼å¹…
Î´ = 0.10  # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
```

### å‹•çš„é‡ã¿èª¿æ•´

```python
# ãƒãƒƒãƒå†…ã®trend_strengthã«å¿œã˜ã¦å‹•çš„èª¿æ•´
def compute_loss_weights(trend_strength_batch):
    scalp_ratio = (trend_strength_batch < 0.7).float().mean()
    swing_ratio = 1.0 - scalp_ratio

    # Î², Î³ã‚’å‹•çš„èª¿æ•´ï¼ˆÎ±, Î´ã¯å›ºå®šï¼‰
    Î²_dynamic = 0.35 * (scalp_ratio / 0.75)  # åŸºæº–75%ã§æ­£è¦åŒ–
    Î³_dynamic = 0.15 * (swing_ratio / 0.25)  # åŸºæº–25%ã§æ­£è¦åŒ–

    return {
        'alpha': 0.40,
        'beta': Î²_dynamic,
        'gamma': Î³_dynamic,
        'delta': 0.10
    }
```

#### å‹•çš„æå¤±é‡ã¿ã®æ­£è¦åŒ–

**ç›®çš„**: é‡ã¿åˆè¨ˆã®è†¨å¼µã«ã‚ˆã‚‹optimizerå®ŸåŠ¹å­¦ç¿’ç‡ã®å¤‰èª¿ã¨åæŸéœ‡ãˆï¼ˆoscillationï¼‰ã‚’é˜²æ­¢ã™ã‚‹ã€‚

**å®Ÿè£…**: Î£w=1 æ­£è¦åŒ– + å€‹åˆ¥clamp [0.02, 3.0] + driftæ¤œå‡º

```python
def normalize_loss_weights(weights: Dict[str, float]) -> Dict[str, float]:
    """
    æå¤±é‡ã¿ã®æ­£è¦åŒ–ã¨åˆ¶ç´„

    Args:
        weights: {'alpha': 0.4, 'beta': 0.35, 'gamma': 0.15, 'delta': 0.1}

    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸé‡ã¿ï¼ˆåˆè¨ˆ=1.0ã€å€‹åˆ¥clampé©ç”¨æ¸ˆã¿ï¼‰
    """
    # å€‹åˆ¥clamp [0.02, 3.0]
    clamped = {
        key: np.clip(val, 0.02, 3.0)
        for key, val in weights.items()
    }

    # ç·å’Œæ­£è¦åŒ–ï¼ˆÎ£w = 1.0ï¼‰
    total = sum(clamped.values())
    normalized = {
        key: val / total
        for key, val in clamped.items()
    }

    return normalized


class LossWeightDriftDetector:
    """é‡ã¿å¤‰å‹•ã®ç›£è¦–"""

    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.history = {
            'alpha': [],
            'beta': [],
            'gamma': [],
            'delta': []
        }

    def update(self, weights: Dict[str, float]):
        """é‡ã¿å±¥æ­´ã‚’æ›´æ–°"""
        for key, val in weights.items():
            self.history[key].append(val)
            if len(self.history[key]) > self.window_size:
                self.history[key].pop(0)

    def check_drift(self, cv_threshold: float = 0.25) -> Dict[str, bool]:
        """
        å¤‰å‹•ä¿‚æ•°ï¼ˆCVï¼‰ã«ã‚ˆã‚‹ drift æ¤œå‡º

        Args:
            cv_threshold: CVé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.25ï¼‰

        Returns:
            å„é‡ã¿ã®driftæœ‰ç„¡ï¼ˆTrue=è­¦å‘Šï¼‰
        """
        drift_flags = {}

        for key, values in self.history.items():
            if len(values) < 10:  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
                drift_flags[key] = False
                continue

            mean = np.mean(values)
            std = np.std(values)
            cv = std / mean if mean > 0 else 0

            drift_flags[key] = cv > cv_threshold

        return drift_flags


# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ä½¿ç”¨
drift_detector = LossWeightDriftDetector(window_size=100)

for epoch in range(num_epochs):
    for batch in dataloader:
        # å‹•çš„é‡ã¿è¨ˆç®—
        raw_weights = compute_loss_weights(batch['trend_strength'])

        # æ­£è¦åŒ– + clamp
        weights = normalize_loss_weights(raw_weights)

        # æå¤±è¨ˆç®—
        loss_total = (
            weights['alpha'] * loss_direction +
            weights['beta'] * loss_magnitude_scalp +
            weights['gamma'] * loss_magnitude_swing +
            weights['delta'] * loss_trend_strength
        )

        # Driftç›£è¦–
        drift_detector.update(weights)
        drift_flags = drift_detector.check_drift(cv_threshold=0.25)

        if any(drift_flags.values()):
            logger.warning(f"Loss weight drift detected: {drift_flags}")

        # é€†ä¼æ’­
        optimizer.zero_grad()
        loss_total.backward()
        optimizer.step()
```

**æˆåŠŸæŒ‡æ¨™**:
- ä¸»è¦headå‹¾é…ãƒãƒ«ãƒ å®‰å®šæ€§: CV < 0.2
- é‡ã¿ç·å’Œä¸å¤‰: Î£w = 1.0 Â± 1e-6
- Driftè­¦å‘Šé »åº¦ < 5% of batches

**æ¤œè¨¼**:
```python
def test_weight_normalization():
    """é‡ã¿æ­£è¦åŒ–ã®æ¤œè¨¼"""
    raw_weights = {'alpha': 0.5, 'beta': 4.0, 'gamma': 0.01, 'delta': 0.2}

    normalized = normalize_loss_weights(raw_weights)

    # åˆè¨ˆ=1.0
    assert abs(sum(normalized.values()) - 1.0) < 1e-6

    # å€‹åˆ¥clampç¢ºèª
    assert all(0.02 <= v <= 3.0 for v in normalized.values())
```

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥

æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸåˆ†å‰²æ–¹æ³•ã‚’å®šç¾©ã€‚

### åŸºæœ¬æ–¹é‡

**æ™‚ç³»åˆ—é †åºã‚’å³å®ˆ**: æœªæ¥ãƒ‡ãƒ¼ã‚¿ã®ãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ã€Train/Val/Testã¯æ™‚ç³»åˆ—é †ã«åˆ†å‰²ã€‚

```
æ™‚ç³»åˆ—é †åº:
â”œâ”€ Train   (60%): 2024-01-01 ~ 2024-07-10
â”œâ”€ Val     (20%): 2024-07-11 ~ 2024-09-05
â””â”€ Test    (20%): 2024-09-06 ~ 2024-10-31
```

### åˆ†å‰²æ¯”ç‡

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | æ¯”ç‡ | æœŸé–“ï¼ˆä¾‹ï¼‰ | ç”¨é€” |
|-------------|-----|-----------|------|
| Train | 60% | ç´„6ãƒ¶æœˆ | ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° |
| Validation | 20% | ç´„2ãƒ¶æœˆ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãƒ»æ—©æœŸåœæ­¢åˆ¤å®š |
| Test | 20% | ç´„2ãƒ¶æœˆ | æœ€çµ‚æ€§èƒ½è©•ä¾¡ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ |

### å®Ÿè£…è©³ç´°

```python
def split_timeseries_data(data_h5_path: str) -> Dict[str, np.ndarray]:
    """
    æ™‚ç³»åˆ—é †åºã‚’ä¿æŒã—ãŸãƒ‡ãƒ¼ã‚¿åˆ†å‰²

    Args:
        data_h5_path: å‰å‡¦ç†æ¸ˆã¿HDF5ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        {'train': (N_train, seq, feat),
         'val': (N_val, seq, feat),
         'test': (N_test, seq, feat)}
    """
    with h5py.File(data_h5_path, 'r') as f:
        sequences = f['sequences'][:]
        timestamps = f['timestamps'][:]

    # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿æƒ³å®šï¼ˆå‰å‡¦ç†æ®µéšã§ä¿è¨¼ï¼‰
    assert np.all(timestamps[:-1] <= timestamps[1:]), "æ™‚ç³»åˆ—é †åºé•å"

    N = len(sequences)
    train_end = int(N * 0.60)
    val_end = int(N * 0.80)

    return {
        'train': sequences[:train_end],
        'val': sequences[train_end:val_end],
        'test': sequences[val_end:]
    }
```

### ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†å‰²ï¼ˆPhase 3å®Ÿè£…äºˆå®šï¼‰

å®šæœŸçš„ãªå†å­¦ç¿’ã®ãŸã‚ã€ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ–¹å¼ã‚’æ¡ç”¨ï¼š

```
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦1: Train[1-6æœˆ] â†’ Val[7-8æœˆ] â†’ Test[9-10æœˆ]
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦2: Train[2-7æœˆ] â†’ Val[8-9æœˆ] â†’ Test[10-11æœˆ]
ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦3: Train[3-8æœˆ] â†’ Val[9-10æœˆ] â†’ Test[11-12æœˆ]
```

#### Walk-forwardè©•ä¾¡ã¨é–¾å€¤æœ€é©åŒ–

**ç›®çš„**: å˜ä¸€æœŸé–“æœ€é©åŒ–ã«ã‚ˆã‚‹é‹ç”¨æœŸå¾…å€¤åŠ£åŒ–ã¨é–¾å€¤éå­¦ç¿’ã‚’é˜²æ­¢ã™ã‚‹ã€‚

**å®Ÿè£…**: Walk-forward Cross-Validation (k=6 folds, overlap=0.5) + Temporal CV

```python
def walk_forward_validation(
    data_h5_path: str,
    k_folds: int = 6,
    overlap: float = 0.5,
    optimize_thresholds: bool = True
) -> Dict[str, Any]:
    """
    Walk-forwardè©•ä¾¡ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

    Args:
        data_h5_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        k_folds: åˆ†å‰²æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 6ï¼‰
        overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ¯”ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5 = 50%ï¼‰
        optimize_thresholds: é–¾å€¤æœ€é©åŒ–ã‚’å®Ÿæ–½ã™ã‚‹ã‹

    Returns:
        {'scores': [...], 'best_thresholds': {...}, 'frontiers': [...]}
    """
    with h5py.File(data_h5_path, 'r') as f:
        sequences = f['sequences'][:]
        timestamps = f['timestamps'][:]

    N = len(sequences)
    fold_size = N // k_folds
    overlap_size = int(fold_size * overlap)

    results = {
        'val_scores': [],
        'train_scores': [],
        'best_thresholds': [],
        'frontier_models': []
    }

    for i in range(k_folds):
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å®šç¾©
        train_start = max(0, i * fold_size - overlap_size)
        train_end = (i + 1) * fold_size
        val_start = train_end
        val_end = min(N, val_start + fold_size // 2)

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        train_data = sequences[train_start:train_end]
        val_data = sequences[val_start:val_end]

        logger.info(f"Fold {i+1}/{k_folds}: Train[{train_start}:{train_end}], Val[{val_start}:{val_end}]")

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = train_model(train_data)

        # Trainæ€§èƒ½è©•ä¾¡
        train_score = evaluate_model(model, train_data)
        results['train_scores'].append(train_score)

        # é–¾å€¤æœ€é©åŒ–ï¼ˆValidationä¸Šã§ï¼‰
        if optimize_thresholds:
            best_thr = optimize_decision_thresholds(
                model,
                val_data,
                search_space={
                    'confidence_threshold': (0.5, 0.9),
                    'neutral_k_atr': (0.2, 0.5),
                    'mode_switch_threshold': (0.6, 0.8)
                },
                method='grid_search',  # or 'bayesian'
                cv_splits=3  # å†…éƒ¨temporal CV
            )
            results['best_thresholds'].append(best_thr)

        # Validationæ€§èƒ½è©•ä¾¡
        val_score = evaluate_model(model, val_data, thresholds=best_thr)
        results['val_scores'].append(val_score)

        # Frontierãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆéå­¦ç¿’æ¤œå‡ºç”¨ï¼‰
        results['frontier_models'].append({
            'fold': i,
            'timestamp_range': (timestamps[train_start], timestamps[val_end]),
            'model_path': f'models/wf_fold{i}.pt',
            'train_score': train_score,
            'val_score': val_score,
            'thresholds': best_thr
        })

    # KPIè¨ˆç®—
    mean_val_score = np.mean(results['val_scores'])
    mean_train_score = np.mean(results['train_scores'])
    val_train_ratio = mean_val_score / mean_train_score

    # éå­¦ç¿’ãƒã‚§ãƒƒã‚¯
    if val_train_ratio < 0.8:
        logger.warning(f"éå­¦ç¿’æ¤œå‡º: val/train = {val_train_ratio:.3f} < 0.8")

    results['summary'] = {
        'mean_val_score': mean_val_score,
        'mean_train_score': mean_train_score,
        'val_train_ratio': val_train_ratio,
        'fold_consistency': np.std(results['val_scores']) / mean_val_score  # CV
    }

    return results


def optimize_decision_thresholds(
    model,
    val_data,
    search_space: Dict[str, Tuple[float, float]],
    method: str = 'grid_search',
    cv_splits: int = 3
) -> Dict[str, float]:
    """
    é–¾å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–ï¼ˆéå­¦ç¿’å¯¾ç­–ä»˜ãï¼‰

    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        val_data: Validationãƒ‡ãƒ¼ã‚¿
        search_space: æ¢ç´¢ç©ºé–“ {'param_name': (min, max)}
        method: 'grid_search' | 'bayesian' | 'random'
        cv_splits: Temporal CVåˆ†å‰²æ•°

    Returns:
        æœ€é©é–¾å€¤ã‚»ãƒƒãƒˆ
    """
    # Validation ã‚’æ›´ã«cv_splitsåˆ†å‰²ï¼ˆtemporalï¼‰
    N = len(val_data)
    fold_size = N // cv_splits

    best_score = -np.inf
    best_thresholds = None

    # æ¢ç´¢ç©ºé–“ã®ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
    if method == 'grid_search':
        grid = generate_threshold_grid(search_space, n_points=10)
    elif method == 'bayesian':
        # Bayesian Optimization
        from skopt import gp_minimize
        # ...çœç•¥...
        return {}
    else:
        # Random search
        grid = generate_random_threshold_samples(search_space, n_samples=50)

    for threshold_set in grid:
        # Temporal CVè©•ä¾¡
        cv_scores = []

        for split_i in range(cv_splits):
            split_start = split_i * fold_size
            split_end = (split_i + 1) * fold_size
            split_data = val_data[split_start:split_end]

            # é–¾å€¤é©ç”¨ã—ã¦è©•ä¾¡
            score = evaluate_with_thresholds(model, split_data, threshold_set)
            cv_scores.append(score)

        # å¹³å‡ã‚¹ã‚³ã‚¢
        mean_score = np.mean(cv_scores)
        score_std = np.std(cv_scores)

        # å®‰å®šæ€§ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆCVå¤§ãã™ãã‚‹å ´åˆï¼‰
        if score_std / mean_score > 0.3:  # å¤‰å‹•ä¿‚æ•° > 30%
            mean_score *= 0.9  # ãƒšãƒŠãƒ«ãƒ†ã‚£

        if mean_score > best_score:
            best_score = mean_score
            best_thresholds = threshold_set

    logger.info(f"æœ€é©é–¾å€¤: {best_thresholds}, ã‚¹ã‚³ã‚¢: {best_score:.4f}")

    return best_thresholds


def generate_threshold_grid(search_space, n_points=10):
    """é–¾å€¤ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ"""
    from itertools import product

    param_ranges = {}
    for param, (min_val, max_val) in search_space.items():
        param_ranges[param] = np.linspace(min_val, max_val, n_points)

    # å…¨çµ„ã¿åˆã‚ã›
    keys = list(param_ranges.keys())
    values = list(param_ranges.values())

    grid = []
    for combo in product(*values):
        grid.append(dict(zip(keys, combo)))

    return grid
```

**æˆåŠŸæŒ‡æ¨™**:
- **Val/Trainæ¯” â‰¥ 0.8**: éå­¦ç¿’é˜²æ­¢
- **Foldä¸€è²«æ€§**: CV(val_scores) < 0.2ï¼ˆå®‰å®šæ€§ï¼‰
- **é–¾å€¤å†ç¾æ€§**: é€£ç¶šfoldé–“ã§ã®é–¾å€¤å¤‰å‹• < 20%

**æ¤œè¨¼**:
```python
def test_walk_forward():
    """Walk-forwardè©•ä¾¡ã®æ¤œè¨¼"""
    results = walk_forward_validation(
        'data/preprocessed.h5',
        k_folds=6,
        overlap=0.5
    )

    # 1. Val/Trainæ¯”ãƒã‚§ãƒƒã‚¯
    assert results['summary']['val_train_ratio'] >= 0.8, "éå­¦ç¿’æ¤œå‡º"

    # 2. Foldä¸€è²«æ€§
    cv = results['summary']['fold_consistency']
    assert cv < 0.2, f"Foldé–“ã°ã‚‰ã¤ãå¤§: CV={cv:.3f}"

    # 3. é–¾å€¤å®‰å®šæ€§
    thresholds = results['best_thresholds']
    for param in thresholds[0].keys():
        values = [t[param] for t in thresholds]
        relative_std = np.std(values) / np.mean(values)
        assert relative_std < 0.2, f"{param}ã®é–¾å€¤ãŒä¸å®‰å®š"
```

### æ³¨æ„äº‹é …

- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ç¦æ­¢**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚`shuffle=False`ã‚’å³å®ˆ
- **ãƒªãƒ¼ã‚¯é˜²æ­¢**: Validation/Testãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’æ™‚ã«ä¸€åˆ‡å‚ç…§ã—ãªã„
- **çµ±è¨ˆé‡è¨ˆç®—**: æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆmean/stdï¼‰ã¯Trainãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰ç®—å‡º
- **ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ä½¿ç”¨ã—ãªã„ï¼ˆTime Series Splitã®ã¿è¨±å¯ï¼‰

---

## ğŸ·ï¸ ãƒ©ãƒ™ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰

### æ–¹å‘ãƒ©ãƒ™ãƒ«

#### NEUTRALé–¾å€¤ã®å®šç¾©ï¼ˆPhaseåˆ¥å®Ÿè£…ï¼‰

```python
# å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º1: åŸºæœ¬å®Ÿè£…ï¼ˆspread + ATRãƒ™ãƒ¼ã‚¹ï¼‰
Î¸_neutral = max(
    spread Ã— k_spread,      # 1.0ï¼ˆã‚³ã‚¹ãƒˆè€ƒæ…®ï¼‰
    ATR_short Ã— k_atr       # 0.3ï¼ˆçŸ­æœŸãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
)

# å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º2: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ è¿½åŠ ï¼ˆå°†æ¥æ‹¡å¼µï¼‰
Î¸_neutral = max(
    spread Ã— k_spread,
    ATR_short Ã— k_atr,
    ATR_ratio Ã— k_ratio     # ATR_short / ATR_longï¼ˆãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼‰
)

# åˆ†é¡
if Î”P > Î¸_neutral: label = UP
elif Î”P < -Î¸_neutral: label = DOWN
else: label = NEUTRAL
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°

```python
# Phase 1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå³å®Ÿè£…å¯èƒ½ï¼‰
params = {
    "k_spread": 1.0,        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰å€ç‡ï¼ˆã‚³ã‚¹ãƒˆå›åå¿…é ˆï¼‰
    "k_atr": 0.3,           # ATRå€ç‡ï¼ˆãƒã‚¤ã‚ºé™¤å¤–ï¼‰
    "atr_period": 14,       # ATRè¨ˆç®—æœŸé–“ï¼ˆM5åŸºæº–ï¼‰
    "spread_default": 1.2   # pipsï¼ˆå‹•çš„å–å¾—å¤±æ•—æ™‚ï¼‰
}

# è¨ˆç®—ä¾‹ï¼ˆEURUSDã€é€šå¸¸æ™‚ï¼‰
spread = 1.2 pips
ATR_short = 8.0 pips (M5, 14æœŸé–“)

Î¸_neutral = max(
    1.2 Ã— 1.0 = 1.2 pips,
    8.0 Ã— 0.3 = 2.4 pips
) = 2.4 pips

# çµæœ: Â±2.4 pipsä»¥å†…ã®å¤‰å‹•ã¯NEUTRAL
```

#### ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œ

```python
# ãƒ¢ãƒ¼ãƒ‰åˆ¥ã«NEUTRALé–¾å€¤ã‚’èª¿æ•´

# Scalp Modeï¼ˆçŸ­æœŸãƒ»é«˜é »åº¦ï¼‰
Î¸_neutral_scalp = max(
    spread Ã— 1.2,           # ã‚³ã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ³å¢—åŠ 
    ATR_M1 Ã— 0.25           # M1ãƒ™ãƒ¼ã‚¹ã®çŸ­æœŸãƒœãƒ©
)

# Swing Modeï¼ˆé•·æœŸãƒ»ä½é »åº¦ï¼‰
Î¸_neutral_swing = max(
    spread Ã— 0.8,           # ã‚³ã‚¹ãƒˆæ„Ÿåº¦ä½ä¸‹
    ATR_H1 Ã— 0.4            # H1ãƒ™ãƒ¼ã‚¹ã®é•·æœŸãƒœãƒ©
)

# ä½¿ç”¨ä¾‹
if trend_strength < 0.7:
    threshold = Î¸_neutral_scalp
else:
    threshold = Î¸_neutral_swing
```

#### Phase 2æ‹¡å¼µ: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆå°†æ¥ï¼‰

```python
# ATRæ¯”ç‡ã«ã‚ˆã‚‹ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š
def calculate_volatility_regime(
    atr_short: float,   # M5 ATR(14)
    atr_long: float     # H1 ATR(14)
) -> str:
    """
    Returns: 'low', 'normal', 'high'
    """
    ratio = atr_short / atr_long
    
    if ratio < 0.6:
        return "low"        # çŸ­æœŸãƒœãƒ©ä½ä¸‹ï¼ˆãƒ¬ãƒ³ã‚¸ï¼‰
    elif ratio > 1.4:
        return "high"       # çŸ­æœŸãƒœãƒ©æ€¥å¢—ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆï¼‰
    else:
        return "normal"

# ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥é–¾å€¤èª¿æ•´
regime_multiplier = {
    "low": 0.8,      # ãƒ¬ãƒ³ã‚¸ç›¸å ´ï¼šé–¾å€¤ã‚’ä¸‹ã’ã¦åå¿œå‘ä¸Š
    "normal": 1.0,
    "high": 1.3      # é«˜ãƒœãƒ©ç›¸å ´ï¼šé–¾å€¤ã‚’ä¸Šã’ã¦ãƒã‚¤ã‚ºé™¤å¤–
}

Î¸_neutral_phase2 = max(
    spread Ã— k_spread,
    ATR_short Ã— k_atr
) Ã— regime_multiplier[regime]
```

### ä¾¡æ ¼å¹…ãƒ©ãƒ™ãƒ«ï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«ï¼‰

```python
# Scalp Modeç”¨ï¼ˆ70-80%ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼‰
# æ¡ä»¶: 1æ™‚é–“ä»¥å†…ã«ã‚¯ãƒ­ãƒ¼ã‚ºã€trend_strength < 0.7
y_scalp = abs(exit_price - entry_price) Ã— 10  # pips
  ç¯„å›²: 0.5-2.0 pips
  TP/SL: å›ºå®šï¼ˆATR Ã— 0.8 / 0.5ï¼‰

# Swing Extensionç”¨ï¼ˆ20-30%ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼‰
# æ¡ä»¶: trend_strength >= 0.7ã€ãƒˆãƒ¬ãƒ¼ãƒ«ã§å»¶é•·
y_swing = abs(final_exit - entry_price) Ã— 10  # pips
  ç¯„å›²: 2.0-5.0 pips
  æ±ºæ¸ˆ: ãƒˆãƒ¬ãƒ¼ãƒªãƒ³ã‚°ã‚¹ãƒˆãƒƒãƒ—ï¼ˆ+0.8 pipsèµ·å‹•ã€0.3 pipså¹…ï¼‰
```

#### ä¾¡æ ¼å¹…ãƒ©ãƒ™ãƒ«ã®ãƒ­ãƒã‚¹ãƒˆåŒ–

**ç›®çš„**: æ¥µç«¯ãªä¾¡æ ¼å¹…ã«ã‚ˆã‚‹æå¤±ä¸»å°ã¨éå­¦ç¿’ã‚’é˜²æ­¢ã™ã‚‹ã€‚

**å®Ÿè£…**: Quantile Clipping (p1, p99) + Adaptive Huber Î´

```python
def compute_robust_magnitude_label(
    magnitude_raw: float,
    mode: str,  # 'scalp' or 'swing'
    historical_magnitudes: np.ndarray,
    quantile_range: Tuple[float, float] = (0.01, 0.99)
) -> float:
    """
    ãƒ­ãƒã‚¹ãƒˆãªä¾¡æ ¼å¹…ãƒ©ãƒ™ãƒ«è¨ˆç®—

    Args:
        magnitude_raw: ç”Ÿã®ä¾¡æ ¼å¹…ï¼ˆpipsï¼‰
        mode: 'scalp' ã¾ãŸã¯ 'swing'
        historical_magnitudes: éå»ã®ä¾¡æ ¼å¹…åˆ†å¸ƒï¼ˆåŒä¸€ãƒ¢ãƒ¼ãƒ‰ï¼‰
        quantile_range: ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: p1, p99ï¼‰

    Returns:
        float: ãƒ­ãƒã‚¹ãƒˆåŒ–ã•ã‚ŒãŸä¾¡æ ¼å¹…
    """
    # åˆ†ä½æ•°ã«ã‚ˆã‚‹ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    p_low, p_high = quantile_range
    q_low = np.quantile(historical_magnitudes, p_low)
    q_high = np.quantile(historical_magnitudes, p_high)

    magnitude_clipped = np.clip(magnitude_raw, q_low, q_high)

    return magnitude_clipped


def compute_adaptive_huber_delta(
    magnitudes: np.ndarray,
    mode: str
) -> float:
    """
    Huber Loss ã® Î´ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©å¿œçš„ã«è¨ˆç®—

    Args:
        magnitudes: ä¾¡æ ¼å¹…åˆ†å¸ƒ
        mode: 'scalp' ã¾ãŸã¯ 'swing'

    Returns:
        float: é©å¿œçš„ãª Î´ å€¤
    """
    # MAD (Median Absolute Deviation) ãƒ™ãƒ¼ã‚¹ã® Î´
    median = np.median(magnitudes)
    mad = np.median(np.abs(magnitudes - median))

    # ãƒ¢ãƒ¼ãƒ‰åˆ¥ä¿‚æ•°
    if mode == 'scalp':
        delta = 1.0 * mad  # ã‚ˆã‚Šæ•æ„Ÿ
    elif mode == 'swing':
        delta = 1.5 * mad  # ã‚ˆã‚Šç·©ã‚„ã‹
    else:
        raise ValueError(f"Unknown mode: {mode}")

    # ä¸‹é™åˆ¶ç´„
    delta = max(delta, 0.1)  # æœ€å° 0.1 pips

    return delta


# å­¦ç¿’æ™‚ã®é©ç”¨ä¾‹
class MagnitudeLoss(nn.Module):
    def __init__(self, mode='scalp'):
        super().__init__()
        self.mode = mode
        self.delta = None  # å‹•çš„ã«æ›´æ–°

    def forward(self, pred, target, historical_targets):
        # Î´ã®å‹•çš„æ›´æ–°ï¼ˆã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰
        if self.delta is None or self.training:
            self.delta = compute_adaptive_huber_delta(
                historical_targets.cpu().numpy(),
                self.mode
            )

        # Huber Loss with adaptive Î´
        loss = F.huber_loss(pred, target, delta=self.delta)

        return loss


# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ä½¿ç”¨
scalp_loss_fn = MagnitudeLoss(mode='scalp')
swing_loss_fn = MagnitudeLoss(mode='swing')

# ãƒãƒƒãƒã”ã¨ã«é©å¿œçš„Î´ã‚’æ›´æ–°
loss_scalp = scalp_loss_fn(
    pred_scalp,
    target_scalp_clipped,  # quantile clippingæ¸ˆã¿
    historical_scalp_targets  # éå»Næ—¥åˆ†
)
```

**æˆåŠŸæŒ‡æ¨™**:
- å¹… loss ã®å¤‰å‹•ä¿‚æ•°ï¼ˆCVï¼‰< 0.3
- å¤–ã‚Œå€¤ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ>p99ï¼‰ã® loss å¯„ä¸ç‡ < 10%

**æ¤œè¨¼**:
```python
def test_magnitude_loss_stability():
    """ä¾¡æ ¼å¹…æå¤±ã®å®‰å®šæ€§ã‚’æ¤œè¨¼"""
    # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
    normal_targets = np.random.normal(1.0, 0.3, 1000)  # å¹³å‡1.0pips

    # å¤–ã‚Œå€¤æ··å…¥
    outlier_targets = np.concatenate([
        normal_targets,
        np.array([10.0, 15.0, 20.0])  # æ¥µç«¯ãªå¹…
    ])

    # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å‰å¾Œã®æå¤±æ¯”è¼ƒ
    loss_before = compute_loss(model, outlier_targets)
    loss_after = compute_loss(model, clip_magnitudes(outlier_targets))

    # æå¤±ã®å®‰å®šåŒ–ã‚’ç¢ºèª
    assert loss_after.std() < loss_before.std() * 0.7, "ãƒ­ãƒã‚¹ãƒˆåŒ–ä¸ååˆ†"
```

### ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ãƒ©ãƒ™ãƒ«

```python
# H1/H4ã®æ¬¡4æ™‚é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ç¶­æŒç‡
y_trend_strength = calculate_trend_persistence(
    current_time,
    lookforward_hours=4,
    timeframes=['H1', 'H4']
)

def calculate_trend_persistence(t, lookforward_hours, timeframes):
    """
    æ¬¡Næ™‚é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ä¸€è‡´ç‡ã‚’è¨ˆç®—

    Returns:
        float: 0-1ã®å€¤ï¼ˆ1=å®Œå…¨ãªãƒˆãƒ¬ãƒ³ãƒ‰ç¶­æŒï¼‰
    """
    direction_t = get_direction_at(t)
    future_bars = []

    for tf in timeframes:
        bars = get_bars(tf, t, t + lookforward_hours)
        direction_consistent = (bars['close'] > bars['open']) == direction_t
        future_bars.append(direction_consistent.mean())

    return np.mean(future_bars)  # TFé–“å¹³å‡
```

#### TrendStrengthç”Ÿæˆé–¢æ•°ã®æ¨™æº–å®šç¾©

**ç›®çš„**: ãƒ©ãƒ™ãƒ«ç”Ÿæˆã®ä¸€è²«æ€§ç¢ºä¿ã¨å†å­¦ç¿’æ™‚ã®å†ç¾æ€§ä¿è¨¼ã€‚

**å®Ÿè£…æ–¹å¼**: EMAæ¯”ç‡ãƒ™ãƒ¼ã‚¹

```python
def compute_trend_strength(
    close_series: np.ndarray,
    window: Tuple[int, int] = (12, 26),  # [w_short, w_long]
    method: str = 'ema_ratio',
    smoothing_gamma: float = 0.1
) -> float:
    """
    ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’è¨ˆç®—ï¼ˆå†ç¾æ€§ä¿è¨¼ï¼‰

    Args:
        close_series: çµ‚å€¤ç³»åˆ—ï¼ˆH1ã¾ãŸã¯H4ï¼‰
        window: [çŸ­æœŸçª“, é•·æœŸçª“]ï¼ˆEMAæœŸé–“ï¼‰
        method: 'ema_ratio' | 'trend_consistency' | 'directional_movement'
        smoothing_gamma: å¹³æ»‘åŒ–ä¿‚æ•°ï¼ˆ0-1ï¼‰

    Returns:
        float: ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ [0, 1]

    ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿:
        - method, window, smoothing_gamma
        - è¨ˆç®—ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚·ãƒ¥

    æˆåŠŸæ¡ä»¶:
        é€£ç¶šå†å­¦ç¿’ã§ãƒ©ãƒ™ãƒ«å†ç¾å·® < 1e-6
    """
    w_short, w_long = window

    if method == 'ema_ratio':
        # EMAæ¯”ç‡ã«ã‚ˆã‚‹å¼·åº¦è¨ˆç®—
        ema_short = pd.Series(close_series).ewm(span=w_short).mean().iloc[-1]
        ema_long = pd.Series(close_series).ewm(span=w_long).mean().iloc[-1]

        # æ­£è¦åŒ–ã•ã‚ŒãŸå·®åˆ†
        strength_raw = abs(ema_short - ema_long) / ema_long

        # [0, 1]ã«ã‚¯ãƒªãƒƒãƒ— + å¹³æ»‘åŒ–
        strength = np.clip(strength_raw * 100, 0, 1)
        strength = strength * (1 - smoothing_gamma) + 0.5 * smoothing_gamma

    elif method == 'trend_consistency':
        # æ–¹å‘ä¸€è‡´ç‡ï¼ˆæ—¢å­˜å®Ÿè£…ï¼‰
        strength = calculate_trend_persistence(...)

    elif method == 'directional_movement':
        # ADXé¢¨ã®æ–¹å‘æ€§æŒ‡æ¨™
        strength = calculate_adx_like(close_series, window[0])

    else:
        raise ValueError(f"Unknown method: {method}")

    return strength


# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆHDF5ï¼‰
def save_trend_strength_metadata(h5_file, params):
    """
    ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜

    ä¿å­˜å†…å®¹:
        /metadata/labeling/trend_strength/
            - method: str
            - window_short: int
            - window_long: int
            - smoothing_gamma: float
            - version_hash: str (params + code hash)
            - timestamp: str
    """
    meta_group = h5_file.require_group('metadata/labeling/trend_strength')
    meta_group.attrs['method'] = params['method']
    meta_group.attrs['window_short'] = params['window'][0]
    meta_group.attrs['window_long'] = params['window'][1]
    meta_group.attrs['smoothing_gamma'] = params['smoothing_gamma']
    meta_group.attrs['version_hash'] = compute_param_hash(params)
    meta_group.attrs['timestamp'] = datetime.now().isoformat()
```

**æ¤œè¨¼**:
```python
# å†ç¾æ€§ãƒ†ã‚¹ãƒˆ
def test_trend_strength_reproducibility():
    """é€£ç¶šå®Ÿè¡Œã§ãƒ©ãƒ™ãƒ«å†ç¾å·® < 1e-6ã‚’ç¢ºèª"""
    params = {'method': 'ema_ratio', 'window': (12, 26), 'smoothing_gamma': 0.1}

    result1 = compute_trend_strength(test_data, **params)
    result2 = compute_trend_strength(test_data, **params)

    assert abs(result1 - result2) < 1e-6, "å†ç¾æ€§é•å"
```

---

## âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```yaml
training:
  optimizer:
    type: AdamW
    lr: 1e-3
    warmup_ratio: 0.05
  
  scheduler:
    type: cosine
    min_lr: 1e-6
  
  loss_weights:
    direction: 0.40          # æ–¹å‘ï¼ˆæœ€é‡è¦ï¼‰
    magnitude_scalp: 0.35    # ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ä¾¡æ ¼å¹…
    magnitude_swing: 0.15    # ã‚¹ã‚¤ãƒ³ã‚°å»¶é•·ä¾¡æ ¼å¹…
    trend_strength: 0.10     # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
    dynamic_adjust: true     # å‹•çš„èª¿æ•´æœ‰åŠ¹åŒ–
  
  precision: fp16            # Mixed precision
  
  labeling:
    neutral_threshold:
      # å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º1: åŸºæœ¬å®Ÿè£…
      k_spread: 1.0          # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰å€ç‡
      k_atr: 0.3             # ATRå€ç‡
      atr_period: 14         # ATRè¨ˆç®—æœŸé–“
      atr_timeframe: "M5"    # ATRãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹TF
      spread_default: 1.2    # pipsï¼ˆå‹•çš„å–å¾—å¤±æ•—æ™‚ï¼‰
      
      # ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰åˆ¥èª¿æ•´
      scalp_mode:
        k_spread_multiplier: 1.2   # ã‚³ã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ³å¢—
        atr_timeframe: "M1"        # M1ãƒ™ãƒ¼ã‚¹
        k_atr: 0.25
      swing_mode:
        k_spread_multiplier: 0.8   # ã‚³ã‚¹ãƒˆæ„Ÿåº¦ä½
        atr_timeframe: "H1"        # H1ãƒ™ãƒ¼ã‚¹
        k_atr: 0.4
      
      # å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ¬ã‚¸ãƒ¼ãƒ æ‹¡å¼µï¼ˆå°†æ¥ï¼‰
      regime_detection:
        enabled: false       # Phase 2ã§æœ‰åŠ¹åŒ–
        atr_ratio_low: 0.6
        atr_ratio_high: 1.4
        regime_multiplier:
          low: 0.8
          normal: 1.0
          high: 1.3
  
  model:
    lstm_hidden: 128
    fusion_method: attention
    d_model: 128
  
  reproducibility:
    seed: 42
    deterministic: true
```

---

## ğŸ“ˆ æœŸå¾…å€¤å†æ§‹æˆ

å­¦ç¿’å¾Œã®å®Ÿç”¨åŒ–ã«å‘ã‘ã¦ï¼š

```python
# æœŸå¾…å€¤è¨ˆç®—
E[Î”P] = Î£ P(direction) Ã— predicted_magnitude

# ã‚³ã‚¹ãƒˆè€ƒæ…®
NetExpectancy = E[Î”P] - (spread + slippage + commission)
```

---

## âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢

### å¿…é ˆãƒã‚§ãƒƒã‚¯
- å…¥åŠ›æœ€çµ‚æ™‚åˆ» < ãƒ©ãƒ™ãƒ«å‚ç…§æ™‚åˆ»
- `shift(-n)` ä½¿ç”¨ç¦æ­¢
- æœªæ¥rollingç¦æ­¢

### æ¤œè¨¼
å­¦ç¿’å‰ã«è‡ªå‹•ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ

---

## ğŸ“Š å“è³ªæ¤œæŸ»

### ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰æ€§
å¹³è¡Œç§»å‹•ãƒ»æ‹¡å¤§å¾Œã® embedding é¡ä¼¼åº¦ > Ï„

### ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
NEUTRALæ¯”ç‡ãŒç¯„å›²å¤–ã®å ´åˆ WARNING

### ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ

çŸ­æœŸãƒ»é•·æœŸäºˆæ¸¬ã®è«–ç†çš„æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ã€çŸ›ç›¾ã™ã‚‹äºˆæ¸¬ï¼ˆè¿‘è·é›¢DOWNã€é è·é›¢UPç­‰ï¼‰ã‚’æ¤œå‡ºï¼š

```python
class MultiHorizonConsistencyChecker:
    """ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³äºˆæ¸¬ã®æ•´åˆæ€§æ¤œè¨¼"""
    
    def __init__(self, config: dict):
        self.max_reversal_rate = config.get("max_reversal_rate", 0.01)  # 1%æœªæº€
        self.monotonicity_penalty = config.get("monotonicity_penalty", 1.0)
    
    def check_consistency(self, predictions: dict) -> tuple[bool, dict]:
        """
        ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³äºˆæ¸¬ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        
        Args:
            predictions: {
                "scalp_magnitude": 0.8 pips,     # çŸ­æœŸï¼ˆæ¬¡æ•°æœ¬ï¼‰
                "swing_magnitude": 2.5 pips,     # ä¸­æœŸï¼ˆæ•°æ™‚é–“ï¼‰
                "scalp_direction": "UP",
                "swing_direction": "UP",
                "trend_strength": 0.65
            }
        
        Returns:
            (is_consistent, statistics)
        """
        stats = {
            "direction_reversal": False,
            "magnitude_monotonic": True,
            "reversal_count": 0,
            "consistency_score": 1.0
        }
        
        # 1. æ–¹å‘é€†è»¢æ¤œå‡º
        scalp_dir = predictions.get("scalp_direction")
        swing_dir = predictions.get("swing_direction")
        
        if scalp_dir and swing_dir and scalp_dir != swing_dir:
            # çŸ­æœŸUPã€é•·æœŸDOWNã¯æˆ¦ç•¥æ··ä¹±ã‚’æ‹›ã
            stats["direction_reversal"] = True
            stats["reversal_count"] += 1
            logger.warning(
                f"æ–¹å‘é€†è»¢æ¤œå‡º: scalp={scalp_dir}, swing={swing_dir}"
            )
        
        # 2. ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰å˜èª¿æ€§ãƒã‚§ãƒƒã‚¯
        scalp_mag = predictions.get("scalp_magnitude", 0)
        swing_mag = predictions.get("swing_magnitude", 0)
        
        # çŸ­æœŸ > é•·æœŸã¯ç•°å¸¸ï¼ˆé€šå¸¸ã¯é•·æœŸã®æ–¹ãŒå¤§ãã„ï¼‰
        if scalp_mag > swing_mag * 1.5:  # 1.5å€ä»¥ä¸Šã®å ´åˆ
            stats["magnitude_monotonic"] = False
            logger.warning(
                f"ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰éå˜èª¿: scalp={scalp_mag:.2f} > swing={swing_mag:.2f}"
            )
        
        # 3. æ•´åˆæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        if stats["direction_reversal"]:
            stats["consistency_score"] *= 0.5  # ãƒšãƒŠãƒ«ãƒ†ã‚£
        
        if not stats["magnitude_monotonic"]:
            stats["consistency_score"] *= 0.8
        
        # 4. ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã¨ã®æ•´åˆæ€§
        trend_strength = predictions.get("trend_strength", 0.5)
        
        # å¼·ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆ>0.7ï¼‰ãªã®ã«æ–¹å‘é€†è»¢ã¯çŸ›ç›¾
        if trend_strength > 0.7 and stats["direction_reversal"]:
            stats["consistency_score"] *= 0.3
            logger.error(
                f"é‡å¤§ãªçŸ›ç›¾: trend_strength={trend_strength:.2f}, "
                f"but direction reversal"
            )
        
        is_consistent = stats["consistency_score"] >= 0.8
        
        return is_consistent, stats
    
    def assert_consistency_during_training(self, batch_predictions: dict):
        """
        å­¦ç¿’æ™‚ã®ãƒãƒƒãƒæ•´åˆæ€§æ¤œè¨¼
        
        Args:
            batch_predictions: {
                "scalp_magnitude": [batch_size],
                "swing_magnitude": [batch_size],
                ...
            }
        """
        batch_size = len(batch_predictions["scalp_direction"])
        reversal_count = 0
        
        for i in range(batch_size):
            pred = {
                "scalp_magnitude": batch_predictions["scalp_magnitude"][i],
                "swing_magnitude": batch_predictions["swing_magnitude"][i],
                "scalp_direction": batch_predictions["scalp_direction"][i],
                "swing_direction": batch_predictions["swing_direction"][i],
                "trend_strength": batch_predictions.get("trend_strength", [0.5])[i]
            }
            
            is_consistent, stats = self.check_consistency(pred)
            
            if stats["direction_reversal"]:
                reversal_count += 1
        
        # é€†è»¢ç‡ãƒã‚§ãƒƒã‚¯
        reversal_rate = reversal_count / batch_size
        
        if reversal_rate > self.max_reversal_rate:
            logger.error(
                f"ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³é€†è»¢ç‡ç•°å¸¸: {reversal_rate:.2%} > "
                f"{self.max_reversal_rate:.2%}, å­¦ç¿’åœæ­¢æ¨å¥¨"
            )
            # ç•°å¸¸æ™‚ã¯ä¾‹å¤–ï¼ˆå­¦ç¿’åœæ­¢ï¼‰
            raise ValueError(
                f"Consistency violation: reversal_rate={reversal_rate:.2%}"
            )
        
        return reversal_rate


# ä½¿ç”¨ä¾‹: å­¦ç¿’æ™‚ã®æ¤œè¨¼
consistency_checker = MultiHorizonConsistencyChecker({
    "max_reversal_rate": 0.01,  # 1%æœªæº€
    "monotonicity_penalty": 1.0
})

# ãƒãƒƒãƒäºˆæ¸¬å¾Œã«ãƒã‚§ãƒƒã‚¯
predictions = {
    "scalp_magnitude": torch.tensor([0.8, 1.2, 0.5, ...]),
    "swing_magnitude": torch.tensor([2.5, 3.0, 1.8, ...]),
    "scalp_direction": ["UP", "UP", "DOWN", ...],
    "swing_direction": ["UP", "DOWN", "DOWN", ...],  # 2ç•ªç›®ãŒé€†è»¢
    "trend_strength": torch.tensor([0.65, 0.55, 0.72, ...])
}

try:
    reversal_rate = consistency_checker.assert_consistency_during_training(predictions)
    logger.info(f"æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯é€šé: é€†è»¢ç‡={reversal_rate:.2%}")
except ValueError as e:
    logger.error(f"æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼: {e}")
    # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã¾ãŸã¯åœæ­¢
```

**ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³æ•´åˆæ€§ãƒ†ã‚¹ãƒˆä»•æ§˜**:
- **é€†è»¢ç‡é–¾å€¤**: < 1%ï¼ˆ99%ä»¥ä¸ŠãŒæ•´åˆï¼‰
- **æ–¹å‘é€†è»¢æ¤œå‡º**: scalp â‰  swing
- **ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰å˜èª¿æ€§**: scalp < swing Ã— 1.5
- **ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦æ•´åˆ**: trend_strength > 0.7 ã§é€†è»¢ç¦æ­¢
- **æˆåŠŸæŒ‡æ¨™**: é€†è»¢ç‡ < 1%ã€æ•´åˆæ€§ã‚¹ã‚³ã‚¢ >= 0.8

**åŠ¹æœ**:
- çŸ›ç›¾äºˆæ¸¬ã«ã‚ˆã‚‹æˆ¦ç•¥æ··ä¹±é˜²æ­¢
- ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®‰å®šæ€§å‘ä¸Š
- ãƒ‡ãƒãƒƒã‚°å¯èƒ½æ€§å‘ä¸Šï¼ˆç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ—©æœŸç™ºè¦‹ï¼‰

---

## ğŸ“ ãƒ­ã‚°å‡ºåŠ›

### æ™‚åˆ»è¡¨ç¤ºãƒ«ãƒ¼ãƒ«
- **å…¨ãƒ­ã‚°**: æ—¥æœ¬æ™‚é–“(JST)ã§è¡¨ç¤º
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: `YYYY-MM-DD HH:MM:SS JST`
- **å­¦ç¿’é–‹å§‹/çµ‚äº†æ™‚åˆ»**: æ—¥æœ¬æ™‚é–“ã§æ˜è¨˜

```
ğŸ”„ ç¬¬4æ®µéš: å­¦ç¿’é–‹å§‹ [2025-10-24 00:15:30 JST]
ğŸ“‚ å…¥åŠ›: models/fx_mtf_20251022_100000_preprocessed.h5
   å­¦ç¿’æœŸé–“: 2024-01-01 00:00:00 JST ï½ 2024-10-31 23:59:00 JST
   æ¤œè¨¼æœŸé–“: 2024-11-01 00:00:00 JST ï½ 2024-12-31 23:59:00 JST

ğŸ”„ Epoch 1/100 [2025-10-24 00:16:12 JST]
ğŸ“Š Loss: total=0.523, direction=0.312, magnitude=0.211
ğŸ¯ Accuracy: direction=0.68, NEUTRAL_ratio=0.33
âš™ï¸ Weights: Î±=0.52, Î²=0.48

âœ… å­¦ç¿’å®Œäº† [2025-10-24 03:45:18 JST]
   ç·å­¦ç¿’æ™‚é–“: 3æ™‚é–“29åˆ†48ç§’
```

---

## ğŸ’¾ å†ç¾æ€§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

HDF5ã«ä¿å­˜:
```
/metadata/training_info
  - config_hash
  - feature_names
  - horizon_set
  - commit_hash
  - seed
  - timestamp
```

---

## ğŸš¨ ã‚¨ãƒ©ãƒ¼æ¡ä»¶

- ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œçŸ¥
- ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç•°å¸¸ï¼ˆNEUTRAL > 60% or < 10%ï¼‰
- æå¤±NaN/Inf
- å‹¾é…çˆ†ç™º

---

## ğŸ”— é–¢é€£ä»•æ§˜æ›¸

- **å‰æ®µéš**: ç¬¬3æ®µéš: [PREPROCESSOR_SPEC.md](./PREPROCESSOR_SPEC.md) - å‰å‡¦ç†
- **æ¬¡å·¥ç¨‹**: ç¬¬5æ®µéš: [VALIDATOR_SPEC.md](./VALIDATOR_SPEC.md) - æ¤œè¨¼
- **è©³ç´°ä»•æ§˜**:
  - [trainer/MULTI_TF_FUSION_SPEC.md](./trainer/MULTI_TF_FUSION_SPEC.md) - ãƒãƒ«ãƒTFèåˆ
  - [trainer/MODEL_ARCHITECTURE_SCALP_EXTENSION_SPEC.md](./trainer/MODEL_ARCHITECTURE_SCALP_EXTENSION_SPEC.md) - ã‚¹ã‚«ãƒ«ãƒ—æ‹¡å¼µ
  - [trainer/GPU_OPTIMIZATION_SPEC.md](./trainer/GPU_OPTIMIZATION_SPEC.md) - GPUæœ€é©åŒ–

---

## é‹ç”¨å“è³ªãƒ»ä¿å®ˆæ€§å‘ä¸Š

### Huber Î´å‹•çš„èª¿æ•´

**ç›®çš„**: å›ºå®šÎ´ã¯ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£regimeå¤‰åŒ–ã§å¤–ã‚Œå€¤æ‰±ã„èª¤å·®ãŒå¢—å¤§

**è§£æ±ºç­–**: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é©å¿œå‹Î´è‡ªå‹•æ›´æ–°

```python
class AdaptiveHuberLoss:
    """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é©å¿œå‹Huberæå¤±"""
    
    def __init__(self, config: dict):
        self.k_factor = config.get("huber_k_factor", 1.5)  # Î´ = median * k
        self.update_interval = config.get("huber_update_interval", 1000)  # steps
        self.min_delta = config.get("huber_min_delta", 0.1)
        self.max_delta = config.get("huber_max_delta", 5.0)
        
        self.step_count = 0
        self.current_delta = 1.0  # åˆæœŸå€¤
    
    def update_delta(self, targets: torch.Tensor):
        """
        Î´ã‚’å‹•çš„æ›´æ–°
        
        Args:
            targets: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ï¼ˆä¾¡æ ¼å¹…ï¼‰ãƒ†ãƒ³ã‚½ãƒ«
        
        Returns:
            updated_delta: æ–°ã—ã„Î´å€¤
        """
        # ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹è¨ˆç®—
        median_abs = torch.median(torch.abs(targets)).item()
        
        # p90å¹…ãƒ™ãƒ¼ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        p90 = torch.quantile(torch.abs(targets), 0.9).item()
        
        # Î´ = max(median * k, p90 * 0.5)
        delta_median = median_abs * self.k_factor
        delta_p90 = p90 * 0.5
        
        new_delta = max(delta_median, delta_p90)
        new_delta = np.clip(new_delta, self.min_delta, self.max_delta)
        
        # å¤‰åŒ–ç‡ãƒã‚§ãƒƒã‚¯
        if hasattr(self, 'current_delta'):
            change_ratio = abs(new_delta - self.current_delta) / self.current_delta
            if change_ratio > 0.3:
                logger.warning(
                    f"Huber Î´å¤§å¹…å¤‰æ›´: {self.current_delta:.3f} â†’ {new_delta:.3f} "
                    f"(å¤‰åŒ–ç‡={change_ratio:.2%})"
                )
        
        self.current_delta = new_delta
        return new_delta
    
    def forward(
        self,
        predictions: torch.Tensor,
        targets: torch.Tensor
    ) -> torch.Tensor:
        """
        é©å¿œå‹Huberæå¤±è¨ˆç®—
        
        Args:
            predictions: äºˆæ¸¬å€¤
            targets: æ­£è§£å€¤
        
        Returns:
            loss: Huberæå¤±
        """
        # Î´æ›´æ–°ï¼ˆå®šæœŸå®Ÿè¡Œï¼‰
        self.step_count += 1
        if self.step_count % self.update_interval == 0:
            self.update_delta(targets)
        
        # Huberæå¤±è¨ˆç®—
        residual = predictions - targets
        abs_residual = torch.abs(residual)
        
        quadratic = torch.min(abs_residual, torch.tensor(self.current_delta))
        linear = abs_residual - quadratic
        
        loss = 0.5 * quadratic ** 2 + self.current_delta * linear
        
        return loss.mean()


# ä½¿ç”¨ä¾‹
adaptive_huber = AdaptiveHuberLoss({
    "huber_k_factor": 1.5,
    "huber_update_interval": 1000,
    "huber_min_delta": 0.1,
    "huber_max_delta": 5.0
})

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—å†…
loss_magnitude = adaptive_huber.forward(pred_magnitude, target_magnitude)
```

**KPIï¼ˆé …ç›®10ï¼‰**:
- Î´å¤‰åŒ–ç‡: é€£ç¶š30%è¶…å¤‰æ›´ â†’ ãƒœãƒ©regime shiftæ¤œå‡º
- å¹…head lossæ”¹å–„ç‡: â‰¥+5%ï¼ˆå›ºå®šÎ´æ¯”è¼ƒï¼‰
- Î´ç¯„å›²ç¶­æŒ: [0.1, 5.0] pipså†…

---

### Multi-headå‹¾é…å¹²æ¸‰å¯¾ç­–ï¼ˆPCGradï¼‰

**ç›®çš„**: é•·æœŸheadå‹¾é…ãŒçŸ­æœŸã‚¹ã‚«ãƒ«ãƒ—headã‚’æŠ¼ã—æ½°ã—æ–¹å‘ç¢ºç‡å¹³æ»‘åŒ–

**è§£æ±ºç­–**: PCGradï¼ˆProject Conflicting Gradientsï¼‰å°å…¥

```python
class PCGradOptimizer:
    """å‹¾é…å¹²æ¸‰è§£æ¶ˆOptimizer"""
    
    def __init__(self, optimizer: torch.optim.Optimizer, config: dict):
        self.optimizer = optimizer
        self.enabled = config.get("pcgrad_enabled", False)
        self.log_interval = config.get("pcgrad_log_interval", 100)
        
        self.step_count = 0
        self.interference_stats = []
    
    def pc_grad_update(self, losses: Dict[str, torch.Tensor]):
        """
        PCGradé©ç”¨
        
        Args:
            losses: ã‚¿ã‚¹ã‚¯åˆ¥æå¤±è¾æ›¸ {"direction": loss1, "magnitude": loss2, ...}
        """
        if not self.enabled:
            # é€šå¸¸æ›´æ–°
            total_loss = sum(losses.values())
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
            return
        
        # ã‚¿ã‚¹ã‚¯åˆ¥å‹¾é…å–å¾—
        grads = {}
        for task_name, loss in losses.items():
            self.optimizer.zero_grad()
            loss.backward(retain_graph=True)
            
            # å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’åé›†
            task_grads = []
            for param in self.optimizer.param_groups[0]['params']:
                if param.grad is not None:
                    task_grads.append(param.grad.clone().flatten())
            
            grads[task_name] = torch.cat(task_grads)
        
        # å¹²æ¸‰æ¤œå‡ºãƒ»å°„å½±
        task_names = list(grads.keys())
        projected_grads = {name: grads[name].clone() for name in task_names}
        
        for i, task_i in enumerate(task_names):
            for j, task_j in enumerate(task_names):
                if i >= j:
                    continue
                
                g_i = grads[task_i]
                g_j = grads[task_j]
                
                # å†…ç©è¨ˆç®—
                dot_product = torch.dot(g_i, g_j)
                
                # å¹²æ¸‰æ¤œå‡ºï¼ˆè² ã®å†…ç© = å¯¾ç«‹ï¼‰
                if dot_product < 0:
                    # task_iã®å‹¾é…ã‹ã‚‰task_jæˆåˆ†ã‚’å°„å½±é™¤å»
                    g_j_norm_sq = torch.dot(g_j, g_j)
                    if g_j_norm_sq > 0:
                        projected_grads[task_i] -= (dot_product / g_j_norm_sq) * g_j
                    
                    # çµ±è¨ˆè¨˜éŒ²
                    interference_ratio = abs(dot_product / (torch.norm(g_i) * torch.norm(g_j) + 1e-8))
                    self.interference_stats.append({
                        "task_i": task_i,
                        "task_j": task_j,
                        "interference_ratio": interference_ratio.item()
                    })
        
        # å°„å½±å¾Œå‹¾é…ã‚’é©ç”¨
        self.optimizer.zero_grad()
        param_idx = 0
        for param in self.optimizer.param_groups[0]['params']:
            if param.grad is not None:
                param_size = param.numel()
                
                # å…¨ã‚¿ã‚¹ã‚¯ã®å°„å½±å‹¾é…ã‚’å¹³å‡
                combined_grad = torch.zeros_like(param.flatten())
                for task_name in task_names:
                    task_grad_slice = projected_grads[task_name][param_idx:param_idx+param_size]
                    combined_grad += task_grad_slice
                
                combined_grad /= len(task_names)
                param.grad = combined_grad.view_as(param)
                param_idx += param_size
        
        self.optimizer.step()
        
        # çµ±è¨ˆãƒ­ã‚°
        self.step_count += 1
        if self.step_count % self.log_interval == 0 and self.interference_stats:
            avg_interference = np.mean([s["interference_ratio"] for s in self.interference_stats])
            logger.info(f"PCGrad å¹²æ¸‰ç‡: {avg_interference:.3f}")
            self.interference_stats = []


# ä½¿ç”¨ä¾‹
pcgrad_optimizer = PCGradOptimizer(
    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),
    config={"pcgrad_enabled": True, "pcgrad_log_interval": 100}
)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
losses = {
    "direction": loss_direction,
    "magnitude_scalp": loss_magnitude_scalp,
    "magnitude_swing": loss_magnitude_swing
}
pcgrad_optimizer.pc_grad_update(losses)
```

**KPIï¼ˆé …ç›®20ï¼‰**:
- å¹²æ¸‰ç‡: <15%
- scalp head accuracyæ”¹å–„: â‰¥+2%ï¼ˆbaselineæ¯”è¼ƒï¼‰
- per-head grad_cosine: >0ï¼ˆå¯¾ç«‹ãªã—ï¼‰

---

### ã‚³ã‚¹ãƒˆè¦ç´ åŒæ™‚æœ€é©åŒ–ãƒ­ã‚¹

**ç›®çš„**: æ–¹å‘ç²¾åº¦åé‡ã§ãƒãƒƒãƒˆåˆ©ç›ŠãŒè² åŒ–

**è§£æ±ºç­–**: Cost-awareè¤‡åˆæå¤±é–¢æ•°

```python
class CostAwareLoss:
    """ã‚³ã‚¹ãƒˆè€ƒæ…®è¤‡åˆæå¤±"""
    
    def __init__(self, config: dict):
        self.cost_penalty_weight = config.get("cost_penalty_weight", 0.3)
        self.direction_weight = config.get("direction_weight", 0.4)
        self.magnitude_weight = config.get("magnitude_weight", 0.3)
        
        self.spread_baseline = config.get("spread_baseline_pips", 1.5)
        self.slip_baseline = config.get("slip_baseline_pips", 0.3)
    
    def forward(
        self,
        pred_direction: torch.Tensor,  # [batch, 3] (UP/DOWN/NEUTRAL)
        pred_magnitude: torch.Tensor,  # [batch, 1]
        target_direction: torch.Tensor,  # [batch]
        target_magnitude: torch.Tensor,  # [batch, 1]
        spread: torch.Tensor,  # [batch, 1]
        slip_estimate: torch.Tensor  # [batch, 1]
    ) -> Dict[str, torch.Tensor]:
        """
        ã‚³ã‚¹ãƒˆè€ƒæ…®è¤‡åˆæå¤±è¨ˆç®—
        
        Returns:
            losses: {"total": total_loss, "direction": ..., "magnitude": ..., "cost_penalty": ...}
        """
        # 1. æ–¹å‘æå¤±ï¼ˆCEï¼‰
        loss_direction = F.cross_entropy(pred_direction, target_direction)
        
        # 2. ä¾¡æ ¼å¹…æå¤±ï¼ˆHuberï¼‰
        loss_magnitude = F.smooth_l1_loss(pred_magnitude, target_magnitude)
        
        # 3. ã‚³ã‚¹ãƒˆãƒšãƒŠãƒ«ãƒ†ã‚£
        # ãƒãƒƒãƒˆæœŸå¾…å€¤ = äºˆæ¸¬ä¾¡æ ¼å¹… - 2*spread - slip
        net_expectancy = pred_magnitude - 2 * spread - slip_estimate
        
        # è² ã®æœŸå¾…å€¤ã«ãƒšãƒŠãƒ«ãƒ†ã‚£
        cost_penalty = torch.relu(-net_expectancy).mean()
        
        # 4. çµ±åˆæå¤±
        total_loss = (
            self.direction_weight * loss_direction +
            self.magnitude_weight * loss_magnitude +
            self.cost_penalty_weight * cost_penalty
        )
        
        return {
            "total": total_loss,
            "direction": loss_direction,
            "magnitude": loss_magnitude,
            "cost_penalty": cost_penalty
        }


# ä½¿ç”¨ä¾‹
cost_aware_loss = CostAwareLoss({
    "cost_penalty_weight": 0.3,
    "direction_weight": 0.4,
    "magnitude_weight": 0.3,
    "spread_baseline_pips": 1.5,
    "slip_baseline_pips": 0.3
})

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
losses = cost_aware_loss.forward(
    pred_direction=model.direction_head(features),
    pred_magnitude=model.magnitude_head(features),
    target_direction=batch["direction_label"],
    target_magnitude=batch["magnitude_label"],
    spread=batch["spread"],
    slip_estimate=batch["slip_estimate"]
)

losses["total"].backward()
optimizer.step()
```

**KPIï¼ˆé …ç›®99ï¼‰**:
- cost-adjusted expectancy>0 ã‚µãƒ³ãƒ—ãƒ«ç‡: +10%
- ãƒãƒƒãƒˆåˆ©ç›Šæ”¹å–„: â‰¥+5%ï¼ˆbaselineæ¯”è¼ƒï¼‰
- ã‚³ã‚¹ãƒˆãƒšãƒŠãƒ«ãƒ†ã‚£æå¤±: å­¦ç¿’é€²è¡Œã§æ¸›å°‘å‚¾å‘

---

## ğŸ“Œ æ³¨æ„äº‹é …

1. **ã‚¨ãƒ©ãƒ¼æ¡ã‚Šã¤ã¶ã—ç¦æ­¢**: ç•°å¸¸æ™‚ã¯ `raise` ã§åœæ­¢
2. **ã‚·ãƒ¼ãƒ‰å›ºå®š**: å†ç¾æ€§ç¢ºä¿
3. **FP16ä½¿ç”¨**: GPUåŠ¹ç‡åŒ–
4. **å‹•çš„æå¤±é‡ã¿**: åæŸå®‰å®šåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

---

## ğŸ”® å°†æ¥æ‹¡å¼µ

- ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚¨ãƒ³ã‚³ãƒ¼ãƒ€çµ±åˆ
- ãƒã‚¤ã‚¯ãƒ­æ§‹é€ ç‰¹å¾´é‡èåˆ
- ãƒã‚¶ãƒ¼ãƒ‰äºˆæ¸¬ãƒ˜ãƒƒãƒ‰è¿½åŠ 
- ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚¾ãƒ³äºˆæ¸¬
- ãƒ¬ã‚¸ãƒ¼ãƒ é©å¿œå‹é–¾å€¤
