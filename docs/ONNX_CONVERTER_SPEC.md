# ONNX_CONVERTER_SPEC.md

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**æ›´æ–°æ—¥**: 2025-10-22
**è²¬ä»»è€…**: core-team
**å‡¦ç†æ®µéš**: ç¬¬6æ®µéš: ONNXå¤‰æ›

---

## ğŸ“‹ ç›®çš„

`src/onnx_converter.py` ãŒ**ç¬¬4æ®µéšã§å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«**ã‚’ONNXå½¢å¼ã«å¤‰æ›ã—ã€**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–**ç”¨ã«æœ€é©åŒ–ã™ã‚‹ã€‚

**è²¬ä»»ç¯„å›²**:
- PyTorchãƒ¢ãƒ‡ãƒ« (.pt) â†’ ONNX (.onnx) ã¸ã®å¤‰æ›
- é‡å­åŒ–ï¼ˆFP16/INT8ï¼‰ã«ã‚ˆã‚‹æ¨è«–é«˜é€ŸåŒ–
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¤œè¨¼ï¼ˆp95 < 10msç›®æ¨™ï¼‰
- ç²¾åº¦åŠ£åŒ–æ¤œè¨¼ï¼ˆ< 1%ï¼‰

**å‡¦ç†æ®µéšã®åˆ†é›¢**:
- **ç¬¬4æ®µéšï¼ˆå­¦ç¿’ï¼‰**: `src/trainer.py` â†’ `models/*_training.pt`
- **ç¬¬5æ®µéšï¼ˆæ¤œè¨¼ï¼‰**: `src/validator.py` â†’ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
- **ç¬¬6æ®µéšï¼ˆONNXå¤‰æ›ï¼‰**: `src/onnx_converter.py` â†’ `models/*_model.onnx`

---

## ğŸ”„ å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
å…¥åŠ›: models/*_training.ptï¼ˆç¬¬4æ®µéšã§å­¦ç¿’ï¼‰
  - MultiTFLSTMModelï¼ˆPyTorchï¼‰
  - ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
  - å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰]
  - æ§‹é€ æ¤œè¨¼ï¼ˆå…¥åŠ›/å‡ºåŠ›å½¢çŠ¶ï¼‰
  - é‡ã¿ãƒ­ãƒ¼ãƒ‰ç¢ºèª
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—2: ONNXå¤‰æ›]
  - torch.onnx.export()
  - ã‚ªãƒšãƒ¬ãƒ¼ã‚¿äº’æ›æ€§ç¢ºèª
  - ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯è»¸è¨­å®š
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—3: é‡å­åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰]
  - FP32 â†’ FP16ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
  - FP16 â†’ INT8ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¤œè¨¼]
  - 1000ã‚µãƒ³ãƒ—ãƒ«ã§æ¨è«–é€Ÿåº¦æ¸¬å®š
  - p95 < 10ms ç¢ºèª
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—5: ç²¾åº¦åŠ£åŒ–æ¤œè¨¼]
  - åŒä¸€å…¥åŠ›ã§PyTorch vs ONNXæ¯”è¼ƒ
  - RMSEåŠ£åŒ– < 1% ç¢ºèª
    â†“
å‡ºåŠ›: models/*_model.onnx
  - æœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
  - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆç²¾åº¦ãƒ»é€Ÿåº¦ï¼‰
```

---

## ğŸ¯ ONNXå¤‰æ›ã®åˆ©ç‚¹

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã«å¿…è¦ãªæ€§èƒ½

| è¦ä»¶ | PyTorch | ONNX Runtime |
|------|---------|--------------|
| **æ¨è«–é€Ÿåº¦** | é…ã„ï¼ˆPython GILï¼‰ | é«˜é€Ÿï¼ˆC++å®Ÿè£…ï¼‰ |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | p95 â‰ˆ 30ms | p95 < 10msï¼ˆç›®æ¨™ï¼‰ |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | å¤§ï¼ˆå‹•çš„ã‚°ãƒ©ãƒ•ï¼‰ | å°ï¼ˆé™çš„ã‚°ãƒ©ãƒ•ï¼‰ |
| **ãƒ‡ãƒ—ãƒ­ã‚¤** | Pythonãƒ©ãƒ³ã‚¿ã‚¤ãƒ å¿…é ˆ | è»½é‡ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  |

### MT5 Expert Advisorçµ±åˆ

```mql5
// MT5å´ã§ONNXãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
#include <Trade\Trade.mqh>
#include <ONNX\ONNX.mqh>

// ONNXãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
long model_handle = OnnxCreateFromBuffer(model_buffer, ONNX_DEFAULT);

// æ¨è«–å®Ÿè¡Œ
float input[5][360][52];  // M1/M5/M15/H1/H4 x 360æœ¬ x 52ç‰¹å¾´é‡
float output[36];         // 36æœ¬å…ˆã¾ã§ã®ä¾¡æ ¼äºˆæ¸¬

OnnxRun(model_handle, input, output);

// pipså¤‰æ› (USDJPY: 0.01å††=1pip)
double predicted_change_pips = output[0] * 100;  // USDJPYç”¨

// æ³¨è¨˜: é€šè²¨ãƒšã‚¢ã”ã¨ã«å¤‰æ›´å¿…è¦
// EURUSD: output[0] * 10000
```

---

## ğŸ“Š å¤‰æ›æˆ¦ç•¥

### 1. æ¨™æº–å¤‰æ›ï¼ˆFP32ï¼‰

```python
import torch
import torch.onnx

def convert_to_onnx_fp32(
    model: torch.nn.Module,
    model_path: str,
    output_path: str,
    input_shape: Tuple[int, ...]
):
    """
    PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNX FP32å½¢å¼ã«å¤‰æ›
    
    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        model_path: .ptãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_path: .onnxãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        input_shape: (batch, tf, seq, features)
                     ä¾‹: (1, 5, 360, 52)
    """
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model.load_state_dict(torch.load(model_path))
    model.eval()
    
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ï¼ˆå½¢çŠ¶ç¢ºèªç”¨ï¼‰
    dummy_input = torch.randn(*input_shape)
    
    # ONNXå¤‰æ›
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=17,  # ONNX Runtime 1.16+
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},   # ãƒãƒƒãƒã‚µã‚¤ã‚ºå¯å¤‰
            'output': {0: 'batch_size'}
        }
    )
    
    logger.info(f"âœ… ONNXå¤‰æ›å®Œäº†: {output_path}")
```

### 2. FP16é‡å­åŒ–ï¼ˆæ¨å¥¨ï¼‰

```python
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

def convert_to_onnx_fp16(
    onnx_fp32_path: str,
    output_path: str
):
    """
    FP32ãƒ¢ãƒ‡ãƒ«ã‚’FP16ã«é‡å­åŒ–
    
    ãƒ¡ãƒªãƒƒãƒˆ:
    - ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º 50%å‰Šæ¸›
    - æ¨è«–é€Ÿåº¦ 1.5-2å€å‘ä¸Š
    - ç²¾åº¦åŠ£åŒ– < 0.5%
    """
    model = onnx.load(onnx_fp32_path)
    
    # FP16å¤‰æ›
    from onnxconverter_common import float16
    model_fp16 = float16.convert_float_to_float16(model)
    
    onnx.save(model_fp16, output_path)
    
    logger.info(f"âœ… FP16å¤‰æ›å®Œäº†: {output_path}")
```

### 3. INT8é‡å­åŒ–ï¼ˆé«˜åº¦ï¼‰

```python
def convert_to_onnx_int8(
    onnx_fp32_path: str,
    output_path: str,
    calibration_data: np.ndarray
):
    """
    FP32ãƒ¢ãƒ‡ãƒ«ã‚’INT8ã«é‡å­åŒ–
    
    æ³¨æ„:
    - ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å¿…é ˆ
    - æ¨è«–é€Ÿåº¦ 2-4å€å‘ä¸Š
    - ç²¾åº¦åŠ£åŒ– 0.5-2%ï¼ˆè¦æ¤œè¨¼ï¼‰
    
    Args:
        calibration_data: (1000, 5, 360, 52)
    """
    quantize_dynamic(
        onnx_fp32_path,
        output_path,
        weight_type=QuantType.QInt8,
        optimize_model=True
    )
    
    logger.info(f"âœ… INT8å¤‰æ›å®Œäº†: {output_path}")
```

---

## ğŸï¸ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¤œè¨¼

### æ¨è«–é€Ÿåº¦æ¸¬å®š

```python
import onnxruntime as ort
import time

def benchmark_latency(
    onnx_path: str,
    input_shape: Tuple[int, ...],
    num_samples: int = 1000
) -> Dict[str, float]:
    """
    ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æ¸¬å®š
    
    ç›®æ¨™: p95 < 10ms
    
    Returns:
        {
            'mean_ms': float,
            'p50_ms': float,
            'p95_ms': float,
            'p99_ms': float
        }
    """
    # ONNXãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    session = ort.InferenceSession(
        onnx_path,
        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
    )
    
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ç”Ÿæˆ
    dummy_input = np.random.randn(*input_shape).astype(np.float32)
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆJITæœ€é©åŒ–ï¼‰
    for _ in range(10):
        session.run(None, {'input': dummy_input})
    
    # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š
    latencies = []
    for _ in range(num_samples):
        start = time.perf_counter()
        session.run(None, {'input': dummy_input})
        end = time.perf_counter()
        latencies.append((end - start) * 1000)  # ms
    
    latencies = np.array(latencies)
    
    results = {
        'mean_ms': np.mean(latencies),
        'p50_ms': np.percentile(latencies, 50),
        'p95_ms': np.percentile(latencies, 95),
        'p99_ms': np.percentile(latencies, 99)
    }
    
    logger.info(f"ğŸ“Š ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: p95={results['p95_ms']:.2f}ms")
    
    # ç›®æ¨™é”æˆç¢ºèª
    if results['p95_ms'] > 10.0:
        logger.warning(f"âš ï¸ p95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç›®æ¨™æœªé”: {results['p95_ms']:.2f}ms > 10ms")
    
    return results
```

### ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–

```python
def optimize_onnx_model(onnx_path: str, output_path: str):
    """
    ONNXãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–
    
    - ä¸è¦ãƒãƒ¼ãƒ‰å‰Šé™¤
    - å®šæ•°ç•³ã¿è¾¼ã¿
    - å†—é•·è¨ˆç®—é™¤å»
    """
    import onnxoptimizer
    
    model = onnx.load(onnx_path)
    
    # æœ€é©åŒ–ãƒ‘ã‚¹
    passes = [
        'eliminate_deadend',
        'eliminate_identity',
        'eliminate_nop_transpose',
        'fuse_consecutive_transposes',
        'fuse_matmul_add_bias_into_gemm',
        'fuse_bn_into_conv'
    ]
    
    optimized_model = onnxoptimizer.optimize(model, passes)
    onnx.save(optimized_model, output_path)
    
    logger.info(f"âœ… ONNXæœ€é©åŒ–å®Œäº†: {output_path}")
```

### GPU/ONNX ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¸¬å®šé–‹å§‹åŸºæº–

**ç›®çš„**: åˆæœŸæ•°ä»¶ãŒçµ±è¨ˆæ­ªã‚SLAèª¤åˆ¤å®šã€‚åˆæœŸé…å»¶ã‚’p50/p95çµ±è¨ˆã‹ã‚‰é™¤å¤–ã™ã‚‹æ˜ç¢ºæ¡ä»¶ä¸åœ¨ã€‚

**å®Ÿè£…**: warmup_calls Nè¶…éå¾Œè¨ˆæ¸¬é–‹å§‹ãƒ«ãƒ¼ãƒ«è¨˜è¿°

```python
def benchmark_latency_with_warmup(
    onnx_path: str,
    input_shape: Tuple[int, ...],
    warmup_calls: int = 20,  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å›æ•°
    measurement_calls: int = 1000,
    warmup_detection: str = 'fixed'  # 'fixed' | 'adaptive'
) -> Dict[str, Any]:
    """
    ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é™¤å¤–ä»˜ããƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š

    Args:
        warmup_calls: å›ºå®šã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å›æ•°ï¼ˆfixed modeï¼‰
        measurement_calls: æ¸¬å®šå›æ•°
        warmup_detection: 'fixed' = å›ºå®šå›æ•°é™¤å¤–
                         'adaptive' = å®‰å®šåŒ–æ¤œå‡ºå¾Œè¨ˆæ¸¬é–‹å§‹

    Returns:
        {
            'mean_ms': float,
            'p50_ms': float,
            'p95_ms': float,
            'p99_ms': float,
            'warmup_completed_at': int,  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            'warmup_latencies': List[float],  # é™¤å¤–ã•ã‚ŒãŸã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—çµæœ
            'measurement_latencies': List[float]  # æ¸¬å®šå¯¾è±¡
        }
    """
    import onnxruntime as ort
    import time

    session = ort.InferenceSession(
        onnx_path,
        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
    )

    dummy_input = np.random.randn(*input_shape).astype(np.float32)

    all_latencies = []
    warmup_completed_at = None

    if warmup_detection == 'fixed':
        # å›ºå®šå›æ•°ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        logger.info(f"ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹ï¼ˆå›ºå®š{warmup_calls}å›ï¼‰")

        for i in range(warmup_calls):
            start = time.perf_counter()
            session.run(None, {'input': dummy_input})
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            all_latencies.append(latency_ms)

        warmup_completed_at = warmup_calls
        logger.info(f"âœ… ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†ï¼ˆ{warmup_calls}å›ï¼‰")

        # æ¸¬å®šé–‹å§‹
        logger.info(f"ğŸ“Š æ¸¬å®šé–‹å§‹ï¼ˆ{measurement_calls}å›ï¼‰")
        for i in range(measurement_calls):
            start = time.perf_counter()
            session.run(None, {'input': dummy_input})
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            all_latencies.append(latency_ms)

    elif warmup_detection == 'adaptive':
        # é©å¿œçš„ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å®‰å®šåŒ–æ¤œå‡ºï¼‰
        logger.info(f"ğŸ”¥ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹ï¼ˆé©å¿œçš„ï¼‰")

        window_size = 10
        stability_threshold = 0.1  # CV < 0.1ã§å®‰å®šã¨åˆ¤å®š

        for i in range(warmup_calls + measurement_calls):
            start = time.perf_counter()
            session.run(None, {'input': dummy_input})
            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            all_latencies.append(latency_ms)

            # å®‰å®šåŒ–åˆ¤å®šï¼ˆæœ€ä½10å›å®Ÿè¡Œå¾Œï¼‰
            if warmup_completed_at is None and len(all_latencies) >= window_size:
                recent = all_latencies[-window_size:]
                mean_lat = np.mean(recent)
                std_lat = np.std(recent)
                cv = std_lat / mean_lat if mean_lat > 0 else 1.0

                if cv < stability_threshold:
                    warmup_completed_at = len(all_latencies)
                    logger.info(
                        f"âœ… ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†ï¼ˆ{warmup_completed_at}å›ã€CV={cv:.3f}ï¼‰"
                    )
                    # æ¸¬å®šç¶™ç¶šï¼ˆæ®‹ã‚Šå›æ•°ï¼‰
                    remaining = measurement_calls - (len(all_latencies) - warmup_completed_at)
                    logger.info(f"ğŸ“Š æ¸¬å®šç¶™ç¶šï¼ˆæ®‹ã‚Š{remaining}å›ï¼‰")

        # é©å¿œçš„ã§å®‰å®šåŒ–ã—ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if warmup_completed_at is None:
            warmup_completed_at = warmup_calls
            logger.warning(
                f"âš ï¸ å®‰å®šåŒ–æœªæ¤œå‡ºã€å›ºå®š{warmup_calls}å›ã§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—çµ‚äº†"
            )

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã¨æ¸¬å®šã®åˆ†é›¢
    warmup_latencies = all_latencies[:warmup_completed_at]
    measurement_latencies = all_latencies[warmup_completed_at:]

    # çµ±è¨ˆè¨ˆç®—ï¼ˆæ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
    measurement_array = np.array(measurement_latencies)

    results = {
        'mean_ms': np.mean(measurement_array),
        'p50_ms': np.percentile(measurement_array, 50),
        'p95_ms': np.percentile(measurement_array, 95),
        'p99_ms': np.percentile(measurement_array, 99),
        'warmup_completed_at': warmup_completed_at,
        'warmup_latencies': warmup_latencies,
        'measurement_latencies': measurement_latencies,
        'num_warmup': len(warmup_latencies),
        'num_measurement': len(measurement_latencies)
    }

    logger.info(f"ğŸ“Š ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·çµ±è¨ˆï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é™¤å¤–å¾Œï¼‰:")
    logger.info(f"   - p50: {results['p50_ms']:.2f}ms")
    logger.info(f"   - p95: {results['p95_ms']:.2f}ms")
    logger.info(f"   - p99: {results['p99_ms']:.2f}ms")
    logger.info(f"   - ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é™¤å¤–: {results['num_warmup']}å›")

    # SLAåˆ¤å®š
    if results['p95_ms'] > 10.0:
        logger.warning(f"âš ï¸ p95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç›®æ¨™æœªé”: {results['p95_ms']:.2f}ms > 10ms")

    return results


def visualize_warmup_effect(results: Dict[str, Any]):
    """
    ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŠ¹æœã®å¯è¦–åŒ–

    Args:
        results: benchmark_latency_with_warmup()ã®çµæœ
    """
    import matplotlib.pyplot as plt

    warmup_idx = results['warmup_completed_at']
    all_lat = results['warmup_latencies'] + results['measurement_latencies']

    plt.figure(figsize=(12, 5))

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: å…¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    plt.subplot(1, 2, 1)
    plt.plot(all_lat, marker='o', markersize=2, alpha=0.6)
    plt.axvline(warmup_idx, color='red', linestyle='--', label=f'Warmupå®Œäº† (call={warmup_idx})')
    plt.axhline(results['p95_ms'], color='green', linestyle='--', label=f'p95={results["p95_ms"]:.2f}ms')
    plt.xlabel('Call #')
    plt.ylabel('Latency (ms)')
    plt.title('Latency Over Time (with Warmup)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æ¯”è¼ƒ
    plt.subplot(1, 2, 2)
    plt.hist(results['warmup_latencies'], bins=30, alpha=0.5, label='Warmup', color='orange')
    plt.hist(results['measurement_latencies'], bins=30, alpha=0.5, label='Measurement', color='blue')
    plt.axvline(results['p95_ms'], color='green', linestyle='--', label=f'p95={results["p95_ms"]:.2f}ms')
    plt.xlabel('Latency (ms)')
    plt.ylabel('Count')
    plt.title('Latency Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('latency_warmup_analysis.png', dpi=150)
    logger.info("ğŸ“Š ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åˆ†æã‚°ãƒ©ãƒ•ä¿å­˜: latency_warmup_analysis.png")


# ä½¿ç”¨ä¾‹
if __name__ == '__main__':
    # å›ºå®šã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    results_fixed = benchmark_latency_with_warmup(
        'models/fx_model.onnx',
        input_shape=(1, 5, 360, 52),
        warmup_calls=20,
        measurement_calls=1000,
        warmup_detection='fixed'
    )

    # é©å¿œçš„ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    results_adaptive = benchmark_latency_with_warmup(
        'models/fx_model.onnx',
        input_shape=(1, 5, 360, 52),
        warmup_calls=50,  # æœ€å¤§ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        measurement_calls=1000,
        warmup_detection='adaptive'
    )

    # å¯è¦–åŒ–
    visualize_warmup_effect(results_fixed)
```

**ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¨å¥¨è¨­å®š**:

| å®Ÿè¡Œç’°å¢ƒ | warmup_calls | warmup_detection | ç†ç”± |
|---------|--------------|------------------|------|
| GPU (åˆå›èµ·å‹•) | 30-50 | adaptive | ã‚«ãƒ¼ãƒãƒ«åˆæœŸåŒ–ãƒ»ãƒ¡ãƒ¢ãƒªè»¢é€ãŒä¸å®‰å®š |
| GPU (2å›ç›®ä»¥é™) | 10-20 | fixed | æ—¢ã«ã‚¦ã‚©ãƒ¼ãƒ ãªã®ã§å›ºå®šã§ååˆ† |
| CPU | 10-15 | fixed | CPUã¯å®‰å®šãŒæ—©ã„ |
| Quantized (INT8) | 20-30 | adaptive | é‡å­åŒ–æ¼”ç®—ã®åˆæœŸåŒ–ã«æ™‚é–“ |

**æˆåŠŸæŒ‡æ¨™**:
- ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é™¤å¤–ç‡: < 5%ï¼ˆéå‰°é™¤å¤–é˜²æ­¢ï¼‰
- æ¸¬å®šã‚µãƒ³ãƒ—ãƒ«æ•°: â‰¥ 1000å›
- CV(measurement_latencies) < 0.15ï¼ˆå®‰å®šæ€§ï¼‰

**æ¤œè¨¼**:
```python
def test_warmup_exclusion():
    """ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é™¤å¤–ã®æ¤œè¨¼"""
    # ãƒ€ãƒŸãƒ¼çµæœ
    results = benchmark_latency_with_warmup(
        'test_model.onnx',
        (1, 5, 360, 52),
        warmup_calls=20,
        measurement_calls=100
    )

    # æ¤œè¨¼1: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãŒé™¤å¤–ã•ã‚Œã¦ã„ã‚‹
    assert results['num_warmup'] == 20
    assert results['num_measurement'] == 100

    # æ¤œè¨¼2: æ¸¬å®šãƒ‡ãƒ¼ã‚¿ã®ã¿ã§çµ±è¨ˆè¨ˆç®—
    measurement_p95 = np.percentile(results['measurement_latencies'], 95)
    assert abs(measurement_p95 - results['p95_ms']) < 0.01

    # æ¤œè¨¼3: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é™¤å¤–ã«ã‚ˆã‚‹æ”¹å–„ç¢ºèª
    warmup_mean = np.mean(results['warmup_latencies'])
    measurement_mean = results['mean_ms']

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã¯é€šå¸¸é…ã„
    assert warmup_mean > measurement_mean, "ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãŒæ¸¬å®šã‚ˆã‚Šé…ã„ã¹ã"
```

---

## ğŸ“Š ç²¾åº¦åŠ£åŒ–æ¤œè¨¼

### PyTorch vs ONNXæ¯”è¼ƒ

```python
def validate_accuracy(
    pytorch_model: torch.nn.Module,
    onnx_path: str,
    test_data: np.ndarray,
    test_targets: np.ndarray
) -> Dict[str, float]:
    """
    PyTorchãƒ¢ãƒ‡ãƒ«ã¨ONNXãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’æ¯”è¼ƒ
    
    è¨±å®¹ç¯„å›²: RMSEåŠ£åŒ– < 1%
    
    Args:
        test_data: (N, 5, 360, 52)
        test_targets: (N, 36)
    
    Returns:
        {
            'pytorch_rmse': float,
            'onnx_rmse': float,
            'degradation_pct': float,
            'accept': bool
        }
    """
    # PyTorchæ¨è«–
    pytorch_model.eval()
    with torch.no_grad():
        pytorch_pred = pytorch_model(
            torch.from_numpy(test_data).float()
        ).numpy()
    
    pytorch_rmse = np.sqrt(mean_squared_error(test_targets, pytorch_pred))
    
    # ONNXæ¨è«–
    session = ort.InferenceSession(onnx_path)
    onnx_pred = session.run(None, {'input': test_data})[0]
    
    onnx_rmse = np.sqrt(mean_squared_error(test_targets, onnx_pred))
    
    # åŠ£åŒ–ç‡
    degradation = (onnx_rmse - pytorch_rmse) / pytorch_rmse * 100
    
    results = {
        'pytorch_rmse': pytorch_rmse,
        'onnx_rmse': onnx_rmse,
        'degradation_pct': degradation,
        'accept': degradation < 1.0
    }
    
    logger.info(f"ğŸ“Š ç²¾åº¦æ¯”è¼ƒ:")
    logger.info(f"   PyTorch RMSE: {pytorch_rmse:.4f}")
    logger.info(f"   ONNX RMSE: {onnx_rmse:.4f}")
    logger.info(f"   åŠ£åŒ–: {degradation:.2f}%")
    
    if not results['accept']:
        logger.warning(f"âš ï¸ ç²¾åº¦åŠ£åŒ–ãŒè¨±å®¹ç¯„å›²ã‚’è¶…é")
    
    return results
```

### ONNXã‚¹ã‚±ãƒ¼ãƒ«äºŒé‡æ›ç®—é˜²æ­¢

**ç›®çš„**: 2é‡pips conversionã¯ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆåç›Šè©•ä¾¡ç ´ç¶»ã—ãƒ¢ãƒ‡ãƒ«æ˜‡æ ¼èª¤åˆ¤å®šã€‚

**å®Ÿè£…**: ONNXã‚°ãƒ©ãƒ•æœ«ç«¯scaleãƒãƒ¼ãƒ‰æœ‰ç„¡æ¤œè¨¼ + metadata.scaling_applied + runtime assert

```python
def verify_output_scaling(
    onnx_path: str,
    check_graph: bool = True,
    check_metadata: bool = True
) -> Dict[str, Any]:
    """
    ONNXå‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨çŠ¶æ…‹ã‚’æ¤œè¨¼

    Args:
        onnx_path: ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        check_graph: ã‚°ãƒ©ãƒ•æ§‹é€ ã‹ã‚‰ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰æ¤œå‡º
        check_metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰scaling_appliedç¢ºèª

    Returns:
        {
            'has_scale_node': bool,
            'metadata_scaling_applied': bool | None,
            'safe': bool,  # True=å®‰å…¨ï¼ˆäºŒé‡é©ç”¨ãªã—ï¼‰
            'warnings': List[str]
        }
    """
    import onnx
    from onnx import helper

    model = onnx.load(onnx_path)
    result = {
        'has_scale_node': False,
        'metadata_scaling_applied': None,
        'safe': True,
        'warnings': []
    }

    # 1. ã‚°ãƒ©ãƒ•æ§‹é€ ãƒã‚§ãƒƒã‚¯
    if check_graph:
        # å‡ºåŠ›ãƒãƒ¼ãƒ‰ç›´å‰ã®Opæ¤œç´¢
        output_nodes = []
        output_names = [o.name for o in model.graph.output]

        for node in model.graph.node:
            if any(out in output_names for out in node.output):
                output_nodes.append(node)

        # ã‚¹ã‚±ãƒ¼ãƒ«é–¢é€£Opæ¤œå‡ºï¼ˆMul, Div, Sub, Addç­‰ï¼‰
        scale_ops = ['Mul', 'Div', 'Add', 'Sub']
        for node in output_nodes:
            if node.op_type in scale_ops:
                result['has_scale_node'] = True
                result['warnings'].append(
                    f"å‡ºåŠ›ç›´å‰ã«ã‚¹ã‚±ãƒ¼ãƒ«Opæ¤œå‡º: {node.op_type} (name={node.name})"
                )
                break

    # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
    if check_metadata:
        metadata_dict = {prop.key: prop.value for prop in model.metadata_props}

        if 'scaling_applied' in metadata_dict:
            result['metadata_scaling_applied'] = (
                metadata_dict['scaling_applied'].lower() == 'true'
            )

            if result['metadata_scaling_applied'] and result['has_scale_node']:
                result['safe'] = False
                result['warnings'].append(
                    "âš ï¸ äºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨æ‡¸å¿µ: "
                    "metadata.scaling_applied=true ã‹ã¤ ã‚°ãƒ©ãƒ•ã«ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰å­˜åœ¨"
                )
        else:
            result['warnings'].append(
                "metadata.scaling_appliedæœªè¨­å®šï¼ˆæ¨å¥¨: æ˜ç¤ºçš„ã«è¨­å®šï¼‰"
            )

    # 3. å®‰å…¨æ€§åˆ¤å®š
    if result['has_scale_node'] and result['metadata_scaling_applied'] is None:
        result['warnings'].append(
            "ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰å­˜åœ¨ã™ã‚‹ãŒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœªè¨­å®š: "
            "runtimeé©ç”¨æ™‚ã«äºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«ãƒªã‚¹ã‚¯"
        )

    return result


def assert_no_double_scaling_at_runtime(
    session: ort.InferenceSession,
    apply_scaling_in_runtime: bool,
    onnx_model_path: str
):
    """
    å®Ÿè¡Œæ™‚ã®ã‚¹ã‚±ãƒ¼ãƒ«äºŒé‡é©ç”¨ã‚’é˜²æ­¢

    Args:
        session: ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³
        apply_scaling_in_runtime: Runtimeã§ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨ã™ã‚‹ã‹
        onnx_model_path: ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ï¼ˆæ¤œè¨¼ç”¨ï¼‰

    Raises:
        AssertionError: äºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨æ‡¸å¿µãŒã‚ã‚‹å ´åˆ
    """
    verification = verify_output_scaling(onnx_model_path)

    # Runtimeé©ç”¨ ã‹ã¤ ã‚°ãƒ©ãƒ•ã«ã‚¹ã‚±ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰å­˜åœ¨
    if apply_scaling_in_runtime and verification['has_scale_node']:
        raise AssertionError(
            "äºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨ã‚¨ãƒ©ãƒ¼: "
            f"Runtimeé©ç”¨={apply_scaling_in_runtime}, "
            f"ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«={verification['has_scale_node']}"
        )

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã®æ•´åˆæ€§ç¢ºèª
    if verification['metadata_scaling_applied']:
        if apply_scaling_in_runtime:
            raise AssertionError(
                "äºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨ã‚¨ãƒ©ãƒ¼: "
                "metadata.scaling_applied=true ã ãŒ Runtimeé©ç”¨ã‚‚æŒ‡å®š"
            )

    logger.info(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ«æ¤œè¨¼å®Œäº†: äºŒé‡é©ç”¨ãªã—")
    logger.info(f"   - ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«: {verification['has_scale_node']}")
    logger.info(f"   - Runtimeé©ç”¨: {apply_scaling_in_runtime}")


# ä½¿ç”¨ä¾‹: ONNXå¤‰æ›æ™‚
def export_onnx_with_scaling_metadata(
    model: torch.nn.Module,
    dummy_input: torch.Tensor,
    output_path: str,
    scaling_applied: bool  # ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã§æ—¢ã«ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿ã‹
):
    """
    ã‚¹ã‚±ãƒ¼ãƒ«çŠ¶æ…‹ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¨˜éŒ²ã—ã¦ONNXå¤‰æ›

    Args:
        scaling_applied: True = ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãŒæ—¢ã«pipsã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿
                        False = rawå€¤ï¼ˆRuntimeå´ã§ã‚¹ã‚±ãƒ¼ãƒ«å¿…è¦ï¼‰
    """
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        input_names=['input'],
        output_names=['output'],
        opset_version=17,
        dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}
    )

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
    import onnx
    onnx_model = onnx.load(output_path)

    # scaling_applied ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    meta = onnx_model.metadata_props.add()
    meta.key = 'scaling_applied'
    meta.value = 'true' if scaling_applied else 'false'

    # èª¬æ˜
    meta_desc = onnx_model.metadata_props.add()
    meta_desc.key = 'scaling_description'
    meta_desc.value = (
        'Output in pips (scaled)' if scaling_applied
        else 'Output in raw units (requires scaling)'
    )

    onnx.save(onnx_model, output_path)

    logger.info(f"âœ… ONNXå¤‰æ›å®Œäº†: scaling_applied={scaling_applied}")


# æ¨è«–æ™‚ã®é©ç”¨ä¾‹
def run_inference_with_scaling_check(
    onnx_path: str,
    input_data: np.ndarray,
    apply_pips_scaling: bool = True,
    pips_multiplier: float = 100.0  # USDJPY: 0.01å††=1pip
) -> np.ndarray:
    """
    ã‚¹ã‚±ãƒ¼ãƒ«æ¤œè¨¼ä»˜ãæ¨è«–

    Args:
        apply_pips_scaling: Runtimeã§pipsã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨ã™ã‚‹ã‹

    Returns:
        predictions (pipsã‚¹ã‚±ãƒ¼ãƒ«)
    """
    session = ort.InferenceSession(onnx_path)

    # äºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«é˜²æ­¢ãƒã‚§ãƒƒã‚¯
    assert_no_double_scaling_at_runtime(
        session,
        apply_pips_scaling,
        onnx_path
    )

    # æ¨è«–
    outputs = session.run(None, {'input': input_data})[0]

    # å¿…è¦ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨
    if apply_pips_scaling:
        outputs = outputs * pips_multiplier

    return outputs
```

**æˆåŠŸæŒ‡æ¨™**:
- ã‚¹ã‚±ãƒ¼ãƒ«äºŒé‡é©ç”¨ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ: 0ä»¶
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®šç‡: 100%
- Runtimeæ¤œè¨¼å®Ÿè¡Œç‡: 100%

**æ¤œè¨¼**:
```python
def test_scaling_verification():
    """ã‚¹ã‚±ãƒ¼ãƒ«æ¤œè¨¼ã®å‹•ä½œç¢ºèª"""
    # ã‚±ãƒ¼ã‚¹1: ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«ã‚ã‚Šã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿true â†’ å®‰å…¨
    # ã‚±ãƒ¼ã‚¹2: ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«ã‚ã‚Šã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿false â†’ è­¦å‘Š
    # ã‚±ãƒ¼ã‚¹3: ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«ãªã—ã€Runtimeé©ç”¨ â†’ å®‰å…¨

    # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«å±¤ã‚ã‚Šï¼‰
    class ModelWithScaling(torch.nn.Module):
        def forward(self, x):
            return x * 100.0  # pipså¤‰æ› (USDJPY)

    model = ModelWithScaling()
    dummy_input = torch.randn(1, 5, 360, 52)

    # ONNXå¤‰æ›ï¼ˆscaling_applied=trueï¼‰
    export_onnx_with_scaling_metadata(
        model, dummy_input, 'test_scaled.onnx', scaling_applied=True
    )

    # æ¤œè¨¼
    result = verify_output_scaling('test_scaled.onnx')
    assert result['has_scale_node'] == True
    assert result['metadata_scaling_applied'] == True
    assert result['safe'] == True  # æ•´åˆæ€§ã‚ã‚Š

    # Runtimeé©ç”¨è©¦è¡Œ â†’ ã‚¨ãƒ©ãƒ¼
    try:
        session = ort.InferenceSession('test_scaled.onnx')
        assert_no_double_scaling_at_runtime(
            session, apply_scaling_in_runtime=True, onnx_model_path='test_scaled.onnx'
        )
        assert False, "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã¹ã"
    except AssertionError as e:
        assert "äºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«" in str(e)
```

---

## ğŸ“ ãƒ­ã‚°å‡ºåŠ›

### æ™‚åˆ»è¡¨ç¤ºãƒ«ãƒ¼ãƒ«
- **å…¨ãƒ­ã‚°**: æ—¥æœ¬æ™‚é–“(JST)ã§è¡¨ç¤º
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: `YYYY-MM-DD HH:MM:SS JST`
- **å¤‰æ›é–‹å§‹/çµ‚äº†æ™‚åˆ»**: æ—¥æœ¬æ™‚é–“ã§æ˜è¨˜

```
ğŸ”„ ç¬¬6æ®µéš: ONNXå¤‰æ›é–‹å§‹ [2025-10-24 04:15:20 JST]
ğŸ“‚ å…¥åŠ›: models/fx_mtf_20251022_150000_training.pt
   - ãƒ¢ãƒ‡ãƒ«: MultiTFLSTMModel
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 1.2M
   - å…¥åŠ›å½¢çŠ¶: (1, 5, 360, 52)
   - å‡ºåŠ›å½¢çŠ¶: (1, 36)

[ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰]
   âœ… æ§‹é€ æ¤œè¨¼å®Œäº†
   âœ… é‡ã¿ãƒ­ãƒ¼ãƒ‰å®Œäº†

[ã‚¹ãƒ†ãƒƒãƒ—2: ONNXå¤‰æ›ï¼ˆFP32ï¼‰]
   - opset_version: 17
   - dynamic_axes: batch_size
   âœ… å¤‰æ›å®Œäº†: models/fx_mtf_20251022_150000_model_fp32.onnx

[ã‚¹ãƒ†ãƒƒãƒ—3: FP16é‡å­åŒ–]
   - ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: 4.8MB â†’ 2.4MBï¼ˆ50%å‰Šæ¸›ï¼‰
   âœ… é‡å­åŒ–å®Œäº†: models/fx_mtf_20251022_150000_model_fp16.onnx

[ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¤œè¨¼]
   - æ¸¬å®šã‚µãƒ³ãƒ—ãƒ«: 1000
   - å¹³å‡: 6.2ms
   - p50: 5.8ms
   - p95: 8.1ms âœ…ï¼ˆç›®æ¨™: <10msï¼‰
   - p99: 9.7ms

[ã‚¹ãƒ†ãƒƒãƒ—5: ç²¾åº¦åŠ£åŒ–æ¤œè¨¼]
   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: 5000ã‚µãƒ³ãƒ—ãƒ«
   - PyTorch RMSE: 0.3245
   - ONNX RMSE: 0.3258
   - åŠ£åŒ–: +0.4% âœ…ï¼ˆç›®æ¨™: <1%ï¼‰

ğŸ’¾ æœ€çµ‚å‡ºåŠ›: models/fx_mtf_20251022_150000_model_fp16.onnx
ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: models/fx_mtf_20251022_150000_onnx_metadata.json
âœ… ç¬¬6æ®µéš: ONNXå¤‰æ›å®Œäº† [2025-10-24 04:18:45 JST]
   å¤‰æ›æ™‚é–“: 3åˆ†25ç§’
```

---

## ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›

### onnx_metadata.json

```json
{
  "conversion": {
    "timestamp": "2025-10-22T15:00:00Z",
    "pytorch_model": "models/fx_mtf_20251022_150000_training.pt",
    "onnx_model": "models/fx_mtf_20251022_150000_model_fp16.onnx",
    "quantization": "FP16",
    "opset_version": 17
  },
  "model": {
    "input_shape": [1, 5, 360, 52],
    "output_shape": [1, 36],
    "parameters": 1200000,
    "size_mb": 2.4
  },
  "performance": {
    "latency_ms": {
      "mean": 6.2,
      "p50": 5.8,
      "p95": 8.1,
      "p99": 9.7
    },
    "target_p95_ms": 10.0,
    "achieved": true
  },
  "accuracy": {
    "pytorch_rmse": 0.3245,
    "onnx_rmse": 0.3258,
    "degradation_pct": 0.4,
    "target_degradation_pct": 1.0,
    "achieved": true
  },
  "validation": {
    "test_samples": 5000,
    "latency_samples": 1000,
    "passed": true
  },
  "scaling": {
    "pips_multiplier": 100.0,
    "scaling_applied": false,
    "notes": "USDJPY: å††â†’pips Ã—100ã€‚ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«æœªé©ç”¨ã€Runtimeé©ç”¨æ¨å¥¨"
  }
}
```

### ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¥‘ç´„ã®æ˜ç¢ºåŒ–

**äºŒé‡pipsã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é˜²æ­¢**: å­¦ç¿’å‡ºåŠ›ã¨ONNXå‡ºåŠ›ã®ã‚¹ã‚±ãƒ¼ãƒ«çŠ¶æ…‹ã‚’æ˜ç¤ºçš„ã«å¥‘ç´„

#### ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ« | metadata.scaling_applied | Runtimeé©ç”¨ | çŠ¶æ…‹ |
|---------|----------------|-------------------------|------------|------|
| **A: ã‚°ãƒ©ãƒ•é©ç”¨æ¸ˆã¿** | âœ“ | `true` | âœ— | âœ… å®‰å…¨ |
| **B: Runtimeé©ç”¨** | âœ— | `false` | âœ“ | âœ… å®‰å…¨ |
| **C: æœªé©ç”¨** | âœ— | `false` | âœ— | âš ï¸ è­¦å‘Šï¼ˆã‚¹ã‚±ãƒ¼ãƒ«å¿˜ã‚Œï¼‰ |
| **D: äºŒé‡é©ç”¨** | âœ“ | `true` | âœ“ | âŒ ã‚¨ãƒ©ãƒ¼ï¼ˆäºŒé‡ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ |

#### æ¨å¥¨æ–¹é‡ï¼ˆPhase 0ï¼‰
- **å­¦ç¿’å‡ºåŠ›**: æ­£è¦åŒ–å€¤ï¼ˆpipsã‚¹ã‚±ãƒ¼ãƒ«æœªé©ç”¨ï¼‰
- **ONNXå¤‰æ›**: ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«å±¤ãªã—ï¼ˆ`scaling_applied: false`ï¼‰
- **MT5 Runtime**: `predicted_pips = output[0] * 100.0` ã§é©ç”¨
- **ç†ç”±**: ãƒ‡ãƒãƒƒã‚°å®¹æ˜“æ€§ã€ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›´ã®æŸ”è»Ÿæ€§

#### ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¿…é ˆé …ç›®
```json
{
  "scaling": {
    "pips_multiplier": 100.0,        // USDJPYç”¨
    "scaling_applied": false,        // ã‚°ãƒ©ãƒ•å†…ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨æœ‰ç„¡
    "notes": "Runtimeé©ç”¨æ¨å¥¨"       // é©ç”¨æ–¹æ³•ã®æ³¨è¨˜
  }
}
```

---

## ğŸš¨ ã‚¨ãƒ©ãƒ¼æ¡ä»¶

| æ¡ä»¶ | é–¾å€¤ | å¯¾å¿œ |
|------|------|------|
| ONNXå¤‰æ›å¤±æ•— | ã‚ªãƒšãƒ¬ãƒ¼ã‚¿éäº’æ› | ã‚¨ãƒ©ãƒ¼çµ‚äº†ï¼ˆãƒ¢ãƒ‡ãƒ«æ§‹é€ ç¢ºèªï¼‰ |
| ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¶…é | p95 > 15ms | è­¦å‘Šï¼ˆINT8é‡å­åŒ–æ¤œè¨ï¼‰ |
| ç²¾åº¦åŠ£åŒ–éå¤§ | > 2% | ã‚¨ãƒ©ãƒ¼çµ‚äº†ï¼ˆé‡å­åŒ–æ–¹æ³•è¦‹ç›´ã—ï¼‰ |
| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºè¶…é | > 10MB | è­¦å‘Šï¼ˆãƒ¢ãƒ‡ãƒ«è»½é‡åŒ–æ¤œè¨ï¼‰ |
| ONNXæ¤œè¨¼å¤±æ•— | å½¢çŠ¶ä¸ä¸€è‡´ | ã‚¨ãƒ©ãƒ¼çµ‚äº†ï¼ˆå¤‰æ›è¨­å®šç¢ºèªï¼‰ |

---

## âš™ï¸ è¨­å®šä¾‹

```yaml
# config/onnx_conversion.yaml
onnx_conversion:
  # å¤‰æ›è¨­å®š
  conversion:
    opset_version: 17  # ONNX Runtime 1.16+
    enable_optimization: true
    dynamic_batch: true
  
  # é‡å­åŒ–
  quantization:
    method: 'FP16'  # FP32 | FP16 | INT8
    calibration_samples: 1000  # INT8ã®ã¿
  
  # æ¤œè¨¼è¨­å®š
  validation:
    latency:
      target_p95_ms: 10.0
      num_samples: 1000
      fail_threshold_ms: 15.0  # ã“ã‚Œã‚’è¶…ãˆãŸã‚‰ã‚¨ãƒ©ãƒ¼
    
    accuracy:
      target_degradation_pct: 1.0
      num_test_samples: 5000
      fail_threshold_pct: 2.0  # ã“ã‚Œã‚’è¶…ãˆãŸã‚‰ã‚¨ãƒ©ãƒ¼
  
  # å‡ºåŠ›è¨­å®š
  output:
    save_fp32: false  # FP32ç‰ˆã‚‚ä¿å­˜
    save_metadata: true
    save_benchmark: true
```

---

## ğŸ”— é–¢é€£ä»•æ§˜æ›¸

- **å‰æ®µéš**:
  - ç¬¬4æ®µéš: [TRAINER_SPEC.md](./TRAINER_SPEC.md) - ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
  - ç¬¬5æ®µéš: [VALIDATOR_SPEC.md](./VALIDATOR_SPEC.md) - ç²¾åº¦æ¤œè¨¼
- **å‚ç…§**:
  - [validator/EXECUTION_LATENCY_SPEC.md](./validator/EXECUTION_LATENCY_SPEC.md) - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®šåŸºæº–
  - [trainer/MULTI_TF_FUSION_SPEC.md](./trainer/MULTI_TF_FUSION_SPEC.md) - ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **ãƒ‡ãƒ—ãƒ­ã‚¤å…ˆ**: MT5 Expert Advisorï¼ˆMQL5ï¼‰

---

## ğŸ“Œ æ³¨æ„äº‹é …

### 1. é‡å­åŒ–æˆ¦ç•¥ã®é¸æŠ

| æ–¹æ³• | ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | æ¨è«–é€Ÿåº¦ | ç²¾åº¦åŠ£åŒ– | æ¨å¥¨ç”¨é€” |
|------|------------|---------|---------|---------|
| FP32 | 100% | 1.0x | 0% | é–‹ç™ºãƒ»æ¤œè¨¼ |
| FP16 | 50% | 1.5-2x | <0.5% | **æœ¬ç•ªæ¨å¥¨** |
| INT8 | 25% | 2-4x | 0.5-2% | è¶…é«˜é€Ÿæ¨è«– |

**æ¨å¥¨**: FP16ï¼ˆç²¾åº¦ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®ãƒãƒ©ãƒ³ã‚¹æœ€é©ï¼‰

### 2. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç›®æ¨™

```
p95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· < 10ms

ç†ç”±:
- MT5 Expert Advisorã®1ãƒ†ã‚£ãƒƒã‚¯å‡¦ç†æ™‚é–“åˆ¶ç´„
- ãƒãƒ«ãƒTFï¼ˆ5ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã®ä¸¦åˆ—å‡¦ç†ã‚’è€ƒæ…®
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ï¼ˆ2-5msï¼‰ã‚’å«ã‚ã¦ã‚‚åˆè¨ˆ <20ms
```

### 3. ONNX Runtimeè¨­å®š

```python
# GPUæ¨è«–ï¼ˆæ¨å¥¨ï¼‰
session = ort.InferenceSession(
    onnx_path,
    providers=[
        ('CUDAExecutionProvider', {
            'device_id': 0,
            'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # 2GB
        }),
        'CPUExecutionProvider'  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    ]
)

# CPUæ¨è«–ï¼ˆé–‹ç™ºç”¨ï¼‰
session = ort.InferenceSession(
    onnx_path,
    providers=['CPUExecutionProvider']
)
```

### 4. æœªæ¥ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼ˆå†ç¢ºèªï¼‰

ONNXå¤‰æ›æ™‚ã‚‚**æœªæ¥ãƒªãƒ¼ã‚¯**ãŒæ··å…¥ã—ãªã„ã‚ˆã†ç¢ºèª:

```python
# âœ… OK: éå»360æœ¬ã‹ã‚‰36æœ¬å…ˆã‚’äºˆæ¸¬
input: (batch, 5, 360, 52)  # t-359 ~ t
output: (batch, 36)          # t+1 ~ t+36

# âŒ NG: æœªæ¥ãƒ‡ãƒ¼ã‚¿ãŒæ··å…¥
# ç¬¬2æ®µéšç‰¹å¾´é‡è¨ˆç®—ã§ shift(-n) ã‚’ä½¿ç”¨ã—ã¦ã„ãªã„ã‹ç¢ºèª
```

---

## é‹ç”¨æœ€é©åŒ–

### INT8æ ¡æ­£ã‚µãƒ³ãƒ—ãƒ«é¸å®šåŸºæº–

**ç›®çš„**: ãƒ©ãƒ³ãƒ€ãƒ æ ¡æ­£ã‚µãƒ³ãƒ—ãƒ«ã¯æ¥µç«¯å€¤ã‚’è¦‹é€ƒã—é‡å­åŒ–èª¤å·®ãŒå¢—å¤§

**è§£æ±ºç­–**: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£regimeãƒ»ä¾¡æ ¼ãƒ¬ãƒ³ã‚¸ã‚’ç¶²ç¾…ã™ã‚‹æˆ¦ç•¥çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
class CalibrationSampleSelector:
    """INT8æ ¡æ­£ç”¨ã‚µãƒ³ãƒ—ãƒ«é¸å®š"""
    
    def __init__(self, config: dict):
        self.n_samples = config.get("calibration_samples", 500)
        self.stratify_by_volatility = config.get("stratify_volatility", True)
        self.stratify_by_price_level = config.get("stratify_price_level", True)
        self.include_extremes = config.get("include_extremes", True)
    
    def select_samples(
        self,
        features: np.ndarray,  # (N, 5, 360, F)
        metadata: pd.DataFrame  # time, atr_h1, close_h1
    ) -> np.ndarray:
        """
        æˆ¦ç•¥çš„æ ¡æ­£ã‚µãƒ³ãƒ—ãƒ«é¸å®š
        
        Returns:
            selected_indices: shape (n_samples,)
        """
        N = features.shape[0]
        indices = []
        
        # 1. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£regimeå±¤åˆ¥ï¼ˆ50%ï¼‰
        if self.stratify_by_volatility:
            atr_values = metadata['atr_h1'].values
            atr_quantiles = [0.1, 0.3, 0.5, 0.7, 0.9]
            
            n_per_regime = int(self.n_samples * 0.5 / len(atr_quantiles))
            
            for i in range(len(atr_quantiles) - 1):
                q_low = np.quantile(atr_values, atr_quantiles[i])
                q_high = np.quantile(atr_values, atr_quantiles[i+1])
                
                regime_mask = (atr_values >= q_low) & (atr_values < q_high)
                regime_indices = np.where(regime_mask)[0]
                
                if len(regime_indices) > 0:
                    sampled = np.random.choice(
                        regime_indices,
                        size=min(n_per_regime, len(regime_indices)),
                        replace=False
                    )
                    indices.extend(sampled.tolist())
        
        # 2. ä¾¡æ ¼ãƒ¬ãƒ™ãƒ«å±¤åˆ¥ï¼ˆ30%ï¼‰
        if self.stratify_by_price_level:
            close_values = metadata['close_h1'].values
            price_quantiles = [0.0, 0.25, 0.5, 0.75, 1.0]
            
            n_per_level = int(self.n_samples * 0.3 / (len(price_quantiles) - 1))
            
            for i in range(len(price_quantiles) - 1):
                q_low = np.quantile(close_values, price_quantiles[i])
                q_high = np.quantile(close_values, price_quantiles[i+1])
                
                level_mask = (close_values >= q_low) & (close_values < q_high)
                level_indices = np.where(level_mask)[0]
                
                if len(level_indices) > 0:
                    sampled = np.random.choice(
                        level_indices,
                        size=min(n_per_level, len(level_indices)),
                        replace=False
                    )
                    indices.extend(sampled.tolist())
        
        # 3. æ¥µç«¯å€¤ï¼ˆ20%ï¼‰
        if self.include_extremes:
            n_extremes = int(self.n_samples * 0.2)
            
            # å„ç‰¹å¾´é‡ã®æ¥µç«¯å€¤ã‚’æŠ½å‡º
            feature_means = features.mean(axis=(1, 2, 3))  # (N,)
            extreme_indices = np.argsort(np.abs(feature_means - feature_means.mean()))[-n_extremes:]
            indices.extend(extreme_indices.tolist())
        
        # é‡è¤‡å‰Šé™¤ãƒ»ã‚µãƒ³ãƒ—ãƒ«æ•°èª¿æ•´
        indices = list(set(indices))
        if len(indices) > self.n_samples:
            indices = np.random.choice(indices, size=self.n_samples, replace=False).tolist()
        elif len(indices) < self.n_samples:
            # ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ è£œå……
            remaining = list(set(range(N)) - set(indices))
            additional = np.random.choice(remaining, size=self.n_samples - len(indices), replace=False)
            indices.extend(additional.tolist())
        
        logger.info(f"æ ¡æ­£ã‚µãƒ³ãƒ—ãƒ«é¸å®š: {len(indices)}ä»¶")
        logger.info(f"  - ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å±¤åˆ¥: {int(self.n_samples * 0.5)}ä»¶")
        logger.info(f"  - ä¾¡æ ¼ãƒ¬ãƒ™ãƒ«å±¤åˆ¥: {int(self.n_samples * 0.3)}ä»¶")
        logger.info(f"  - æ¥µç«¯å€¤: {int(self.n_samples * 0.2)}ä»¶")
        
        return np.array(indices)


# ä½¿ç”¨ä¾‹
selector = CalibrationSampleSelector({
    "calibration_samples": 500,
    "stratify_volatility": True,
    "stratify_price_level": True,
    "include_extremes": True
})

calibration_indices = selector.select_samples(features, metadata)
calibration_data = features[calibration_indices]

# INT8é‡å­åŒ–
quantizer.calibrate(calibration_data)
```

**é¸å®šåŸºæº–**:

| å±¤ | å‰²åˆ | ç›®çš„ |
|----|------|------|
| ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£regime | 50% | é™ç©ãƒ»é€šå¸¸ãƒ»æ¿€å‹•æœŸã‚’ç¶²ç¾… |
| ä¾¡æ ¼ãƒ¬ãƒ™ãƒ« | 30% | é«˜å€¤ãƒ»å®‰å€¤åœã®åˆ†å¸ƒå†ç¾ |
| æ¥µç«¯å€¤ | 20% | å¤–ã‚Œå€¤ã®é‡å­åŒ–èª¤å·®æŠ‘åˆ¶ |

**KPIï¼ˆé …ç›®17ï¼‰**:
- INT8ç²¾åº¦åŠ£åŒ–: <1.5%ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”è¼ƒã§æ”¹å–„ï¼‰
- ãƒœãƒ©regimeç¶²ç¾…ç‡: 5æ®µéšã™ã¹ã¦â‰¥10%
- æ¥µç«¯å€¤ã‚«ãƒãƒ¼: p99.5ä»¥ä¸Šã®ã‚µãƒ³ãƒ—ãƒ«â‰¥20ä»¶

---

### INT8 A/Bæ¯”è¼ƒæ‰‹é †

**ç›®çš„**: FP16ã¨INT8ã®ç²¾åº¦å·®ãƒ»é€Ÿåº¦å·®ãŒä¸æ˜ã§å°å…¥åˆ¤æ–­ã§ããš

**è§£æ±ºç­–**: æ¨™æº–åŒ–A/Bæ¯”è¼ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«

```python
class INT8ABTestProtocol:
    """INT8 vs FP16 A/Bæ¯”è¼ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«"""
    
    def __init__(self, config: dict):
        self.test_samples = config.get("ab_test_samples", 10000)
        self.latency_iterations = config.get("latency_iterations", 1000)
        self.accuracy_threshold = config.get("accuracy_degradation_threshold", 1.5)  # %
        self.latency_improvement_target = config.get("latency_improvement_target", 30)  # %
    
    def run_ab_test(
        self,
        model_fp16_path: str,
        model_int8_path: str,
        test_data: np.ndarray,
        test_targets: np.ndarray
    ) -> Dict[str, Any]:
        """
        A/Bæ¯”è¼ƒå®Ÿè¡Œ
        
        Returns:
            {
                "accuracy": {
                    "fp16_mae": float,
                    "int8_mae": float,
                    "degradation_%": float,
                    "pass": bool
                },
                "latency": {
                    "fp16_p95_ms": float,
                    "int8_p95_ms": float,
                    "improvement_%": float,
                    "pass": bool
                },
                "recommendation": "adopt" | "reject"
            }
        """
        import onnxruntime as ort
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
        session_fp16 = ort.InferenceSession(model_fp16_path)
        session_int8 = ort.InferenceSession(model_int8_path)
        
        # 1. ç²¾åº¦æ¯”è¼ƒ
        pred_fp16 = []
        pred_int8 = []
        
        for i in range(len(test_data)):
            sample = test_data[i:i+1]
            
            pred_fp16.append(
                session_fp16.run(None, {"input": sample})[0]
            )
            pred_int8.append(
                session_int8.run(None, {"input": sample})[0]
            )
        
        pred_fp16 = np.concatenate(pred_fp16, axis=0)
        pred_int8 = np.concatenate(pred_int8, axis=0)
        
        mae_fp16 = np.abs(pred_fp16 - test_targets).mean()
        mae_int8 = np.abs(pred_int8 - test_targets).mean()
        
        accuracy_degradation = (mae_int8 - mae_fp16) / mae_fp16 * 100
        accuracy_pass = accuracy_degradation < self.accuracy_threshold
        
        # 2. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒ
        latency_fp16 = []
        latency_int8 = []
        
        sample = test_data[0:1]  # å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«
        
        for _ in range(self.latency_iterations):
            start = time.perf_counter()
            session_fp16.run(None, {"input": sample})
            latency_fp16.append((time.perf_counter() - start) * 1000)
        
        for _ in range(self.latency_iterations):
            start = time.perf_counter()
            session_int8.run(None, {"input": sample})
            latency_int8.append((time.perf_counter() - start) * 1000)
        
        fp16_p95 = np.percentile(latency_fp16, 95)
        int8_p95 = np.percentile(latency_int8, 95)
        
        latency_improvement = (fp16_p95 - int8_p95) / fp16_p95 * 100
        latency_pass = latency_improvement >= self.latency_improvement_target
        
        # 3. åˆ¤å®š
        recommendation = "adopt" if (accuracy_pass and latency_pass) else "reject"
        
        result = {
            "accuracy": {
                "fp16_mae": mae_fp16,
                "int8_mae": mae_int8,
                "degradation_%": accuracy_degradation,
                "pass": accuracy_pass
            },
            "latency": {
                "fp16_p95_ms": fp16_p95,
                "int8_p95_ms": int8_p95,
                "improvement_%": latency_improvement,
                "pass": latency_pass
            },
            "recommendation": recommendation
        }
        
        logger.info(f"INT8 A/Bæ¯”è¼ƒçµæœ:")
        logger.info(f"  - ç²¾åº¦åŠ£åŒ–: {accuracy_degradation:.2f}% (é–¾å€¤={self.accuracy_threshold}%)")
        logger.info(f"  - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ”¹å–„: {latency_improvement:.2f}% (ç›®æ¨™={self.latency_improvement_target}%)")
        logger.info(f"  - æ¨å¥¨: {recommendation}")
        
        return result


# ä½¿ç”¨ä¾‹
ab_test = INT8ABTestProtocol({
    "ab_test_samples": 10000,
    "latency_iterations": 1000,
    "accuracy_degradation_threshold": 1.5,  # 1.5%ã¾ã§è¨±å®¹
    "latency_improvement_target": 30  # 30%æ”¹å–„å¿…é ˆ
})

result = ab_test.run_ab_test(
    model_fp16_path="models/model_fp16.onnx",
    model_int8_path="models/model_int8.onnx",
    test_data=test_features,
    test_targets=test_targets
)

if result["recommendation"] == "adopt":
    logger.info("INT8ãƒ¢ãƒ‡ãƒ«æ¡ç”¨ã‚’æ¨å¥¨")
else:
    logger.warning("INT8ãƒ¢ãƒ‡ãƒ«ä¸æ¡ç”¨: åŸºæº–æœªé”")
```

**æ¯”è¼ƒé …ç›®**:

| æŒ‡æ¨™ | FP16 | INT8 | åˆ¤å®šåŸºæº– |
|------|------|------|---------|
| MAEç²¾åº¦ | baseline | +X% | X < 1.5% |
| p95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | baseline | -Y% | Y â‰¥ 30% |
| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | 100% | ~25% | - |

**KPIï¼ˆé …ç›®76ï¼‰**:
- ç²¾åº¦åŠ£åŒ–: <1.5%
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ”¹å–„: â‰¥30%
- A/Båˆ¤å®šã®å†ç¾æ€§: 5å›å®Ÿè¡Œã§åŒä¸€çµè«–

---

## ğŸ”® å®Ÿè£…è¨ˆç”»

### Phase 4: ONNXå¤‰æ›å®Ÿè£…ï¼ˆ1é€±é–“ï¼‰
- [ ] FP32å¤‰æ›å®Ÿè£…
- [ ] FP16é‡å­åŒ–å®Ÿè£…
- [ ] ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®šå®Ÿè£…
- [ ] ç²¾åº¦åŠ£åŒ–æ¤œè¨¼å®Ÿè£…

### Phase 4-2: æœ€é©åŒ–ï¼ˆ1é€±é–“ï¼‰
- [ ] ONNXã‚°ãƒ©ãƒ•æœ€é©åŒ–
- [ ] INT8é‡å­åŒ–å®Ÿè£…ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- [ ] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›å®Ÿè£…
- [ ] å˜ä½“ãƒ†ã‚¹ãƒˆä½œæˆ

### Phase 4-3: MT5çµ±åˆæº–å‚™ï¼ˆ1é€±é–“ï¼‰
- [ ] MQL5ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ä½œæˆ
- [ ] ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™
- [ ] ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœå…¬é–‹

---

## ğŸ“š å‚è€ƒè³‡æ–™

### ONNX Runtime
- å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://onnxruntime.ai/docs/
- é‡å­åŒ–ã‚¬ã‚¤ãƒ‰: https://onnxruntime.ai/docs/performance/quantization.html
- GPUå®Ÿè¡Œ: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html

### PyTorch ONNX Export
- å…¬å¼ã‚¬ã‚¤ãƒ‰: https://pytorch.org/docs/stable/onnx.html
- ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html

### MT5 ONNXçµ±åˆ
- MQL5 ONNXé–¢æ•°: https://www.mql5.com/en/docs/integration/onnx

---

**æœ€çµ‚æ›´æ–°**: 2025-10-22  
**æ‰¿èªè€…**: (æœªæ‰¿èª)  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: ãƒ‰ãƒ©ãƒ•ãƒˆ

### FP16å®‰å®šæ€§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œå‡º

**ç›®çš„**: FP16æ¨è«–æ™‚ã€æ•°å€¤ä¸å®‰å®šã«ã‚ˆã‚‹NaN/Infç™ºç”Ÿã§silent failure

**è§£æ±ºç­–**: è‡ªå‹•ç²¾åº¦åˆ‡æ›¿æ©Ÿæ§‹ï¼ˆFP16 â‡„ FP32ï¼‰

```python
class FP16StabilityMonitor:
    """FP16å®‰å®šæ€§ç›£è¦–"""
    
    def __init__(self, config: dict):
        self.consecutive_clean_threshold = config.get("consecutive_clean", 10)
        self.fallback_cooldown = config.get("fallback_cooldown_sec", 60)
        
        self.current_precision = "fp16"
        self.consecutive_clean_count = 0
        self.last_fallback_time = 0
        self.nan_detected_count = 0
    
    def check_output(self, prediction: np.ndarray) -> Tuple[bool, str]:
        """
        æ¨è«–å‡ºåŠ›ã®æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
        
        Returns:
            (is_valid, precision_to_use)
        """
        has_nan = np.isnan(prediction).any()
        has_inf = np.isinf(prediction).any()
        
        if has_nan or has_inf:
            self.nan_detected_count += 1
            self.consecutive_clean_count = 0
            
            # FP32ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.current_precision == "fp16":
                logger.warning(f"FP16ã§NaN/Infæ¤œå‡º â†’ FP32ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ¤œå‡º{self.nan_detected_count}å›ç›®ï¼‰")
                self.current_precision = "fp32"
                self.last_fallback_time = time.time()
            
            return (False, "fp32")
        
        else:
            # Cleanå‡ºåŠ›
            self.consecutive_clean_count += 1
            
            # FP16å¾©å¸°æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if self.current_precision == "fp32":
                cooldown_elapsed = (time.time() - self.last_fallback_time) > self.fallback_cooldown
                
                if self.consecutive_clean_count >= self.consecutive_clean_threshold and cooldown_elapsed:
                    logger.info(f"é€£ç¶š{self.consecutive_clean_count}å›clean â†’ FP16å¾©å¸°")
                    self.current_precision = "fp16"
                    self.consecutive_clean_count = 0
            
            return (True, self.current_precision)
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆå–å¾—"""
        return {
            "current_precision": self.current_precision,
            "nan_detected_count": self.nan_detected_count,
            "consecutive_clean_count": self.consecutive_clean_count
        }


# æ¨è«–ãƒ«ãƒ¼ãƒ—çµ±åˆä¾‹
stability_monitor = FP16StabilityMonitor({
    "consecutive_clean": 10,
    "fallback_cooldown_sec": 60
})

# ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆFP16 + FP32ä¸¡æ–¹ç”¨æ„ï¼‰
session_fp16 = ort.InferenceSession("model_fp16.onnx", providers=["CUDAExecutionProvider"])
session_fp32 = ort.InferenceSession("model_fp32.onnx", providers=["CUDAExecutionProvider"])

while True:
    features = get_latest_features()
    
    # ç¾åœ¨ã®ç²¾åº¦ã§æ¨è«–
    if stability_monitor.current_precision == "fp16":
        prediction = session_fp16.run(None, {"input": features})[0]
    else:
        prediction = session_fp32.run(None, {"input": features})[0]
    
    # å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
    is_valid, precision_to_use = stability_monitor.check_output(prediction)
    
    if not is_valid:
        # FP32ã§å†å®Ÿè¡Œ
        prediction = session_fp32.run(None, {"input": features})[0]
    
    execute_trade(prediction)
```

**ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥**:
1. NaN/Infæ¤œå‡º â†’ å³åº§ã«FP32åˆ‡æ›¿
2. é€£ç¶š10å›cleanå‡ºåŠ› + 60ç§’çµŒé â†’ FP16å¾©å¸°
3. FP32ä¸­ã‚‚ç›£è¦–ç¶™ç¶š

**KPIï¼ˆé …ç›®28ï¼‰**: NaNç™ºç”Ÿç‡=0ï¼ˆæ¤œå‡ºå¾Œå³åº§ã«FP32ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

---
