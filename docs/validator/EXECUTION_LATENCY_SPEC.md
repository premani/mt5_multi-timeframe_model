# EXECUTION_LATENCY_SPEC.md

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**æ›´æ–°æ—¥**: 2025-10-21  
**è²¬ä»»è€…**: core-team

---

## ğŸ“‹ ç›®çš„

M1ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ç’°å¢ƒã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã«ãŠã‘ã‚‹**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶**ã¨**æœ€é©åŒ–æˆ¦ç•¥**ã‚’å®šç¾©ã—ã€ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰ï¼ˆscalp/swingï¼‰ãƒˆãƒ¬ãƒ¼ãƒ‰æˆ¦ç•¥ã«ãŠã„ã¦æœŸå¾…å€¤åŠ£åŒ–ã‚’é˜²ãå®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¢ºä¿ã™ã‚‹ã€‚

---

## ğŸ¯ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶ï¼ˆSLO: Service Level Objectivesï¼‰

### å…¨ä½“ç›®æ¨™

```yaml
inference_latency_slo:
  # End-to-End ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆãƒ‡ãƒ¼ã‚¿å—ä¿¡â†’äºˆæ¸¬å‡ºåŠ›ï¼‰
  total:
    p50: < 5.0 ms     # ä¸­å¤®å€¤
    p95: < 10.0 ms    # 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
    p99: < 20.0 ms    # 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
    max: < 50.0 ms    # æœ€å¤§å€¤ï¼ˆç•°å¸¸æ™‚ï¼‰
  
  # Fast Pathï¼ˆé€šå¸¸å®Ÿè¡Œãƒ‘ã‚¹ï¼‰
  fast_path:
    p50: < 3.0 ms
    p95: < 8.0 ms
    max: < 15.0 ms
  
  # Slow Pathï¼ˆãƒ•ãƒ«å†è¨ˆç®—ãŒå¿…è¦ãªå ´åˆï¼‰
  slow_path:
    p95: < 30.0 ms
    max: < 100.0 ms
```

### ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰åˆ¥è¦ä»¶

```yaml
mode_specific_requirements:
  # Scalp Modeï¼ˆ70-80%ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼‰
  scalp:
    critical: true
    target_duration: < 1 hour
    entry_timing_sensitivity: high
    max_acceptable_latency: 10 ms  # p95åŸºæº–
    reason: "M1/M5/M15ã®çŸ­æœŸã‚¨ãƒ³ãƒˆãƒªãƒ¼ã§ã¯æ•°ç§’ã®é…å»¶ãŒè‡´å‘½çš„"
  
  # Swing Extension Modeï¼ˆ20-30%ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼‰
  swing:
    critical: false
    target_duration: < 6 hours
    entry_timing_sensitivity: medium
    max_acceptable_latency: 50 ms  # p95åŸºæº–
    reason: "H1/H4å‚ç…§ã®ãƒˆãƒ¬ãƒ¼ãƒ«æˆ¦ç•¥ã§ã¯æ•°åmsã®é…å»¶ã¯è¨±å®¹å¯èƒ½"
```

---

## ğŸ—ï¸ å®Ÿè¡Œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ†è§£

### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Input Bind        (ãƒ‡ãƒ¼ã‚¿å—ä¿¡ãƒ»ãƒã‚¤ãƒ³ãƒ‰)                 â”‚ ~0.5ms
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Diff Update       (å·®åˆ†å‰å‡¦ç†ãƒ»ç‰¹å¾´é‡æ›´æ–°)               â”‚ ~1.5ms
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Model Forward     (LSTMæ¨è«– 5TF + Attention Fusion)      â”‚ ~4.0ms
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Postprocess       (ç¢ºç‡æ­£è¦åŒ–ãƒ»æœŸå¾…å€¤è¨ˆç®—)               â”‚ ~0.5ms
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. Publish           (çµæœå‡ºåŠ›ãƒ»ãƒ­ã‚°è¨˜éŒ²)                   â”‚ ~0.5ms
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total Target: ~7.0ms (p50)
```

### ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥SLO

```python
stage_latency_targets = {
    "input_bind": {
        "p50": 0.3,  # ms
        "p95": 0.6,
        "description": "HDF5èª­è¾¼ or ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡å–å¾—"
    },
    "diff_update": {
        "p50": 1.0,
        "p95": 2.0,
        "description": "å¢—åˆ†ç‰¹å¾´é‡è¨ˆç®—ï¼ˆEMA/Welfordæ›´æ–°ï¼‰"
    },
    "model_forward_fast": {
        "p50": 3.0,
        "p95": 5.0,
        "description": "ONNX FP16æ¨è«–ï¼ˆ5TF LSTM + Fusionï¼‰"
    },
    "model_forward_lazy": {
        "p95": 15.0,
        "description": "ãƒ•ãƒ«å†è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ï¼‰"
    },
    "postprocess": {
        "p50": 0.4,
        "p95": 0.8,
        "description": "Softmax + æœŸå¾…å€¤è¨ˆç®— + ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š"
    },
    "publish": {
        "p50": 0.3,
        "p95": 0.6,
        "description": "éåŒæœŸãƒ­ã‚°ã‚­ãƒ¥ãƒ¼ + çµæœè¿”å´"
    }
}
```

---

## âš¡ æœ€é©åŒ–æˆ¦ç•¥

### 1. å·®åˆ†å‰å‡¦ç†ï¼ˆIncremental Feature Updateï¼‰

```python
# ãƒªãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
class IncrementalFeatureUpdater:
    """
    æ–°ã—ã„ãƒãƒ¼ãŒ1æœ¬è¿½åŠ ã•ã‚Œã‚‹ãŸã³ã«ã€O(1)ã§ç‰¹å¾´é‡ã‚’æ›´æ–°
    """
    def __init__(self, window_size: int = 360):
        self.ring_buffer = RingBuffer(window_size)
        self.ema_state = {}  # EMAçŠ¶æ…‹ä¿æŒ
        self.welford_state = {}  # åˆ†æ•£è¨ˆç®—ç”¨
    
    def update(self, new_bar: dict) -> np.ndarray:
        """
        Target: p95 < 2.0ms
        
        å·®åˆ†æ›´æ–°å¯¾è±¡:
        - EMAç³»ï¼ˆexponential moving averageï¼‰
        - Rollingçµ±è¨ˆï¼ˆmean, std via Welfordï¼‰
        - ATRï¼ˆAverage True Rangeï¼‰
        - Tick activityç‡
        """
        self.ring_buffer.append(new_bar)
        
        # O(1)æ›´æ–°
        self._update_ema()
        self._update_welford_variance()
        self._update_atr()
        
        return self.get_features()  # æœ€æ–°360æœ¬åˆ†
```

**åŠ¹æœ**: å…¨ç‰¹å¾´é‡å†è¨ˆç®—ï¼ˆ~10msï¼‰ â†’ å·®åˆ†æ›´æ–°ï¼ˆ~1.5msï¼‰

### 2. ãƒ¢ãƒ‡ãƒ«æ¨è«–æœ€é©åŒ–

#### ONNX Runtime + FP16

```yaml
model_optimization:
  format: ONNX
  precision: FP16  # Mixed precision
  execution_provider: CUDAExecutionProvider
  graph_optimization_level: ORT_ENABLE_ALL
  
  quantization:
    phase1: FP16  # åˆæœŸ
    phase2: INT8  # æ¤œè¨¼å¾Œï¼ˆç²¾åº¦ç¢ºèªå¿…é ˆï¼‰
```

#### Attentionæœ€é©åŒ–

```python
# QKå°„å½±æ¬¡å…ƒå‰Šæ¸›
attention_config = {
    "d_model": 128,
    "d_k_projection": 32,  # é€šå¸¸64â†’32ã¸å‰Šæ¸›
    "num_heads": 4,
    "description": "Query/Keyå°„å½±ã‚’ä½æ¬¡å…ƒåŒ–ã—ã¦QK^Tè¨ˆç®—é‡å‰Šæ¸›"
}

# Age-based Weight ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–
# å‹•çš„è¨ˆç®—ï¼ˆ~0.5msï¼‰ â†’ ãƒ†ãƒ¼ãƒ–ãƒ«å‚ç…§ï¼ˆ~0.01msï¼‰
age_weight_table = np.exp(-np.arange(360) * 0.01)  # äº‹å‰è¨ˆç®—
```

#### ãƒãƒƒãƒå‡¦ç†ã®å›é¿

```python
# âŒ NG: ãƒãƒƒãƒæ¨è«–ï¼ˆå¾…æ©Ÿæ™‚é–“å¢—åŠ ï¼‰
# predictions = model.predict(batch_samples)

# âœ… OK: å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å³æ™‚æ¨è«–
prediction = model.predict(single_sample)  # latencyæœ€å°åŒ–
```

### 3. Fast Path / Slow Path åˆ†å²

```python
class AdaptiveInferenceEngine:
    def predict(self, new_bar: dict) -> dict:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆåˆ¤å®š
        if self._is_incremental_update_valid():
            # Fast Pathï¼ˆå·®åˆ†æ›´æ–°ã®ã¿ï¼‰
            features = self.feature_updater.update(new_bar)
            return self._fast_predict(features)  # ~5ms
        else:
            # Slow Pathï¼ˆãƒ•ãƒ«å†è¨ˆç®—ï¼‰
            features = self._full_recompute(new_bar)
            return self._full_predict(features)  # ~30ms
    
    def _is_incremental_update_valid(self) -> bool:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–æ¡ä»¶:
        - ãƒ‡ãƒ¼ã‚¿ã‚®ãƒ£ãƒƒãƒ—æ¤œå‡ºï¼ˆæ™‚åˆ»é£›ã³ï¼‰
        - ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ç•°å¸¸å€¤ï¼ˆ10Ïƒè¶…ï¼‰
        - ãƒ¢ãƒ‡ãƒ«å†ãƒ­ãƒ¼ãƒ‰ç›´å¾Œ
        """
        return (
            self._no_data_gap() and
            self._spread_within_normal_range() and
            self._cache_fresh()
        )
```

---

## ğŸ“Š è¨ˆæ¸¬ãƒ»ç›£è¦–

### è¨ˆæ¸¬å®Ÿè£…

```python
import time

class LatencyTracker:
    """é …ç›®62, 108å¯¾å¿œ: GPU/ONNXã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¸¬å®šé–‹å§‹åŸºæº–æ˜ç¢ºåŒ–"""
    
    def __init__(self):
        self.stage_times = defaultdict(list)
        self.warmup_count = 32  # æœ€åˆ32å›ã¯é™¤å¤–ï¼ˆGPUã‚«ãƒ¼ãƒãƒ«åˆæœŸåŒ–å¯¾å¿œï¼‰
        self.call_count = 0
        self.warmup_completed = False
    
    @contextmanager
    def measure(self, stage_name: str):
        """
        Monotonic time ã«ã‚ˆã‚‹é«˜ç²¾åº¦è¨ˆæ¸¬
        
        ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŸºæº–ï¼ˆé …ç›®62, 108ï¼‰:
        - GPU: æœ€åˆ32å›ã®forwardå‘¼ã³å‡ºã—ã§ã‚«ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ­ãƒ¼ãƒ‰
        - ONNX: æœ€åˆ32å›ã§ORTæœ€é©åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆãƒ»ãƒ¡ãƒ¢ãƒªã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
        - åˆæœŸé…å»¶ï¼ˆåˆå›100msè¶…ï¼‰ã‚’çµ±è¨ˆã‹ã‚‰é™¤å¤–ã—ã¦SLAèª¤åˆ¤å®šé˜²æ­¢
        """
        start = time.perf_counter_ns()  # ãƒŠãƒç§’ç²¾åº¦
        try:
            yield
        finally:
            elapsed_ms = (time.perf_counter_ns() - start) / 1e6
            self.call_count += 1
            
            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†å¾Œã®ã¿çµ±è¨ˆã«å«ã‚ã‚‹
            if self.call_count > self.warmup_count:
                if not self.warmup_completed:
                    self.warmup_completed = True
                    logger.info(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†: {self.warmup_count}å›, çµ±è¨ˆè¨ˆæ¸¬é–‹å§‹")
                self.stage_times[stage_name].append(elapsed_ms)
            else:
                # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã¯çµ±è¨ˆå¤–ï¼ˆãƒ­ã‚°ã®ã¿ï¼‰
                if self.call_count <= 3:  # åˆå›3å›ã®ã¿ãƒ­ã‚°
                    logger.debug(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­[{self.call_count}/{self.warmup_count}]: "
                               f"{stage_name}={elapsed_ms:.2f}ms")
    
    def get_percentiles(self, stage_name: str) -> dict:
        """p50, p95, p99, max ã‚’ç®—å‡º"""
        times = self.stage_times[stage_name]
        if len(times) == 0:
            logger.warning(f"çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãªã—: {stage_name}ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ï¼Ÿï¼‰")
            return {"p50": 0, "p95": 0, "p99": 0, "max": 0}
        
        return {
            "p50": np.percentile(times, 50),
            "p95": np.percentile(times, 95),
            "p99": np.percentile(times, 99),
            "max": np.max(times),
            "sample_count": len(times)
        }
    
    def validate_slo(self, stage_name: str, slo_p95_ms: float) -> bool:
        """SLAæ¤œè¨¼ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†å¾Œã®ã¿ï¼‰"""
        if not self.warmup_completed:
            return True  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã¯æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—
        
        stats = self.get_percentiles(stage_name)
        return stats["p95"] <= slo_p95_ms


# ãƒ‡ãƒ—ãƒ­ã‚¤warmupæ‰‹é †
def deployment_warmup(model, device: str, warmup_calls: int = 32):
    """
    ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã®warmupæ‰‹é †æ˜ç¢ºåŒ–
    
    ç›®çš„:
    - cold startæ™‚ã®p95æ‚ªåŒ–é˜²æ­¢
    - åˆæœŸ5åˆ†SLAé”æˆä¿è¨¼
    - GPUã‚«ãƒ¼ãƒãƒ«ãƒ»ONNXã‚°ãƒ©ãƒ•äº‹å‰æœ€é©åŒ–
    """
    logger.info(f"ãƒ‡ãƒ—ãƒ­ã‚¤warmupé–‹å§‹: {warmup_calls}å›")
    
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã¨åŒã˜ï¼‰
    dummy_input = {
        "M1": torch.randn(1, 480, 128).to(device),
        "M5": torch.randn(1, 288, 128).to(device),
        "M15": torch.randn(1, 192, 128).to(device),
        "H1": torch.randn(1, 96, 128).to(device),
        "H4": torch.randn(1, 48, 128).to(device),
    }
    
    model.eval()
    with torch.no_grad():
        for i in range(warmup_calls):
            _ = model(**dummy_input)
            
            if i in [0, 1, 2, warmup_calls//2, warmup_calls-1]:
                logger.debug(f"warmupé€²è¡Œ: {i+1}/{warmup_calls}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ—ãƒ©ã‚¤ãƒŸãƒ³ã‚°å®Œäº†
    if device.startswith("cuda"):
        torch.cuda.synchronize()
    
    logger.info("ãƒ‡ãƒ—ãƒ­ã‚¤warmupå®Œäº†: æœ¬ç•ªçµ±è¨ˆè¨ˆæ¸¬é–‹å§‹")


**ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŸºæº–ä»•æ§˜ï¼ˆé …ç›®62, 108ï¼‰**:
- **warmup_calls**: 32å›ï¼ˆGPU/ONNXã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–å®Œäº†ã«ååˆ†ï¼‰
- **çµ±è¨ˆé™¤å¤–**: æœ€åˆ32å›ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯ p50/p95/p99 è¨ˆç®—ã‹ã‚‰é™¤å¤–
- **SLAæ¤œè¨¼**: warmupå®Œäº†å¾Œã‹ã‚‰ã®ã¿SLOåˆ¤å®šé–‹å§‹
- **ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †**: ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹å‰ã«å¿…ãš `deployment_warmup()` å®Ÿè¡Œ
- **æˆåŠŸæŒ‡æ¨™**: åˆæœŸ5åˆ†ï¼ˆwarmupå¾Œï¼‰ã®p95 SLAé”æˆç‡ >= 95%
```

### ãƒ­ã‚°å‡ºåŠ›

**æ³¨è¨˜**: timestampã¯UTCã€ãƒ­ã‚°è¡¨ç¤ºã¯æ—¥æœ¬æ™‚é–“(JST)ã§å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚è©³ç´°: [docs/utils/TIMEZONE_UTILS_SPEC.md](../../utils/TIMEZONE_UTILS_SPEC.md)

```python
# 1ç§’ã”ã¨ã¾ãŸã¯256å‘¼ã³å‡ºã—ã”ã¨ã«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
log_snapshot = {
    "timestamp": "2025-10-21T10:30:00Z",
    "timestamp_jst": "2025-10-21 19:30:00 JST",
    "category": "latency",
    "interval_seconds": 1.0,
    "call_count": 256,
    
    "total_latency_ms": {
        "p50": 6.2,
        "p95": 9.8,
        "p99": 15.3,
        "max": 23.1,
        "slo_violations": 2  # p95>10ms ã®å›æ•°
    },
    
    "stage_breakdown_ms": {
        "input_bind": {"p50": 0.4, "p95": 0.7},
        "diff_update": {"p50": 1.3, "p95": 2.1},
        "model_forward": {"p50": 3.8, "p95": 5.5},
        "postprocess": {"p50": 0.5, "p95": 0.9},
        "publish": {"p50": 0.2, "p95": 0.6}
    },
    
    "path_distribution": {
        "fast_path_ratio": 0.94,  # 94%ãŒFast Path
        "slow_path_ratio": 0.06,
        "slow_path_avg_ms": 28.5
    }
}
```

---

## ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»ç•°å¸¸æ¤œçŸ¥

### é–¾å€¤å®šç¾©

```yaml
alerts:
  # WARNING: æ³¨æ„ãŒå¿…è¦
  warning:
    total_p95_ms: > 12.0
    fast_path_p95_ms: > 10.0
    diff_update_p95_ms: > 3.0
    slow_path_ratio: > 0.15
  
  # CRITICAL: ç·Šæ€¥å¯¾å¿œå¿…è¦
  critical:
    total_p95_ms: > 20.0
    fast_path_p95_ms: > 15.0
    slo_violation_rate: > 0.05  # 5%è¶…ãŒSLOé•å
    consecutive_slow_path: > 10  # é€£ç¶š10å›Slow Path
```

### ç•°å¸¸æ™‚ã®å¯¾å¿œ

```python
class LatencyAnomalyHandler:
    def handle_slo_violation(self, stage: str, actual_ms: float):
        """
        SLOé•åæ™‚ã®å¯¾å¿œãƒ•ãƒ­ãƒ¼
        """
        if stage == "model_forward" and actual_ms > 15.0:
            # ãƒ¢ãƒ‡ãƒ«æ¨è«–ãŒé…ã„
            self._check_gpu_utilization()
            self._check_model_cache()
            self._log_heavy_sample()
        
        elif stage == "diff_update" and actual_ms > 3.0:
            # ç‰¹å¾´é‡è¨ˆç®—ãŒé…ã„
            self._profile_feature_calculators()
            self._check_data_anomaly()
        
        # é€£ç¶šé•åã§ã‚¢ãƒ©ãƒ¼ãƒˆ
        if self._consecutive_violations(stage) >= 5:
            self._send_alert(severity="CRITICAL", stage=stage)
```

---

## ğŸ”„ æ˜‡æ ¼ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯åŸºæº–

### æ–°ãƒ¢ãƒ‡ãƒ«æ˜‡æ ¼æ¡ä»¶ï¼ˆElevationï¼‰

```yaml
elevation_criteria:
  # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·åŸºæº–
  latency:
    fast_path_p95_delta: <= +2.0 ms  # æ—§ãƒ¢ãƒ‡ãƒ«æ¯”
    total_p95_absolute: < 10.0 ms
    slow_path_ratio_increase: <= +0.05
  
  # æ€§èƒ½åŸºæº–ï¼ˆä½µç”¨ï¼‰
  performance:
    net_expectancy_delta: >= -0.02
    direction_accuracy_delta: >= -0.03
  
  # é€£ç¶šé•åãªã—
  stability:
    consecutive_slo_violations: < 5
```

### ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¡ä»¶

```yaml
rollback_triggers:
  # å³åº§ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
  immediate:
    - total_p95_ms > 25.0 (é€£ç¶š3åˆ†)
    - fast_path_p95_ms > 15.0 (é€£ç¶š5åˆ†)
    - net_expectancy < baseline - 0.05 (é€£ç¶š3æ™‚é–“)
  
  # æ®µéšçš„è­¦å‘Š
  gradual:
    - total_p95_ms > 15.0 (é€£ç¶š10åˆ†) â†’ è­¦å‘Š
    - slo_violation_rate > 0.1 (1æ™‚é–“) â†’ è­¦å‘Š
```

---

## ğŸ¯ Phaseåˆ¥å®Ÿè£…è¨ˆç”»

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º1: åŸºç›¤æ§‹ç¯‰
- âœ… ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥è¨ˆæ¸¬å®Ÿè£…
- âœ… Fast/Slow Path åˆ†å²
- âœ… å·®åˆ†å‰å‡¦ç†ï¼ˆEMA/Welfordï¼‰
- âœ… ONNX FP16æ¨è«–
- âœ… ãƒ­ã‚°ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‡ºåŠ›

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º2: æœ€é©åŒ–
- â³ Attention QKå°„å½±å‰Šæ¸›ï¼ˆd_k=32ï¼‰
- â³ Age-weight ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–
- â³ GPU warmupæœ€é©åŒ–
- â³ ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–æ¤œè¨¼ï¼ˆINT8ï¼‰

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º3: ç›£è¦–å¼·åŒ–
- â³ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆçµ±åˆ
- â³ è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹
- â³ Latency-aware position sizing
- â³ ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰åˆ¥latencyåˆ†æ

---

## ğŸ“ è¨­å®šä¾‹

```yaml
# config/execution_latency_config.yaml

execution:
  latency_tracking:
    enabled: true
    warmup_calls: 32
    snapshot_interval_seconds: 1.0
    snapshot_interval_calls: 256
    log_rotation_hours: 24
  
  slo:
    total_p95_ms: 10.0
    fast_path_p95_ms: 8.0
    slow_path_max_ms: 100.0
  
  optimization:
    use_onnx: true
    precision: fp16
    enable_diff_update: true
    attention_d_k: 32
    cache_validation_interval: 100
  
  alerts:
    warning_threshold_ms: 12.0
    critical_threshold_ms: 20.0
    consecutive_violation_limit: 5
```

---

## æ¨è«–æœ€é©åŒ–

### å·®åˆ†æ›´æ–°APIçµ±ä¸€

**ç›®çš„**: ç‰¹å¾´é‡è¨ˆç®—å™¨ã”ã¨ã«APIå½¢å¼ãŒç•°ãªã‚Šçµ±åˆå›°é›£

**è§£æ±ºç­–**: æ¨™æº–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

class DifferentialUpdatable(ABC):
    """å·®åˆ†æ›´æ–°å¯èƒ½ãªç‰¹å¾´é‡è¨ˆç®—å™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def initialize_state(self, initial_window: np.ndarray) -> Dict[str, Any]:
        """
        åˆæœŸçŠ¶æ…‹ã®æ§‹ç¯‰
        
        Args:
            initial_window: åˆæœŸçª“ãƒ‡ãƒ¼ã‚¿ (window_size, n_features)
        
        Returns:
            state: çŠ¶æ…‹è¾æ›¸ï¼ˆæ¬¡å›update_incrementalã§ä½¿ç”¨ï¼‰
        """
        pass
    
    @abstractmethod
    def update_incremental(
        self,
        state: Dict[str, Any],
        new_tick: np.ndarray
    ) -> tuple[np.ndarray, Dict[str, Any]]:
        """
        å·®åˆ†æ›´æ–°ï¼ˆæ–°ç€1æœ¬ã®ã¿å‡¦ç†ï¼‰
        
        Args:
            state: å‰å›ã®çŠ¶æ…‹
            new_tick: æ–°ç€ãƒ‡ãƒ¼ã‚¿ (n_features,)
        
        Returns:
            (updated_feature, new_state)
        """
        pass
    
    @abstractmethod
    def supports_diff_update(self) -> bool:
        """å·®åˆ†æ›´æ–°å¯¾å¿œãƒ•ãƒ©ã‚°"""
        pass


class EMACalculatorDiff(DifferentialUpdatable):
    """EMAå·®åˆ†æ›´æ–°å®Ÿè£…ä¾‹"""
    
    def __init__(self, period: int = 9):
        self.period = period
        self.alpha = 2.0 / (period + 1)
    
    def initialize_state(self, initial_window: np.ndarray) -> Dict[str, Any]:
        """åˆæœŸEMAè¨ˆç®—"""
        ema = initial_window[:, 0].mean()  # å˜ç´”å¹³å‡ã§åˆæœŸåŒ–
        
        for price in initial_window[:, 0]:
            ema = self.alpha * price + (1 - self.alpha) * ema
        
        return {"ema": ema}
    
    def update_incremental(
        self,
        state: Dict[str, Any],
        new_tick: np.ndarray
    ) -> tuple[np.ndarray, Dict[str, Any]]:
        """å·®åˆ†æ›´æ–°"""
        prev_ema = state["ema"]
        new_price = new_tick[0]
        
        # EMAæ›´æ–°å¼
        new_ema = self.alpha * new_price + (1 - self.alpha) * prev_ema
        
        return np.array([new_ema]), {"ema": new_ema}
    
    def supports_diff_update(self) -> bool:
        return True


class ATRCalculatorDiff(DifferentialUpdatable):
    """ATRå·®åˆ†æ›´æ–°å®Ÿè£…ä¾‹"""
    
    def __init__(self, period: int = 14):
        self.period = period
        self.alpha = 1.0 / period
    
    def initialize_state(self, initial_window: np.ndarray) -> Dict[str, Any]:
        """åˆæœŸATRè¨ˆç®—"""
        # True Rangeè¨ˆç®—
        trs = []
        for i in range(1, len(initial_window)):
            high = initial_window[i, 1]
            low = initial_window[i, 2]
            prev_close = initial_window[i-1, 0]
            
            tr = max(
                high - low,
                abs(high - prev_close),
                abs(low - prev_close)
            )
            trs.append(tr)
        
        atr = np.mean(trs[-self.period:])
        
        return {"atr": atr, "prev_close": initial_window[-1, 0]}
    
    def update_incremental(
        self,
        state: Dict[str, Any],
        new_tick: np.ndarray  # [close, high, low]
    ) -> tuple[np.ndarray, Dict[str, Any]]:
        """å·®åˆ†æ›´æ–°"""
        prev_atr = state["atr"]
        prev_close = state["prev_close"]
        
        close, high, low = new_tick[0], new_tick[1], new_tick[2]
        
        # True Range
        tr = max(
            high - low,
            abs(high - prev_close),
            abs(low - prev_close)
        )
        
        # ATRæ›´æ–°ï¼ˆWilder's Smoothingï¼‰
        new_atr = self.alpha * tr + (1 - self.alpha) * prev_atr
        
        return np.array([new_atr]), {"atr": new_atr, "prev_close": close}
    
    def supports_diff_update(self) -> bool:
        return True


# çµ±ä¸€ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
class DifferentialUpdateManager:
    """å·®åˆ†æ›´æ–°çµ±ä¸€ç®¡ç†"""
    
    def __init__(self):
        self.calculators: Dict[str, DifferentialUpdatable] = {}
        self.states: Dict[str, Dict[str, Any]] = {}
    
    def register_calculator(self, name: str, calculator: DifferentialUpdatable):
        """è¨ˆç®—å™¨ç™»éŒ²"""
        if not calculator.supports_diff_update():
            raise ValueError(f"{name} ã¯å·®åˆ†æ›´æ–°éå¯¾å¿œ")
        
        self.calculators[name] = calculator
        logger.info(f"å·®åˆ†æ›´æ–°è¨ˆç®—å™¨ç™»éŒ²: {name}")
    
    def initialize_all(self, initial_data: Dict[str, np.ndarray]):
        """å…¨è¨ˆç®—å™¨ã®åˆæœŸåŒ–"""
        for name, calculator in self.calculators.items():
            if name in initial_data:
                self.states[name] = calculator.initialize_state(initial_data[name])
                logger.info(f"{name} åˆæœŸåŒ–å®Œäº†")
    
    def update_all(self, new_data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """å…¨è¨ˆç®—å™¨ã®å·®åˆ†æ›´æ–°"""
        results = {}
        
        for name, calculator in self.calculators.items():
            if name in new_data and name in self.states:
                feature, new_state = calculator.update_incremental(
                    self.states[name],
                    new_data[name]
                )
                results[name] = feature
                self.states[name] = new_state
        
        return results


# ä½¿ç”¨ä¾‹
manager = DifferentialUpdateManager()
manager.register_calculator("ema9", EMACalculatorDiff(period=9))
manager.register_calculator("atr14", ATRCalculatorDiff(period=14))

# åˆæœŸåŒ–
initial_data = {
    "ema9": historical_close[:480],  # M1: 480æœ¬
    "atr14": historical_hlc[:480]
}
manager.initialize_all(initial_data)

# æ–°ç€tickå‡¦ç†
while True:
    new_tick = get_new_tick()
    new_data = {
        "ema9": new_tick["close"],
        "atr14": np.array([new_tick["close"], new_tick["high"], new_tick["low"]])
    }
    
    features = manager.update_all(new_data)
    # features: {"ema9": [value], "atr14": [value]}
```

**APIä»•æ§˜**:

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¼•æ•° | æˆ»ã‚Šå€¤ | è¨ˆç®—é‡ |
|---------|------|--------|--------|
| `initialize_state` | åˆæœŸçª“ãƒ‡ãƒ¼ã‚¿ | çŠ¶æ…‹è¾æ›¸ | O(N) |
| `update_incremental` | çŠ¶æ…‹+æ–°ç€1æœ¬ | (ç‰¹å¾´é‡, æ–°çŠ¶æ…‹) | O(1) |
| `supports_diff_update` | ãªã— | bool | O(1) |

**KPIï¼ˆé …ç›®23ï¼‰**:
- APIæº–æ‹ ç‡: â‰¥80%ã®å·®åˆ†å¯¾å¿œè¨ˆç®—å™¨
- å‘¼ã³å‡ºã—æˆåŠŸç‡: >99.9%
- çµ±ä¸€åŒ–ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰å‰Šæ¸›: â‰¥30%

---

### é…å»¶ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

**ç›®çš„**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¶…éæ™‚ã«å…¨é‡å†è¨ˆç®—ã§æ›´ã«é…å»¶æ‚ªåŒ–

**è§£æ±ºç­–**: è»½é‡ãƒ¢ãƒ¼ãƒ‰è‡ªå‹•åˆ‡æ›¿

```python
class LatencyFallbackController:
    """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ¶å¾¡"""
    
    def __init__(self, config: dict):
        self.warning_threshold_ms = config.get("warning_threshold_ms", 12.0)
        self.critical_threshold_ms = config.get("critical_threshold_ms", 20.0)
        self.fallback_mode = "normal"  # normal | degraded | emergency
        
        self.latency_history = deque(maxlen=100)
    
    def record_latency(self, latency_ms: float):
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨˜éŒ²"""
        self.latency_history.append(latency_ms)
        
        # ç›´è¿‘10å›ã®å¹³å‡
        if len(self.latency_history) >= 10:
            recent_avg = np.mean(list(self.latency_history)[-10:])
            
            if recent_avg > self.critical_threshold_ms:
                self._switch_mode("emergency")
            elif recent_avg > self.warning_threshold_ms:
                self._switch_mode("degraded")
            else:
                self._switch_mode("normal")
    
    def _switch_mode(self, new_mode: str):
        """ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿"""
        if self.fallback_mode != new_mode:
            logger.warning(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿: {self.fallback_mode} â†’ {new_mode}")
            self.fallback_mode = new_mode
    
    def get_execution_strategy(self) -> Dict[str, Any]:
        """å®Ÿè¡Œæˆ¦ç•¥å–å¾—"""
        if self.fallback_mode == "normal":
            return {
                "use_full_features": True,
                "attention_heads": 4,
                "precision": "fp16",
                "diff_update": True
            }
        elif self.fallback_mode == "degraded":
            return {
                "use_full_features": True,
                "attention_heads": 2,  # å‰Šæ¸›
                "precision": "fp16",
                "diff_update": True
            }
        else:  # emergency
            return {
                "use_full_features": False,  # ç‰¹å¾´é‡å‰Šæ¸›
                "attention_heads": 1,
                "precision": "fp16",
                "diff_update": False  # å…¨é‡å†è¨ˆç®—ã‚‚åœæ­¢
            }


# ä½¿ç”¨ä¾‹
fallback_ctrl = LatencyFallbackController({
    "warning_threshold_ms": 12.0,
    "critical_threshold_ms": 20.0
})

# æ¨è«–ãƒ«ãƒ¼ãƒ—
while True:
    start = time.perf_counter()
    
    # å®Ÿè¡Œæˆ¦ç•¥å–å¾—
    strategy = fallback_ctrl.get_execution_strategy()
    
    # æˆ¦ç•¥ã«åŸºã¥ã„ã¦æ¨è«–
    if strategy["use_full_features"]:
        features = compute_all_features(new_tick)
    else:
        features = compute_minimal_features(new_tick)  # 10åˆ—ã®ã¿
    
    prediction = model.predict(features, **strategy)
    
    # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨˜éŒ²
    latency_ms = (time.perf_counter() - start) * 1000
    fallback_ctrl.record_latency(latency_ms)
    
    logger.info(f"ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·={latency_ms:.2f}ms, ãƒ¢ãƒ¼ãƒ‰={fallback_ctrl.fallback_mode}")
```

**ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¬ãƒ™ãƒ«**:

| Mode | ç‰¹å¾´é‡ | Attention Heads | æœŸå¾…ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ç²¾åº¦åŠ£åŒ– |
|------|--------|----------------|---------------|---------|
| normal | å…¨åˆ—ï¼ˆ50-80ï¼‰ | 4 | <10ms | 0% |
| degraded | å…¨åˆ— | 2 | <15ms | <2% |
| emergency | 10åˆ—ã®ã¿ | 1 | <5ms | <5% |

**KPIï¼ˆé …ç›®24ï¼‰**:
- degradedç§»è¡Œé »åº¦: <5%
- emergencyç§»è¡Œé »åº¦: <1%
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸­ã®ç²¾åº¦: >92%ï¼ˆnormal=95%åŸºæº–ï¼‰

---

### ç•°å¸¸æ™‚è»½é‡ãƒ‘ã‚¹

**ç›®çš„**: ã‚¨ãƒ©ãƒ¼æ™‚ã«æ¨è«–åœæ­¢ã¯ãƒˆãƒ¬ãƒ¼ãƒ‰æ©Ÿä¼šæå¤±

**è§£æ±ºç­–**: Degraded Modeè¨­è¨ˆ

```python
class DegradedModeExecutor:
    """ç¸®é€€ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ"""
    
    def __init__(self, config: dict):
        self.minimal_features = config.get(
            "minimal_features",
            ["m1_close_return", "m5_close_return", "atr_14",
             "ema_9", "ema_21", "rsi_14", "m1_m5_close_diff",
             "spread_ema5", "volume_zscore", "direction_flip_rate"]
        )
        self.fallback_model_path = config.get("fallback_model_path", None)
    
    def detect_anomaly(self, error: Exception) -> str:
        """ç•°å¸¸æ¤œå‡º"""
        if isinstance(error, MemoryError):
            return "memory_exhausted"
        elif isinstance(error, TimeoutError):
            return "latency_exceeded"
        elif "NaN" in str(error):
            return "feature_corruption"
        else:
            return "unknown_error"
    
    def execute_degraded(
        self,
        anomaly_type: str,
        available_features: Dict[str, float]
    ) -> Dict[str, Any]:
        """ç¸®é€€å®Ÿè¡Œ"""
        logger.warning(f"ç¸®é€€ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ: {anomaly_type}")
        
        # æœ€å°ç‰¹å¾´é‡æŠ½å‡º
        minimal_vector = []
        for feat in self.minimal_features:
            if feat in available_features:
                minimal_vector.append(available_features[feat])
            else:
                minimal_vector.append(0.0)  # æ¬ æã¯0åŸ‹ã‚
        
        minimal_vector = np.array(minimal_vector).reshape(1, -1)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼ˆãƒ¢ãƒ‡ãƒ«ä¸ä½¿ç”¨ï¼‰
        if anomaly_type == "memory_exhausted":
            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹: EMA ã‚¯ãƒ­ã‚¹ã‚ªãƒ¼ãƒãƒ¼
            if len(minimal_vector[0]) >= 5:
                ema9, ema21 = minimal_vector[0][3], minimal_vector[0][4]
                direction = "UP" if ema9 > ema21 else "DOWN"
            else:
                direction = "NEUTRAL"
            
            return {
                "direction": direction,
                "confidence": 0.5,  # ä½ä¿¡é ¼åº¦
                "mode": "degraded_rule_based"
            }
        
        elif anomaly_type == "latency_exceeded":
            # å‰å›äºˆæ¸¬ã‚’è¿”ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            return {
                "direction": getattr(self, "last_prediction", "NEUTRAL"),
                "confidence": 0.3,
                "mode": "degraded_cached"
            }
        
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: NEUTRAL
            return {
                "direction": "NEUTRAL",
                "confidence": 0.0,
                "mode": "degraded_safe"
            }


# ä½¿ç”¨ä¾‹
degraded_executor = DegradedModeExecutor({
    "minimal_features": ["m1_close_return", "ema_9", "ema_21", "rsi_14"],
    "fallback_model_path": None
})

# æ¨è«–ãƒ«ãƒ¼ãƒ—
try:
    features = compute_full_features(new_tick)
    prediction = model.predict(features)
except Exception as e:
    # ç•°å¸¸æ¤œå‡º
    anomaly_type = degraded_executor.detect_anomaly(e)
    
    # ç¸®é€€å®Ÿè¡Œ
    available_features = compute_available_features(new_tick)
    prediction = degraded_executor.execute_degraded(anomaly_type, available_features)
    
    logger.error(f"ç•°å¸¸ç™ºç”Ÿ: {anomaly_type}, ç¸®é€€ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š")
```

**Degraded Modeæˆ¦ç•¥**:

| ç•°å¸¸ã‚¿ã‚¤ãƒ— | å¯¾å¿œ | ä¿¡é ¼åº¦ | SLA |
|-----------|------|--------|-----|
| memory_exhausted | ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ¤å®š | 0.5 | <3ms |
| latency_exceeded | å‰å›äºˆæ¸¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | 0.3 | <1ms |
| feature_corruption | NEUTRALè¿”å´ | 0.0 | <1ms |

**KPIï¼ˆé …ç›®31ï¼‰**:
- ç¸®é€€ç™ºå‹•é »åº¦: <1%
- ç¸®é€€ä¸­ã®ä¿¡é ¼åº¦: â‰¥0.3ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‰ç¶šè¡Œå¯èƒ½ï¼‰
- ç¸®é€€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: <5ms

---

### Fast/Slow Pathç†ç”±åˆ†é¡

**ç›®çš„**: é…å»¶ç™ºç”Ÿç†ç”±ãŒä¸æ˜ã§æ”¹å–„å›°é›£

**è§£æ±ºç­–**: ãƒ‘ã‚¹é¸æŠç†ç”±ã®ãƒ­ã‚°è¨˜éŒ²

```python
class PathReasonLogger:
    """ãƒ‘ã‚¹é¸æŠç†ç”±ãƒ­ã‚®ãƒ³ã‚°"""
    
    # ç†ç”±ã‚³ãƒ¼ãƒ‰å®šç¾©
    REASON_CODES = {
        # Fast Pathç†ç”±
        "FAST_DIFF_UPDATE": "å·®åˆ†æ›´æ–°æˆåŠŸ",
        "FAST_CACHE_HIT": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ",
        "FAST_MINIMAL_FEATURES": "æœ€å°ç‰¹å¾´é‡ãƒ¢ãƒ¼ãƒ‰",
        
        # Slow Pathç†ç”±
        "SLOW_CACHE_MISS": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼ˆåˆå›ãƒ»ãƒªã‚»ãƒƒãƒˆï¼‰",
        "SLOW_FULL_RECOMPUTE": "å…¨é‡å†è¨ˆç®—å¿…é ˆ",
        "SLOW_FEATURE_CORRUPTION": "ç‰¹å¾´é‡ç ´ææ¤œå‡º",
        "SLOW_STATE_INCONSISTENT": "çŠ¶æ…‹ä¸æ•´åˆ",
        "SLOW_DRIFT_DETECTED": "ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º",
        
        # Emergencyç†ç”±
        "EMERGENCY_LATENCY": "ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¶…é",
        "EMERGENCY_MEMORY": "ãƒ¡ãƒ¢ãƒªä¸è¶³",
        "EMERGENCY_ERROR": "äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼"
    }
    
    def __init__(self):
        self.path_stats = {code: 0 for code in self.REASON_CODES.keys()}
    
    def log_path(self, reason_code: str, latency_ms: float, details: str = ""):
        """ãƒ‘ã‚¹è¨˜éŒ²"""
        if reason_code not in self.REASON_CODES:
            logger.warning(f"ä¸æ˜ãªç†ç”±ã‚³ãƒ¼ãƒ‰: {reason_code}")
            return
        
        self.path_stats[reason_code] += 1
        
        logger.debug(
            f"Path={reason_code}, "
            f"ç†ç”±={self.REASON_CODES[reason_code]}, "
            f"ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·={latency_ms:.2f}ms, "
            f"è©³ç´°={details}"
        )
    
    def get_summary(self) -> Dict[str, Any]:
        """çµ±è¨ˆã‚µãƒãƒª"""
        total = sum(self.path_stats.values())
        
        summary = {
            "total_calls": total,
            "fast_path_ratio": sum(
                self.path_stats[k] for k in self.path_stats if k.startswith("FAST")
            ) / total * 100 if total > 0 else 0,
            "slow_path_ratio": sum(
                self.path_stats[k] for k in self.path_stats if k.startswith("SLOW")
            ) / total * 100 if total > 0 else 0,
            "emergency_ratio": sum(
                self.path_stats[k] for k in self.path_stats if k.startswith("EMERGENCY")
            ) / total * 100 if total > 0 else 0,
            "breakdown": self.path_stats
        }
        
        return summary


# ä½¿ç”¨ä¾‹
path_logger = PathReasonLogger()

# æ¨è«–ãƒ«ãƒ¼ãƒ—
while True:
    start = time.perf_counter()
    
    try:
        if can_use_diff_update():
            prediction = predict_with_diff_update(new_tick)
            reason = "FAST_DIFF_UPDATE"
        else:
            prediction = predict_full_recompute(new_tick)
            reason = "SLOW_FULL_RECOMPUTE"
    except MemoryError:
        prediction = fallback_predict()
        reason = "EMERGENCY_MEMORY"
    
    latency_ms = (time.perf_counter() - start) * 1000
    path_logger.log_path(reason, latency_ms)

# å®šæœŸã‚µãƒãƒª
summary = path_logger.get_summary()
logger.info(f"Fast Pathç‡: {summary['fast_path_ratio']:.1f}%")
logger.info(f"Slow Pathç‡: {summary['slow_path_ratio']:.1f}%")
logger.info(f"Emergencyç‡: {summary['emergency_ratio']:.1f}%")
```

**ç†ç”±ã‚³ãƒ¼ãƒ‰åˆ†é¡**:

| ã‚«ãƒ†ã‚´ãƒª | ã‚³ãƒ¼ãƒ‰æ•° | æœŸå¾…æ¯”ç‡ |
|---------|---------|---------|
| FAST_* | 3 | >90% |
| SLOW_* | 5 | <10% |
| EMERGENCY_* | 3 | <1% |

**KPIï¼ˆé …ç›®60ï¼‰**:
- Fast Pathç‡: â‰¥90%
- ãƒ­ã‚°ç²’åº¦: ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã§å…¨è¨˜éŒ²
- ã‚µãƒãƒªå‡ºåŠ›: 1æ™‚é–“æ¯

---

## ğŸ”— é–¢é€£ä»•æ§˜

- `README.md` - ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰æˆ¦ç•¥æ¦‚è¦
- `TRAINER_SPEC.md` - ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°
- `PREPROCESSOR_SPEC.md` - ç‰¹å¾´é‡è¨ˆç®—ä»•æ§˜
- `LOGGING_OPERATIONS_SPEC.md` - ãƒ­ã‚°çµ±åˆä»•æ§˜
- `LIFECYCLE_ROLLING_RETRAIN_SPEC.md` - ãƒ¢ãƒ‡ãƒ«æ˜‡æ ¼ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

---

## ğŸ“š å‚è€ƒè³‡æ–™

### ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–
- ONNX Runtime Performance Tuning: https://onnxruntime.ai/docs/performance/
- PyTorch JIT Script: https://pytorch.org/docs/stable/jit.html
- CUDA Graph Optimization: https://developer.nvidia.com/blog/cuda-graphs/

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–
- Welford's Online Algorithm: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
- Ring Buffer Implementation: https://en.wikipedia.org/wiki/Circular_buffer
- Low-Latency Trading Systems: "Trading and Exchanges" by Larry Harris

---

**æ›´æ–°å±¥æ­´**:
- 2025-10-21: åˆç‰ˆä½œæˆï¼ˆãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰æˆ¦ç•¥å¯¾å¿œã€Phase 1ä»•æ§˜ç¢ºå®šï¼‰
