# PREPROCESSOR_SPEC.md

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 2.1
**æ›´æ–°æ—¥**: 2025-10-25
**è²¬ä»»è€…**: core-team
**å‡¦ç†æ®µéš**: ç¬¬3æ®µéš: å‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ãƒ»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ï¼‰

---

## ğŸ“‹ ç›®çš„

`src/preprocessor.py` ãŒ**ç¬¬2æ®µéšã§è¨ˆç®—æ¸ˆã¿ã®ç‰¹å¾´é‡**ã‚’å­¦ç¿’å¯èƒ½ãªå½¢å¼ã«å¤‰æ›ã™ã‚‹ã€‚

**è²¬ä»»ç¯„å›²**:
- ç‰¹å¾´é‡ã®æ­£è¦åŒ–ï¼ˆRobustScalerç­‰ï¼‰
- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ï¼ˆæ™‚ç³»åˆ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç”Ÿæˆï¼‰
- å“è³ªæ¤œè¨¼ï¼ˆNaN/å®šæ•°åˆ—/é«˜ç›¸é–¢é™¤å¤–ï¼‰
- æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»

**å‡¦ç†æ®µéšã®åˆ†é›¢**:
- **ç¬¬1æ®µéšï¼ˆãƒ‡ãƒ¼ã‚¿åé›†ï¼‰**: `src/data_collector.py` â†’ `data/data_collector.h5`
- **ç¬¬2æ®µéšï¼ˆç‰¹å¾´é‡è¨ˆç®—ï¼‰**: `src/feature_calculator.py` â†’ `data/feature_calculator.h5`
- **ç¬¬3æ®µéšï¼ˆå‰å‡¦ç†ï¼‰**: `src/preprocessor.py` â†’ `data/preprocessor.h5`

---

## ğŸ”„ å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
å…¥åŠ›: models/*_features.h5ï¼ˆç¬¬2æ®µéšã§ç”Ÿæˆï¼‰
  â”œâ”€ features: (N, 50-80) è¨ˆç®—æ¸ˆã¿ç‰¹å¾´é‡
  â”œâ”€ feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
  â””â”€ metadata: è¨ˆç®—çµ±è¨ˆæƒ…å ±
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—1: HDF5ãƒ­ãƒ¼ãƒ‰]
  - ç¬¬2æ®µéšã§è¨ˆç®—æ¸ˆã¿ã®ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—2: å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°]
  - NaN/Inf æ¤œå‡ºãƒ»é™¤å¤–
  - å®šæ•°åˆ—é™¤å¤–ï¼ˆIQR < 1e-6ï¼‰
  - é«˜ç›¸é–¢ãƒšã‚¢é™¤å¤–ï¼ˆ|Ï| > 0.95ï¼‰
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—3: æ­£è¦åŒ–]
  - RobustScaleré©ç”¨ï¼ˆå¤–ã‚Œå€¤è€æ€§ï¼‰
  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ï¼ˆé€†å¤‰æ›ç”¨ï¼‰
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—4: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–]
  - ãƒãƒ«ãƒTFåˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ä½œæˆ
    - M1: (N, 480, F)  # 8æ™‚é–“
    - M5: (N, 288, F)  # 24æ™‚é–“
    - M15: (N, 192, F) # 48æ™‚é–“
    - H1: (N, 96, F)   # 4æ—¥
    - H4: (N, 48, F)   # 8æ—¥
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—5: æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»]
  - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †åºç¢ºèª
  - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ã‚ˆã‚Šæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿æ··å…¥ãƒã‚§ãƒƒã‚¯
    â†“
[ã‚¹ãƒ†ãƒƒãƒ—6: å“è³ªçµ±è¨ˆå‡ºåŠ›]
  - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
  - æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹çµ±è¨ˆ
    â†“
å‡ºåŠ›: data/preprocessor.h5
  â”œâ”€ sequences: Dict[str, array] # TFåˆ¥ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
  â”œâ”€ scaler_params: bytes (JSON) # æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  â”œâ”€ feature_names: array         # æœ€çµ‚ç‰¹å¾´é‡å
  â””â”€ metadata: bytes (JSON)       # å‡¦ç†çµ±è¨ˆ

â€» æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã€JSTæ—¥æ™‚ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã§ãƒªãƒãƒ¼ãƒ é€€é¿
  ä¾‹: data/20251023_143045_preprocessor.h5
```

### ç¬¬2æ®µéšã¨ã®åˆ†é›¢

| å‡¦ç† | ç¬¬2æ®µéš (feature_calculator.py) | ç¬¬3æ®µéš (preprocessor.py) |
|------|----------------------------------|---------------------------|
| **ç‰¹å¾´é‡è¨ˆç®—** | âœ… RSI, MACD, ATR, ä¾¡æ ¼å¤‰åŒ–ç­‰ | âŒ è¨ˆç®—æ¸ˆã¿ã‚’èª­ã¿è¾¼ã‚€ã®ã¿ |
| **TFé–“ç‰¹å¾´** | âœ… M5-M1å·®åˆ†, ç›¸é–¢ç­‰ | âŒ è¨ˆç®—æ¸ˆã¿ã‚’èª­ã¿è¾¼ã‚€ã®ã¿ |
| **æ­£è¦åŒ–** | âŒ ç”Ÿå€¤ã®ã¾ã¾å‡ºåŠ› | âœ… RobustScaleré©ç”¨ |
| **ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–** | âŒ DataFrameå½¢å¼ | âœ… TFåˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç”Ÿæˆ |
| **å“è³ªãƒ•ã‚£ãƒ«ã‚¿** | âŒ å…¨ç‰¹å¾´é‡å‡ºåŠ› | âœ… NaN/ç›¸é–¢/åˆ†æ•£é™¤å¤– |

---

## ğŸ“Š å‡¦ç†è©³ç´°

### 1. å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
def filter_features(features: pd.DataFrame) -> pd.DataFrame:
    """
    å“è³ªåŸºæº–ã«æº€ãŸãªã„ç‰¹å¾´é‡ã‚’é™¤å¤–
    
    é™¤å¤–æ¡ä»¶:
    - NaN/Inf å«æœ‰ç‡ > 1%ï¼ˆåˆ—å˜ä½ï¼‰
    - NaN/Inf å«æœ‰è¡Œï¼ˆè¡Œå˜ä½ã§å®Œå…¨é™¤å¤–ï¼‰ â† è¿½åŠ 
    - IQR < 1e-6ï¼ˆå®šæ•°åˆ—ï¼‰
    - ä»–ç‰¹å¾´ã¨ã®ç›¸é–¢ |Ï| > 0.95
    """
    # 1. NaN/Infå«æœ‰åˆ—é™¤å¤–ï¼ˆåˆ—å˜ä½ï¼‰
    nan_ratio = features.isna().sum() / len(features)
    inf_ratio = np.isinf(features.select_dtypes(include=[np.number])).sum() / len(features)
    valid_cols = (nan_ratio + inf_ratio) <= 0.01
    features = features.loc[:, valid_cols]
    
    # 2. NaN/Infå«æœ‰è¡Œé™¤å¤–ï¼ˆè¡Œå˜ä½ã§å®Œå…¨é™¤å¤–ï¼‰ â† è¿½åŠ 
    features = features.replace([np.inf, -np.inf], np.nan)
    features = features.dropna()
    logger.info(f"   NaN/Infå«æœ‰è¡Œé™¤å¤–: {len(features)} è¡Œæ®‹å­˜")
    
    # 3. å®šæ•°åˆ—é™¤å¤–
    from scipy.stats import iqr
    feature_iqr = features.apply(iqr)
    features = features.loc[:, feature_iqr >= 1e-6]
    
    # 4. é«˜ç›¸é–¢ãƒšã‚¢é™¤å¤–ï¼ˆä¸Šä¸‰è§’èµ°æŸ»ï¼‰
    corr_matrix = features.corr().abs()
    upper_tri = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    to_drop = [col for col in upper_tri.columns 
               if any(upper_tri[col] > 0.95)]
    features = features.drop(columns=to_drop)
    
    return features
```

### 2. æ­£è¦åŒ–ï¼ˆRobustScalerï¼‰

```python
from sklearn.preprocessing import RobustScaler

def normalize_features(features: pd.DataFrame) -> Tuple[np.ndarray, dict]:
    """
    RobustScalerã§æ­£è¦åŒ–ï¼ˆå¤–ã‚Œå€¤è€æ€§ï¼‰
    
    Returns:
        normalized: æ­£è¦åŒ–å¾Œã®é…åˆ—
        params: é€†å¤‰æ›ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆcenter_, scale_ï¼‰
    """
    scaler = RobustScaler()
    normalized = scaler.fit_transform(features)
    
    # æ­£è¦åŒ–å¾Œã®æ¤œè¨¼ï¼ˆNaN/Infæ¤œå‡ºï¼‰ â† è¿½åŠ 
    if np.isnan(normalized).any() or np.isinf(normalized).any():
        raise ValueError("æ­£è¦åŒ–å¾Œã«NaN/InfãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    
    params = {
        'center_': scaler.center_.tolist(),
        'scale_': scaler.scale_.tolist(),
        'feature_names': features.columns.tolist()
    }
    
    return normalized, params
```

### 3. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–

```python
def create_sequences(
    features: np.ndarray,
    tf_configs: Dict[str, int]
) -> Dict[str, np.ndarray]:
    """
    TFåˆ¥ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç”Ÿæˆ

    Args:
        features: (N, F) æ­£è¦åŒ–æ¸ˆã¿ç‰¹å¾´é‡
        tf_configs: {'M1': 480, 'M5': 288, ...} # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é•·

    Returns:
        {'M1': (N-480, 480, F), 'M5': (N-288, 288, F), ...}
    """
    sequences = {}

    for tf_name, window_size in tf_configs.items():
        seq_list = []
        for i in range(len(features) - window_size):
            seq_list.append(features[i:i+window_size])
        sequences[tf_name] = np.array(seq_list)

    return sequences
```

#### TFåˆ¥ãƒã‚¹ã‚¯å‡¦ç†

**ç›®çš„**: TFé•·å·®ç•°ã«ã‚ˆã‚‹æš—é»™çš„forward fillã¨æƒ…å ±æ­ªã¿ï¼ˆé«˜æ™‚é–“è»¸ã¸ã®å‹¾é…é›†ä¸­ï¼‰ã‚’é˜²æ­¢ã™ã‚‹ã€‚

**å®Ÿè£…**: per-TF maskè¡Œåˆ— + æœ‰åŠ¹ç‡é–¾å€¤ï¼ˆmax_valid_ratio=0.95ï¼‰ã«ã‚ˆã‚‹å“è³ªç®¡ç†

```python
def create_sequences_with_mask(
    features_per_tf: Dict[str, pd.DataFrame],  # TFåˆ¥ç”Ÿãƒ‡ãƒ¼ã‚¿
    tf_configs: Dict[str, int],
    max_valid_ratio: float = 0.95,
    filled_weight_decay: float = 0.6
) -> Dict[str, Any]:
    """
    TFåˆ¥ãƒã‚¹ã‚¯å‡¦ç†ä»˜ãã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ

    Args:
        features_per_tf: {'M1': DataFrame(N_m1, F), 'M5': DataFrame(N_m5, F), ...}
        tf_configs: {'M1': 480, 'M5': 288, ...}
        max_valid_ratio: æ¬ æç‡é–¾å€¤ï¼ˆè¶…éæ™‚TFé™¤å¤–ï¼‰
        filled_weight_decay: forward fillå¾Œã®æå¤±é‡ã¿æ¸›è¡°ç‡

    Returns:
        {
            'sequences': {'M1': (N, 480, F), ...},
            'masks': {'M1': (N, 480), ...},  # 1=æœ‰åŠ¹, 0=æ¬ æ/filled
            'loss_weights': {'M1': (N, 480), ...},  # å­¦ç¿’é‡ã¿
            'tf_validity': {'M1': 0.98, ...},  # TFåˆ¥æœ‰åŠ¹ç‡
            'excluded_tfs': ['H4']  # é™¤å¤–ã•ã‚ŒãŸTF
        }
    """
    result = {
        'sequences': {},
        'masks': {},
        'loss_weights': {},
        'tf_validity': {},
        'excluded_tfs': []
    }

    for tf_name, window_size in tf_configs.items():
        if tf_name not in features_per_tf:
            continue

        df = features_per_tf[tf_name]
        N = len(df)

        # æ¬ æãƒ•ãƒ©ã‚°ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ï¼‰
        is_missing = df.isna().any(axis=1).values  # (N,)

        # Forward fillé©ç”¨
        df_filled = df.fillna(method='ffill')

        # filled ãƒ•ãƒ©ã‚°
        was_filled = is_missing & ~df_filled.isna().any(axis=1).values

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–
        seq_list = []
        mask_list = []
        weight_list = []

        for i in range(N - window_size):
            window = df_filled.iloc[i:i+window_size].values  # (window, F)
            seq_list.append(window)

            # ãƒã‚¹ã‚¯ç”Ÿæˆï¼ˆæ¬ æ+filledç®‡æ‰€ã‚’0ï¼‰
            window_missing = is_missing[i:i+window_size]
            window_filled = was_filled[i:i+window_size]
            mask = ~(window_missing | window_filled)  # æœ‰åŠ¹=True
            mask_list.append(mask.astype(float))

            # æå¤±é‡ã¿ç”Ÿæˆ
            weight = np.ones(window_size, dtype=float)
            weight[window_filled] = filled_weight_decay  # filledç®‡æ‰€ã‚’æ¸›è¡°
            weight_list.append(weight)

        sequences = np.array(seq_list)  # (N, window, F)
        masks = np.array(mask_list)  # (N, window)
        weights = np.array(weight_list)  # (N, window)

        # TFåˆ¥æœ‰åŠ¹ç‡è¨ˆç®—
        valid_ratio = masks.mean()

        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if valid_ratio < (1.0 - max_valid_ratio):
            logger.warning(
                f"TF {tf_name} é™¤å¤–: æœ‰åŠ¹ç‡={valid_ratio:.3f} < {1.0-max_valid_ratio:.3f}"
            )
            result['excluded_tfs'].append(tf_name)
            continue

        result['sequences'][tf_name] = sequences
        result['masks'][tf_name] = masks
        result['loss_weights'][tf_name] = weights
        result['tf_validity'][tf_name] = valid_ratio

    return result
```

**æ•´åˆ—å›³**:
```
TFåˆ¥ãƒ‡ãƒ¼ã‚¿æ•´åˆ—ï¼ˆæ¬ æå·®ç•°ã®å¯è¦–åŒ–ï¼‰

M1:  [â– â– â– â– â– â– â– â– â– â–¡â–¡â– â– â– â– â– â– â– â– â– ]  â† æ¬ æ2ç®‡æ‰€ï¼ˆâ–¡ï¼‰
M5:  [â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– ]  â† æ¬ æãªã—
H1:  [â– â– â–¡â–¡â–¡â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– ]  â† æ¬ æ3ç®‡æ‰€
H4:  [â–¡â–¡â–¡â–¡â–¡â–¡â– â– â– â– â– â– â– â– â– â– â– â– â– â– ]  â† æ¬ æ6ç®‡æ‰€ï¼ˆé™¤å¤–å€™è£œï¼‰

maské©ç”¨å¾Œ:
M1:  [1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1]
M5:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
H1:  [1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
H4:  [0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]  â† æœ‰åŠ¹ç‡60% â†’ é™¤å¤–

loss_weighté©ç”¨:
M1:  [1.0 1.0 ... 0.6 0.6 1.0 ...]  â† filledç®‡æ‰€ã¯0.6
```

**æˆåŠŸæŒ‡æ¨™**:
- ç„¡åŠ¹TFé™¤å¤–ç‡ < 5%
- æ¬ æè£œå®Œãƒãƒ¼å­¦ç¿’é‡ã¿ â‰¤ 0.6
- è£œå®Œã‚µãƒ³ãƒ—ãƒ«ã®æœŸå¾…å€¤èª¤å·® |Î”| < 0.3pips

**æ¤œè¨¼**:
```python
def test_tf_mask_processing():
    """TFåˆ¥ãƒã‚¹ã‚¯å‡¦ç†ã®æ¤œè¨¼"""
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆM5ã«æ¬ æãªã—ã€H4ã«40%æ¬ æï¼‰
    features_per_tf = {
        'M5': pd.DataFrame(np.random.randn(1000, 50)),
        'H4': pd.DataFrame(np.random.randn(1000, 50))
    }
    # H4ã«æ¬ ææ³¨å…¥
    features_per_tf['H4'].iloc[:400] = np.nan

    result = create_sequences_with_mask(
        features_per_tf,
        tf_configs={'M5': 288, 'H4': 48},
        max_valid_ratio=0.95
    )

    # H4ãŒé™¤å¤–ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    assert 'H4' in result['excluded_tfs'], "H4ãŒé™¤å¤–ã•ã‚Œã‚‹ã¹ã"
    assert 'M5' in result['sequences'], "M5ã¯æ®‹ã‚‹ã¹ã"

    # M5ã®æœ‰åŠ¹ç‡ç¢ºèª
    assert result['tf_validity']['M5'] > 0.95, "M5æœ‰åŠ¹ç‡ãŒé«˜ã„ã¹ã"
```

#### æ¬ æè£œå®Œæ™‚ã®å­¦ç¿’é‡ã¿æ¸›è¡°

**ç›®çš„**: forward fillè£œå®Œã«ã‚ˆã‚‹æ–¹å‘ç¢ºç‡éä¿¡ï¼ˆconfidenceéå¤§ï¼‰ã‚’é˜²æ­¢ã™ã‚‹ã€‚

**å®Ÿè£…**: filled_flagä»˜ä¸ â†’ loss_weightæŒ‡æ•°æ¸›è¡°ï¼ˆé€£ç¶šè£œå®Œæ™‚ï¼‰ â†’ å“è³ªãƒ­ã‚°å‡ºåŠ›

```python
def compute_filled_loss_weights(
    filled_flags: np.ndarray,  # (N, window) bool
    base_weight: float = 1.0,
    decay_per_consecutive: float = 0.9,
    min_weight: float = 0.3
) -> np.ndarray:
    """
    é€£ç¶šæ¬ æè£œå®Œã«å¯¾ã™ã‚‹æŒ‡æ•°æ¸›è¡°é‡ã¿è¨ˆç®—

    Args:
        filled_flags: filledç®‡æ‰€ã®ãƒ•ãƒ©ã‚° (True=filled)
        base_weight: åŸºæœ¬é‡ã¿
        decay_per_consecutive: é€£ç¶šã”ã¨ã®æ¸›è¡°ç‡ï¼ˆä¾‹: 0.9^kï¼‰
        min_weight: æœ€å°é‡ã¿

    Returns:
        loss_weights: (N, window) å­¦ç¿’é‡ã¿
    """
    N, window = filled_flags.shape
    weights = np.full((N, window), base_weight, dtype=float)

    for i in range(N):
        consecutive_count = 0
        for j in range(window):
            if filled_flags[i, j]:
                consecutive_count += 1
                # æŒ‡æ•°æ¸›è¡°: base_weight * (decay^k)
                decay_factor = decay_per_consecutive ** consecutive_count
                weights[i, j] = max(base_weight * decay_factor, min_weight)
            else:
                consecutive_count = 0  # ãƒªã‚»ãƒƒãƒˆ

    return weights


# å“è³ªãƒ­ã‚°å‡ºåŠ›
class FilledDataQualityLogger:
    """è£œå®Œãƒ‡ãƒ¼ã‚¿ã®å“è³ªç›£è¦–"""

    def __init__(self):
        self.filled_ratios = []
        self.expectancy_errors = []

    def log_batch(self, filled_flags, predictions, targets):
        """
        ãƒãƒƒãƒã”ã¨ã®è£œå®Œãƒ‡ãƒ¼ã‚¿å“è³ªã‚’è¨˜éŒ²

        Args:
            filled_flags: (batch, seq) bool
            predictions: (batch,) predicted values
            targets: (batch,) true values
        """
        # è£œå®Œç‡
        filled_ratio = filled_flags.sum() / filled_flags.size
        self.filled_ratios.append(filled_ratio)

        # filledç®‡æ‰€ã®æœŸå¾…å€¤èª¤å·®
        filled_mask = filled_flags.any(axis=1)  # ãƒãƒƒãƒãƒ¬ãƒ™ãƒ«
        if filled_mask.sum() > 0:
            filled_preds = predictions[filled_mask]
            filled_targets = targets[filled_mask]
            error = np.abs(filled_preds - filled_targets).mean()
            self.expectancy_errors.append(error)

    def check_quality_kpi(self):
        """KPIé”æˆç¢ºèª"""
        mean_error = np.mean(self.expectancy_errors)

        if mean_error > 0.3:  # 0.3pipsé–¾å€¤
            logger.warning(
                f"è£œå®Œã‚µãƒ³ãƒ—ãƒ«æœŸå¾…å€¤èª¤å·®è¶…é: {mean_error:.4f} pips > 0.3 pips"
            )
            return False

        return True


# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—å†…
quality_logger = FilledDataQualityLogger()

for batch in dataloader:
    sequences, masks, filled_flags = batch['sequences'], batch['masks'], batch['filled']

    # é‡ã¿è¨ˆç®—
    loss_weights = compute_filled_loss_weights(filled_flags)

    # äºˆæ¸¬
    predictions = model(sequences, masks)

    # æå¤±è¨ˆç®—ï¼ˆé‡ã¿é©ç”¨ï¼‰
    loss = weighted_loss(predictions, targets, loss_weights)

    # å“è³ªãƒ­ã‚°
    quality_logger.log_batch(filled_flags, predictions.detach(), targets)

# ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚
quality_logger.check_quality_kpi()
```

**æˆåŠŸæŒ‡æ¨™**:
- è£œå®Œã‚µãƒ³ãƒ—ãƒ«ã®æœŸå¾…å€¤èª¤å·® |Î”| < 0.3pips
- é€£ç¶šè£œå®Œ3å›ä»¥ä¸Šã®ç®‡æ‰€: é‡ã¿ â‰¤ 0.5

**æ¤œè¨¼**:
```python
def test_filled_weight_decay():
    """é€£ç¶šè£œå®Œã®é‡ã¿æ¸›è¡°ã‚’æ¤œè¨¼"""
    # é€£ç¶š3ç®‡æ‰€filled
    filled_flags = np.array([[False, True, True, True, False, False]])

    weights = compute_filled_loss_weights(
        filled_flags,
        base_weight=1.0,
        decay_per_consecutive=0.9,
        min_weight=0.3
    )

    # é€£ç¶š1å›ç›®: 1.0 * 0.9 = 0.9
    # é€£ç¶š2å›ç›®: 1.0 * 0.9^2 = 0.81
    # é€£ç¶š3å›ç›®: 1.0 * 0.9^3 = 0.729
    assert abs(weights[0, 1] - 0.9) < 0.01
    assert abs(weights[0, 2] - 0.81) < 0.01
    assert abs(weights[0, 3] - 0.729) < 0.01
    assert weights[0, 0] == 1.0
    assert weights[0, 4] == 1.0
```

### 4. æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»

```python
def check_future_leak(
    sequences: Dict[str, np.ndarray],
    timestamps: np.ndarray,
    target_times: np.ndarray
) -> bool:
    """
    ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ã‚ˆã‚Šæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ãŒæ··å…¥ã—ã¦ã„ãªã„ã‹ç¢ºèª
    
    Returns:
        True: ãƒªãƒ¼ã‚¯ãªã—
        False: ãƒªãƒ¼ã‚¯æ¤œå‡º
    """
    for tf_name, seq in sequences.items():
        seq_end_times = timestamps[len(timestamps) - len(seq):]
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æœ€çµ‚æ™‚åˆ» < ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»
        if not all(seq_end_times < target_times[:len(seq_end_times)]):
            logger.error(f"æœªæ¥ãƒªãƒ¼ã‚¯æ¤œå‡º: {tf_name}")
            return False
    
    return True
```

---

## ğŸ“Š HDF5ã‚¹ã‚­ãƒ¼ãƒ

### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: `models/*_preprocessed.h5`

```python
with h5py.File('preprocessed.h5', 'w') as f:
    # TFåˆ¥ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
    sequences_group = f.create_group('sequences')
    sequences_group.create_dataset('M1', data=seq_M1, dtype='float32')
    sequences_group.create_dataset('M5', data=seq_M5, dtype='float32')
    sequences_group.create_dataset('M15', data=seq_M15, dtype='float32')
    sequences_group.create_dataset('H1', data=seq_H1, dtype='float32')
    sequences_group.create_dataset('H4', data=seq_H4, dtype='float32')
    
    # æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆJSONï¼‰
    f.create_dataset('scaler_params',
                     data=json.dumps(scaler_params).encode())
    
    # æœ€çµ‚ç‰¹å¾´é‡å
    f.create_dataset('feature_names',
                     data=[name.encode() for name in feature_names])
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    f.create_dataset('metadata',
                     data=json.dumps({
                         'input_features': 60,
                         'filtered_features': 52,
                         'nan_filtered': 3,
                         'const_filtered': 2,
                         'corr_filtered': 3,
                         'sequence_lengths': {
                             'M1': 480, 'M5': 288, 'M15': 192,
                             'H1': 96, 'H4': 48
                         },
                         'future_leak_check': 'PASS',
                         'timestamp': '2025-10-22T12:00:00Z'
                     }).encode())
```

---

## å“è³ªçµ±è¨ˆ

ãƒ­ã‚°å‡ºåŠ›ä¾‹:
```
ğŸ“‚ ç¬¬2æ®µéšç‰¹å¾´é‡èª­è¾¼: 60åˆ—
ğŸ” å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°:
   - NaNé™¤å¤–: 3åˆ—
   - å®šæ•°åˆ—é™¤å¤–: 2åˆ—
   - é«˜ç›¸é–¢é™¤å¤–: 3åˆ—
   â†’ æœ€çµ‚ç‰¹å¾´é‡: 52åˆ—

âš™ï¸ æ­£è¦åŒ–: RobustScaleré©ç”¨
   - centerç¯„å›²: [-0.05, 0.08]
   - scaleç¯„å›²: [0.3, 2.1]

ğŸ“Š ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–:
   - M1: (45000, 480, 52)
   - M5: (45000, 288, 52)
   - M15: (45000, 192, 52)
   - H1: (45000, 96, 52)
   - H4: (45000, 48, 52)

âœ… æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»: PASS
   å‡ºåŠ›: models/fx_mtf_20251022_120000_preprocessed.h5
```

---

## ğŸš¨ ã‚¨ãƒ©ãƒ¼æ¡ä»¶

| æ¡ä»¶ | é–¾å€¤ | å¯¾å¿œ |
|------|------|------|
| NaN/Inf å«æœ‰ç‡ | >1% | ã‚¨ãƒ©ãƒ¼çµ‚äº†ï¼ˆç¬¬2æ®µéšç¢ºèªï¼‰ |
| ãƒ•ã‚£ãƒ«ã‚¿å¾Œç‰¹å¾´é‡æ•° | <30åˆ— | è­¦å‘Šï¼ˆç¬¬2æ®µéšã§ç‰¹å¾´è¿½åŠ æ¤œè¨ï¼‰ |
| æœªæ¥ãƒªãƒ¼ã‚¯æ¤œå‡º | 1ä»¶ã§ã‚‚ | ã‚¨ãƒ©ãƒ¼çµ‚äº† |
| ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆå¤±æ•— | æ™‚ç³»åˆ—ä¸æ•´åˆ | ã‚¨ãƒ©ãƒ¼çµ‚äº†ï¼ˆç¬¬1æ®µéšç¢ºèªï¼‰ |
| æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç•°å¸¸ | center/scale NaN | ã‚¨ãƒ©ãƒ¼çµ‚äº† |

---

## ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

| ãƒ•ã‚¡ã‚¤ãƒ«å | å†…å®¹ | Gitç®¡ç† |
|-----------|------|---------|
| `data/preprocessor.h5` | æ­£è¦åŒ–æ¸ˆã¿ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ | âŒ é™¤å¤– |
| `data/preprocessor_report.json` | æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»å“è³ªçµ±è¨ˆ | âŒ é™¤å¤– |
| `data/preprocessor_report.md` | äººé–“å¯èª­ãƒ¬ãƒãƒ¼ãƒˆ | âŒ é™¤å¤– |

**ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ `YYYYMMDD_HHMMSS_preprocessor.<ext>` ã«ãƒªãƒãƒ¼ãƒ  (JST)

ä¾‹: `20251024_143500_preprocessor.h5`

---

## ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### JSONãƒ¬ãƒãƒ¼ãƒˆ (`data/preprocessor_report.json`)

æ¬¡å‡¦ç†ï¼ˆå­¦ç¿’ï¼‰ãŒèª­ã¿è¾¼ã‚€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±:

```json
{
  "timestamp": "2025-10-24T14:35:15+09:00",
  "process": "preprocessor",
  "version": "1.0",
  "input": {
    "file": "data/feature_calculator.h5",
    "source_report": "data/feature_calculator_report.json",
    "features": 66,
    "samples": 2500000
  },
  "output": {
    "file": "data/preprocessor.h5",
    "size_mb": 350
  },
  "filtering": {
    "original_features": 66,
    "removed_nan": 2,
    "removed_constant": 1,
    "removed_correlation": 3,
    "final_features": 60,
    "removed_columns": ["col_A", "col_B", "col_C", "col_D", "col_E", "col_F"]
  },
  "normalization": {
    "method": "RobustScaler",
    "scaler_params": {
      "feature_1": {"center": 0.0015, "scale": 0.0023},
      "feature_2": {"center": 0.0008, "scale": 0.0019},
      "...": "60 features total"
    }
  },
  "sequences": {
    "window_size": 360,
    "stride": 1,
    "total_sequences": 2499640,
    "valid_sequences": 2499500,
    "dropped_sequences": 140
  },
  "quality": {
    "avg_quality_score": 0.92,
    "low_quality_sequences": 124975,
    "low_quality_ratio": 0.05,
    "future_leak_check": "passed",
    "monotonic_check": "passed"
  },
  "performance": {
    "execution_time_sec": 45,
    "memory_peak_mb": 8000
  }
}
```

### Markdownãƒ¬ãƒãƒ¼ãƒˆ (`data/preprocessor_report.md`)

äººé–“ã«ã‚ˆã‚‹æ¤œè¨¼ç”¨ã®å¯èª­ãƒ¬ãƒãƒ¼ãƒˆ:

```markdown
# å‰å‡¦ç† å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: 2025-10-24 14:35:15 JST  
**å‡¦ç†æ™‚é–“**: 45ç§’  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0

## ğŸ“Š å…¥åŠ›

- **å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«**: `data/feature_calculator.h5`
- **ç‰¹å¾´é‡æ•°**: 66åˆ—
- **ã‚µãƒ³ãƒ—ãƒ«æ•°**: 2,500,000

## ğŸ¯ å‡¦ç†çµæœ

- **å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«**: `data/preprocessor.h5`
- **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: 350 MB
- **æœ€çµ‚ç‰¹å¾´é‡æ•°**: 60åˆ—
- **ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°**: 2,499,500

### ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ

| é …ç›® | é™¤å¤–æ•° | æ®‹å­˜æ•° |
|-----|--------|--------|
| å…ƒã®ç‰¹å¾´é‡ | - | 66 |
| NaNåˆ—é™¤å¤– | 2 | 64 |
| å®šæ•°åˆ—é™¤å¤– | 1 | 63 |
| é«˜ç›¸é–¢é™¤å¤– | 3 | 60 |

**é™¤å¤–ã•ã‚ŒãŸåˆ—**: col_A, col_B, col_C, col_D, col_E, col_F

### æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

**æ‰‹æ³•**: RobustScaler (IQRåŸºæº–)

æ­£è¦åŒ–ä¾‹ (æœ€åˆã®5ç‰¹å¾´é‡):

| ç‰¹å¾´é‡å | Center | Scale |
|---------|--------|-------|
| M1_price_change_pips | 0.0015 | 0.0023 |
| M5_price_change_pips | 0.0008 | 0.0019 |
| M15_price_change_pips | 0.0012 | 0.0021 |
| spread_pips | 0.0005 | 0.0003 |
| tick_volume | 1200.0 | 850.5 |

### ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ

| é …ç›® | å€¤ |
|-----|-----|
| ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º | 360 (6æ™‚é–“) |
| ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ | 1 |
| ç·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ | 2,499,640 |
| æœ‰åŠ¹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ | 2,499,500 |
| é™¤å¤–ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ | 140 (0.006%) |

## ğŸ“ˆ å“è³ªçµ±è¨ˆ

| é …ç›® | å€¤ |
|-----|-----|
| å¹³å‡å“è³ªã‚¹ã‚³ã‚¢ | 0.92 |
| ä½å“è³ªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ | 124,975 (5.0%) |
| æœªæ¥ãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ | âœ… åˆæ ¼ |
| å˜èª¿æ€§ãƒã‚§ãƒƒã‚¯ | âœ… åˆæ ¼ |

## âš™ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

- **å®Ÿè¡Œæ™‚é–“**: 45ç§’
- **ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª**: 8,000 MB

## âš ï¸ è­¦å‘Šãƒ»æ³¨æ„äº‹é …

- 6åˆ—ã‚’å“è³ªãƒ•ã‚£ãƒ«ã‚¿ã§é™¤å¤–ï¼ˆNaNã€å®šæ•°ã€é«˜ç›¸é–¢ï¼‰
- ä½å“è³ªã‚·ãƒ¼ã‚±ãƒ³ã‚¹5.0%ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç•°å¸¸æœŸé–“ï¼‰
- 140ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’é™¤å¤–ï¼ˆNaNå«æœ‰ï¼‰

## âœ… æ¤œè¨¼çµæœ

- âœ… å…¨ç‰¹å¾´é‡ã®æ­£è¦åŒ–å®Œäº†
- âœ… æœªæ¥ãƒªãƒ¼ã‚¯ãªã—
- âœ… æ™‚ç³»åˆ—ã®å˜èª¿æ€§ç¢ºèª
- âœ… æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜å®Œäº†
```

---

## ğŸ“ ãƒ­ã‚°å‡ºåŠ›

### æ™‚åˆ»è¡¨ç¤ºãƒ«ãƒ¼ãƒ«
- **å…¨ãƒ­ã‚°**: æ—¥æœ¬æ™‚é–“(JST)ã§è¡¨ç¤º
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: `YYYY-MM-DD HH:MM:SS JST`
- **ãƒ‡ãƒ¼ã‚¿æœŸé–“**: æ—¥æœ¬æ™‚é–“ã§æ˜è¨˜
- **è©³ç´°**: [TIMEZONE_UTILS_SPEC.md](./utils/TIMEZONE_UTILS_SPEC.md)

### å‡ºåŠ›ä¾‹
```
ğŸ”„ ç¬¬3æ®µéš: å‰å‡¦ç†é–‹å§‹ [2025-10-23 23:50:30 JST]
ğŸ“‚ å…¥åŠ›: models/fx_mtf_20251022_100000_features.h5
   æœŸé–“: 2024-01-01 00:00:00 JST ï½ 2024-12-31 23:59:00 JST
   ç‰¹å¾´é‡: 58åˆ—

ğŸ” å…¥åŠ›å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
   å¹³å‡ã‚¹ã‚³ã‚¢: 0.92, ä½å“è³ªã‚·ãƒ¼ã‚±ãƒ³ã‚¹: 5%

ğŸ§¹ å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
   NaNé™¤å¤–: 2åˆ—, å®šæ•°åˆ—é™¤å¤–: 1åˆ—, é«˜ç›¸é–¢é™¤å¤–: 3åˆ—
   æ®‹å­˜ç‰¹å¾´é‡: 52åˆ—

ğŸ“Š æ­£è¦åŒ–å®Ÿæ–½
   RobustScaleré©ç”¨å®Œäº†

âœ… å‰å‡¦ç†å®Œäº† [2025-10-23 23:51:15 JST]
   å‡ºåŠ›: models/fx_mtf_20251022_100000_preprocessed.h5
```

---

## âš™ï¸ è¨­å®šä¾‹

```yaml
# config/preprocessing.yaml
preprocessing:
  # å“è³ªãƒ•ã‚£ãƒ«ã‚¿
  quality_filter:
    max_nan_ratio: 0.01          # 1%
    min_iqr: 1.0e-6              # å®šæ•°åˆ—é–¾å€¤
    max_correlation: 0.95         # é«˜ç›¸é–¢é–¾å€¤
  
  # æ­£è¦åŒ–
  normalization:
    method: robust                # robust | standard | minmax
    quantile_range: [25, 75]     # RobustScalerç”¨
  
  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–
  sequences:
    M1: 480   # 8æ™‚é–“
    M5: 288   # 24æ™‚é–“
    M15: 192  # 48æ™‚é–“
    H1: 96    # 4æ—¥
    H4: 48    # 8æ—¥
  
  # æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»
  leak_check:
    enabled: true
    strict_mode: true
  
  # ã‚¨ãƒ©ãƒ¼é–¾å€¤
  thresholds:
    min_features_after_filter: 30
```

---

## ğŸ”— é–¢é€£ä»•æ§˜æ›¸

- **å‰æ®µéš**:
  - ç¬¬1æ®µéš: [DATA_COLLECTOR_SPEC.md](./DATA_COLLECTOR_SPEC.md) - ç”Ÿãƒ‡ãƒ¼ã‚¿åé›†
  - ç¬¬2æ®µéš: [FEATURE_CALCULATOR_SPEC.md](./FEATURE_CALCULATOR_SPEC.md) - ç‰¹å¾´é‡è¨ˆç®—
- **æ¬¡å·¥ç¨‹**: ç¬¬4æ®µéš: [TRAINER_SPEC.md](./TRAINER_SPEC.md) - å­¦ç¿’
- **å‚ç…§**:
  - [FUTURE_LEAK_PREVENTION_SPEC.md](./validator/FUTURE_LEAK_PREVENTION_SPEC.md) - æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»è©³ç´°

---

## ğŸ“Œ æ³¨æ„äº‹é …

### å‡¦ç†æ®µéšåˆ†é›¢ã®é‡è¦æ€§

**æ—§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¤±æ•—**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ç‰¹å¾´é‡è¨ˆç®— â†’ è²¬ä»»ä¸æ˜ç¢ºãƒ»ãƒ‡ãƒãƒƒã‚°å›°é›£

**æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ”¹å–„**:
```
ç¬¬2æ®µéš: ç‰¹å¾´é‡è¨ˆç®—
  - å…¥åŠ›: raw_data.h5ï¼ˆOHLCVï¼‰
  - å‡¦ç†: RSI, MACD, ATR, TFé–“å·®åˆ†ç­‰
  - å‡ºåŠ›: features.h5ï¼ˆDataFrameå½¢å¼ï¼‰

ç¬¬3æ®µéš: å‰å‡¦ç†ï¼ˆæœ¬ä»•æ§˜æ›¸ï¼‰
  - å…¥åŠ›: features.h5ï¼ˆè¨ˆç®—æ¸ˆã¿ç‰¹å¾´é‡ï¼‰
  - å‡¦ç†: æ­£è¦åŒ–ãƒ»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ãƒ»å“è³ªæ¤œè¨¼
  - å‡ºåŠ›: preprocessed.h5ï¼ˆå­¦ç¿’å¯èƒ½å½¢å¼ï¼‰
```

### å®Ÿè£…æ™‚ã®æ³¨æ„

1. **ç¬¬2æ®µéšã®è¨ˆç®—çµæœã‚’ä¿¡é ¼**: ç‰¹å¾´é‡ã®å†è¨ˆç®—ç¦æ­¢
2. **æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿å­˜å¿…é ˆ**: 
   - æ¨è«–æ™‚ã®é€†å¤‰æ›ã«å¿…è¦ï¼ˆ`scaler_params` ã‚’å¿…ãšä¿å­˜ï¼‰
   - å­¦ç¿’æ™‚ã¨æ¨è«–æ™‚ã§åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆå†è¨ˆç®—ç¦æ­¢ï¼‰
   - ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨ `scaler_params` ã‚’å¿…ãšã‚»ãƒƒãƒˆã§ç®¡ç†
   - è©³ç´°ã¯ã€Œ[ğŸ”„ æ¨è«–æ™‚ã®å¾©å…ƒ](#-æ¨è«–æ™‚ã®å¾©å…ƒ)ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‚ç…§
3. **ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®å›ºå®š**: MULTI_TF_FUSION_SPEC.md ã®è¨­è¨ˆã«æº–æ‹ 
4. **æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»ã¯å¿…é ˆ**: ã‚¨ãƒ©ãƒ¼æ™‚ã¯å³åº§ã«åœæ­¢

---

## ğŸ”® å°†æ¥æ‹¡å¼µ

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º1: åŸºæœ¬å®Ÿè£…ï¼ˆç¾åœ¨ï¼‰
- RobustScaleræ­£è¦åŒ–
- TFåˆ¥å›ºå®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
- åŸºæœ¬å“è³ªãƒ•ã‚£ãƒ«ã‚¿

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º2: æ‹¡å¼µæ©Ÿèƒ½
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆAugmentationï¼‰
  - ãƒã‚¤ã‚ºä»˜åŠ 
  - æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«å¤‰å‹•
- å‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º3: æœ€é©åŒ–
- ä¸¦åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼ˆlazy loadingï¼‰
- GPUæ­£è¦åŒ–ï¼ˆcupyçµ±åˆï¼‰

---

## ğŸ”„ æ¨è«–æ™‚ã®å¾©å…ƒ

### æ¦‚è¦

**å­¦ç¿’æ™‚ã«ä¿å­˜ã—ãŸæ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å€¤ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒã™ã‚‹æ‰‹é †**ã€‚

**é‡è¦åŸå‰‡**:
1. **å­¦ç¿’æ™‚ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨**: æ¨è«–æ™‚ã«æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†è¨ˆç®—ã—ã¦ã¯ã„ã‘ãªã„
2. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€æ„æ€§ä¿è¨¼**: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨ `scaler_params` ã‚’å¿…ãšã‚»ãƒƒãƒˆã§ç®¡ç†
3. **ç‰¹å¾´é‡é †åºã®ä¸€è‡´**: `feature_names` ã§åˆ—é †åºã‚’ä¿è¨¼

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿

**HDF5ã‹ã‚‰æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿**:

```python
import h5py
import json
import numpy as np
from sklearn.preprocessing import RobustScaler

def load_scaler_params(preprocessed_h5_path: str) -> RobustScaler:
    """
    å­¦ç¿’æ™‚ã«ä¿å­˜ã—ãŸæ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’RobustScalerã¨ã—ã¦å¾©å…ƒ
    
    Args:
        preprocessed_h5_path: å‰å‡¦ç†æ¸ˆã¿HDF5ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        å¾©å…ƒã•ã‚ŒãŸRobustScalerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        
    Raises:
        KeyError: scaler_paramsãŒå­˜åœ¨ã—ãªã„
        ValueError: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢å¼ãŒä¸æ­£
    """
    with h5py.File(preprocessed_h5_path, 'r') as f:
        # JSONå½¢å¼ã§ä¿å­˜ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        scaler_params = json.loads(f['scaler_params'][()])
        
        # RobustScalerã‚’å¾©å…ƒ
        scaler = RobustScaler()
        scaler.center_ = np.array(scaler_params['center_'])
        scaler.scale_ = np.array(scaler_params['scale_'])
        
        # ç‰¹å¾´é‡åã‚‚å¾©å…ƒï¼ˆé †åºæ¤œè¨¼ç”¨ï¼‰
        feature_names = scaler_params['feature_names']
        scaler.feature_names_in_ = np.array(feature_names)
        scaler.n_features_in_ = len(feature_names)
        
    return scaler, feature_names
```

### äºˆæ¸¬å€¤ã®é€†å¤‰æ›

**ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼ˆæ­£è¦åŒ–å€¤ï¼‰ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒ**:

```python
def inverse_transform_predictions(
    predictions_normalized: np.ndarray,
    scaler: RobustScaler
) -> np.ndarray:
    """
    æ­£è¦åŒ–ã•ã‚ŒãŸäºˆæ¸¬å€¤ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒ
    
    Args:
        predictions_normalized: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼ˆæ­£è¦åŒ–å€¤ï¼‰
        scaler: load_scaler_paramsã§å¾©å…ƒã—ãŸScaler
        
    Returns:
        å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒã•ã‚ŒãŸäºˆæ¸¬å€¤
        
    Example:
        >>> # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ï¼ˆæ­£è¦åŒ–å€¤ï¼‰
        >>> predictions_norm = model.predict(input_sequences)
        >>> # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒ
        >>> predictions_original = inverse_transform_predictions(
        ...     predictions_norm, scaler
        ... )
    """
    # inverse_transformã§å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒ
    predictions_original = scaler.inverse_transform(predictions_normalized)
    
    return predictions_original
```

### æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ä¾‹

```python
def inference_pipeline(
    raw_data: pd.DataFrame,
    preprocessed_h5_path: str,
    model_path: str
) -> np.ndarray:
    """
    æ¨è«–æ™‚ã®å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    Args:
        raw_data: ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹å¾´é‡è¨ˆç®—å‰ï¼‰
        preprocessed_h5_path: å­¦ç¿’æ™‚ã®å‰å‡¦ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        model_path: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        
    Returns:
        å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®äºˆæ¸¬å€¤
    """
    # 1. æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    scaler, feature_names = load_scaler_params(preprocessed_h5_path)
    
    # 2. åŒã˜é †åºã§ç‰¹å¾´é‡ã‚’é¸æŠãƒ»æ­£è¦åŒ–
    features = raw_data[feature_names]  # åˆ—é †åºã‚’ä¿è¨¼
    features_normalized = scaler.transform(features)  # fit_transformã§ã¯ãªãtransform
    
    # 3. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼‰
    sequences = create_sequences_for_inference(features_normalized)
    
    # 4. ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
    model = load_model(model_path)
    predictions_normalized = model.predict(sequences)
    
    # 5. å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒ
    predictions_original = inverse_transform_predictions(
        predictions_normalized, scaler
    )
    
    return predictions_original
```

### é‡è¦ãªæ³¨æ„äº‹é …

#### 1. å­¦ç¿’æ™‚ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨

âŒ **NG**: æ¨è«–æ™‚ã«å†è¨ˆç®—
```python
# æ¨è«–æ™‚ã«æ–°ãŸã«fit â†’ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãƒãƒ©ã¤ã
scaler = RobustScaler()
scaler.fit(inference_data)  # çµ¶å¯¾ç¦æ­¢
predictions_norm = model.predict(scaler.transform(inference_data))
```

âœ… **OK**: å­¦ç¿’æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
```python
# å­¦ç¿’æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
scaler, _ = load_scaler_params('data/preprocessor.h5')
predictions_norm = model.predict(scaler.transform(inference_data))
predictions_original = scaler.inverse_transform(predictions_norm)
```

#### 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€æ„æ€§ä¿è¨¼

**ãƒ¢ãƒ‡ãƒ«ã¨ `scaler_params` ã‚’å¿…ãšã‚»ãƒƒãƒˆã§ç®¡ç†**:

```
models/
â”œâ”€â”€ fx_lstm_model_20251023_143045.pth         # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â””â”€â”€ fx_lstm_model_20251023_143045_preprocessed.h5  # å¯¾å¿œã™ã‚‹æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
```

**æ¨å¥¨ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†æ–¹æ³•**:
- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§å¯¾å¿œé–¢ä¿‚ã‚’ä¿è¨¼
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã« `preprocessed_h5_hash` ã‚’è¨˜éŒ²
- èª¤ã£ãŸçµ„ã¿åˆã‚ã›æ¤œå‡ºæ©Ÿèƒ½

#### 3. ç‰¹å¾´é‡é †åºã®ä¸€è‡´

**`feature_names` ã§åˆ—é †åºã‚’ä¿è¨¼**:

```python
# HDF5ã‹ã‚‰ç‰¹å¾´é‡åã‚’èª­ã¿è¾¼ã¿
_, feature_names = load_scaler_params('data/preprocessor.h5')

# æ¨è«–æ™‚ã‚‚åŒã˜é †åºã§é¸æŠ
inference_features = raw_data[feature_names]  # é †åºãŒä¸€è‡´
```

#### 4. ONNXå¤‰æ›æ™‚ã®æ‰±ã„

**Option 1: å‰å‡¦ç†ãƒ»å¾Œå‡¦ç†ã‚’å¤–éƒ¨åŒ–**
```python
# ONNXå¤‰æ›å‰ã«æ­£è¦åŒ–
features_norm = scaler.transform(features)
onnx_output = onnx_model.run(None, {'input': features_norm})
predictions = scaler.inverse_transform(onnx_output[0])
```

**Option 2: å‰å‡¦ç†ã‚’ONNXã‚°ãƒ©ãƒ•ã«çµ±åˆ**ï¼ˆå°†æ¥æ‹¡å¼µï¼‰
- RobustScaleræ¼”ç®—ã‚’ONNX opã«å¤‰æ›
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆcenter_, scale_ï¼‰ã‚’ã‚°ãƒ©ãƒ•å®šæ•°ã¨ã—ã¦åŸ‹ã‚è¾¼ã¿

### æ¤œè¨¼æ–¹æ³•

**å­¦ç¿’æ™‚ã¨æ¨è«–æ™‚ã®ä¸€è‡´ã‚’ç¢ºèª**:

```python
def verify_inference_consistency():
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã€
    å­¦ç¿’æ™‚ã®æ­£è¦åŒ–å€¤ã¨ä¸€è‡´ã™ã‚‹ã‹æ¤œè¨¼
    """
    # å­¦ç¿’æ™‚ã®æ­£è¦åŒ–å€¤ã‚’èª­ã¿è¾¼ã¿
    with h5py.File('data/preprocessor.h5', 'r') as f:
        train_normalized = f['sequences']['M5'][:]  # ä¾‹: M5ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
    
    # æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    scaler, feature_names = load_scaler_params('data/preprocessor.h5')
    inference_normalized = scaler.transform(train_features[feature_names])
    
    # ä¸€è‡´ç¢ºèªï¼ˆæµ®å‹•å°æ•°ç‚¹èª¤å·®ã‚’è€ƒæ…®ï¼‰
    np.testing.assert_allclose(
        train_normalized,
        inference_normalized,
        rtol=1e-7,
        err_msg="æ¨è«–æ™‚ã®æ­£è¦åŒ–ãŒå­¦ç¿’æ™‚ã¨ä¸ä¸€è‡´"
    )
```

---

## ğŸ“š ã‚µãƒ–ä»•æ§˜æ›¸

è©³ç´°ãªå®Ÿè£…ä»•æ§˜ã¯ä»¥ä¸‹ã®ã‚µãƒ–ä»•æ§˜æ›¸ã‚’å‚ç…§ï¼š

### å…¥åŠ›å“è³ªç®¡ç†
- **[INPUT_QUALITY_SPEC.md](./preprocessor/INPUT_QUALITY_SPEC.md)** - å…¥åŠ›å“è³ªåŠ£åŒ–è¨­è¨ˆã€æ¬ æåˆ¤å®šã€ã‚®ãƒ£ãƒƒãƒ—é™¤å¤–

  **å†…å®¹**:
  - å…¥åŠ›å“è³ªã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
  - ä¸»è¦åˆ—/è£œåŠ©åˆ—ã®æ¬ æåˆ†é›¢å‡¦ç†
  - é•·æœŸæ¬ æå¾Œã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†æ–­
  - é€£ç¶šã‚®ãƒ£ãƒƒãƒ—é™¤å¤–åŸºæº–

### ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼
- **[DATA_INTEGRITY_SPEC.md](./preprocessor/DATA_INTEGRITY_SPEC.md)** - åˆ—é †åºæ¤œè¨¼ã€TFãƒãƒƒãƒ”ãƒ³ã‚°

  **å†…å®¹**:
  - åˆ—é †åºãƒãƒƒã‚·ãƒ¥æ¤œè¨¼
  - TFãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

### æ­£è¦åŒ–ä»•æ§˜
- **[NORMALIZATION_SPEC.md](./preprocessor/NORMALIZATION_SPEC.md)** - æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†

  **å†…å®¹**:
  - RobustScalerè©³ç´°ä»•æ§˜
  - æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ãƒ»ç®¡ç†

---

## ğŸ”§ æ¤œè¨¼ãƒ„ãƒ¼ãƒ«

### å‰å‡¦ç†çµæœç¢ºèªãƒ„ãƒ¼ãƒ«

**ãƒ„ãƒ¼ãƒ«**: `tools/preprocessor/inspect_preprocessor.py`

å‰å‡¦ç†æ¸ˆã¿HDF5ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`data/preprocessor.h5`ï¼‰ã®å†…å®¹ã‚’è©³ç´°ã«ç¢ºèªã§ãã‚‹å°‚ç”¨ãƒ„ãƒ¼ãƒ«ã€‚

**æ©Ÿèƒ½**:
- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æƒ…å ±è¡¨ç¤ºï¼ˆTFåˆ¥ã®å½¢çŠ¶ãƒ»çµ±è¨ˆï¼‰
- æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°ï¼ˆcenter_, scale_ç­‰ï¼‰
- ç‰¹å¾´é‡åãƒªã‚¹ãƒˆè¡¨ç¤º
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºï¼ˆç”Ÿæˆæ—¥æ™‚ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆï¼‰
- HDF5æ§‹é€ ã®ãƒ„ãƒªãƒ¼è¡¨ç¤º
- **NaN/Infæ¤œè¨¼**ï¼ˆå…¨TFãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼ï¼‰ â† è¿½åŠ 

**ä½¿ç”¨æ–¹æ³•**:

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆdata/preprocessor.h5ï¼‰ã‚’ç¢ºèª
bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py

# ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š
bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py data/preprocessor.h5

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py data/20251023_143045_preprocessor.h5
```

**å‡ºåŠ›ä¾‹**:

```
================================================================================
ğŸ” å‰å‡¦ç†çµæœç¢ºèªãƒ„ãƒ¼ãƒ«
================================================================================

ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: /workspace/data/preprocessor.h5
   ã‚µã‚¤ã‚º: 2.19 GB

================================================================================
ğŸ“Š ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æƒ…å ±
================================================================================

åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ : ['H1', 'H4', 'M1', 'M15', 'M5']

â±ï¸  M1:
   Shape: (31162, 480, 17)
   - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: 31,162
   - ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: 480
   - ç‰¹å¾´é‡æ•°: 17
   - ãƒ‡ãƒ¼ã‚¿å‹: float32
   - ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: 970.01 MB

ğŸ“ˆ ç·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: 157,106

================================================================================
ğŸ¯ æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
================================================================================

æ­£è¦åŒ–æ–¹æ³•: robust
ç‰¹å¾´é‡æ•°: 17

ã€RobustScaler ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘
å››åˆ†ä½ç¯„å›²: [25, 75]

Centerï¼ˆå…ˆé ­5å€‹ï¼‰: [0.0, 2.200, 5.599, ...]
Scaleï¼ˆå…ˆé ­5å€‹ï¼‰: [2.101, 2.000, 4.200, ...]

ã€ç‰¹å¾´é‡åãƒªã‚¹ãƒˆã€‘ï¼ˆå…¨17å€‹ï¼‰
   1. M1_price_change_pips
   2. M1_range_pips
   ...

================================================================================
âœ… NaN/Infæ¤œè¨¼
================================================================================

ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼:
   - M1: NaN 0å€‹ (0.00%), Inf 0å€‹ (0.00%)
   - M5: NaN 0å€‹ (0.00%), Inf 0å€‹ (0.00%)
   - M15: NaN 0å€‹ (0.00%), Inf 0å€‹ (0.00%)
   - H1: NaN 0å€‹ (0.00%), Inf 0å€‹ (0.00%)
   - H4: NaN 0å€‹ (0.00%), Inf 0å€‹ (0.00%)

âœ… å…¨ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã§NaN/Infãªã—ï¼ˆå“è³ªOKï¼‰
```

**ä½¿ç”¨ã‚·ãƒ¼ãƒ³**:
- å‰å‡¦ç†å®Ÿè¡Œå¾Œã®çµæœç¢ºèª
- æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å½¢çŠ¶ã®æ¤œè¨¼
- ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

---

**æ›´æ–°å±¥æ­´**:
- 2025-10-25 (v2.1): æ¨è«–æ™‚ã®å¾©å…ƒã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»é€†å¤‰æ›æ‰‹é †ã®æ˜ç¢ºåŒ–ï¼‰
- 2025-10-22 (v2.0): ã‚µãƒ–ä»•æ§˜æ›¸ã«åˆ†é›¢ï¼ˆINPUT_QUALITY, DATA_INTEGRITY, NORMALIZATIONï¼‰
