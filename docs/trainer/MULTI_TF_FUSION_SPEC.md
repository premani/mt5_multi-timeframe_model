# MULTI_TF_FUSION_SPEC.md

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**æ›´æ–°æ—¥**: 2025-10-21  
**è²¬ä»»è€…**: core-team

---

## ğŸ“‹ ç›®çš„

ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆM1/M5/M15/H1/H4ï¼‰ã®ç‰¹å¾´é‡ã‚’çµ±åˆã—ã€ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ãªãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®èåˆæ©Ÿæ§‹ã®è©³ç´°ä»•æ§˜ã‚’å®šç¾©ã™ã‚‹ã€‚

---

## ğŸ¯ è¨­è¨ˆåŸå‰‡

### 1. ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸­å¿ƒã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

äººé–“ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã®ãƒãƒ£ãƒ¼ãƒˆåˆ¤æ–­ã‚’æ¨¡å€£:
- **ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—**: 2ã¤ã®å±±ã®å½¢çŠ¶ï¼ˆæ™‚é–“å¹…ã¯å¯å¤‰ï¼‰
- **ãƒ¬ãƒ³ã‚¸ãƒ–ãƒ¬ã‚¤ã‚¯**: é«˜å€¤/å®‰å€¤ã®çªç ´ï¼ˆä½•æœ¬å‰ã‹ã¯ä¸å•ï¼‰
- **ãƒˆãƒ¬ãƒ³ãƒ‰è»¢æ›**: ä¸Šæ˜‡â†’ä¸‹é™ã®ãƒªã‚ºãƒ ï¼ˆæœŸé–“ã¯çµæœçš„ã«æ±ºã¾ã‚‹ï¼‰

### 2. æœ€å¤§å…¬ç´„æ•°çš„ãªå›ºå®šçª“

```yaml
è¨­è¨ˆæ€æƒ³:
- å›ºå®šçª“ = ã€Œãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã«ååˆ†ãªæœ€å¤§é•·ã€
- Attention = ã€Œçª“å†…ã§å‹•çš„ã«ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã€
- çµæœ = åŒä¸€çª“å†…ã§50æœ¬ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚200æœ¬ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚èªè­˜å¯èƒ½
```

### 3. ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰æ€§

å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã¯**åŒã˜æ™‚é–“ç¯„å›²ã‚’ç•°ãªã‚‹è§£åƒåº¦ã§è¦³æ¸¬**:
- M1: è©³ç´°ãªæ³¢å½¢ï¼ˆãƒã‚¤ã‚ºå¤šã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ç²¾åº¦é«˜ï¼‰
- H4: å¤§å±€çš„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ»‘ã‚‰ã‹ã€ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šç”¨ï¼‰

---

## ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·å®šç¾©

### å„TFã®å›ºå®šçª“ã‚µã‚¤ã‚º

| TF | æœ¬æ•° | ã‚«ãƒãƒ¼æ™‚é–“ | ä¸»ãªç”¨é€” | æƒ³å®šãƒ‘ã‚¿ãƒ¼ãƒ³ä¾‹ |
|----|------|-----------|---------|---------------|
| **M1** | 480 | 8æ™‚é–“ | çŸ­æœŸã‚¨ãƒ³ãƒˆãƒªãƒ¼ã€ãƒã‚¤ã‚¯ãƒ­ãƒ‘ã‚¿ãƒ¼ãƒ³ | ã‚¹ã‚­ãƒ£ãƒ«ãƒ—ç”¨ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ï¼ˆ30-60æœ¬ï¼‰ |
| **M5** | 288 | 24æ™‚é–“ | çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€ã‚¹ã‚­ãƒ£ãƒ«ãƒ—ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | ãƒ‡ã‚¤ãƒˆãƒ¬ç”¨ãƒ¬ãƒ³ã‚¸ï¼ˆ50-150æœ¬ï¼‰ |
| **M15** | 192 | 48æ™‚é–“ | ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã€ã‚¹ã‚¤ãƒ³ã‚°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— | ã‚¹ã‚¤ãƒ³ã‚°ç”¨ä¸‰è§’æŒã¡åˆã„ï¼ˆ80-120æœ¬ï¼‰ |
| **H1** | 96 | 96æ™‚é–“ï¼ˆ4æ—¥ï¼‰ | ä¸»è¦ãƒˆãƒ¬ãƒ³ãƒ‰ã€æ—¥è¶³æ§‹é€  | é€±è¶³ãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¬ãƒ³ãƒ‰è»¢æ›ï¼ˆ40-80æœ¬ï¼‰ |
| **H4** | 48 | 192æ™‚é–“ï¼ˆ8æ—¥ï¼‰ | ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºã€ãƒã‚¯ãƒ­ãƒã‚¤ã‚¢ã‚¹ | æœˆè¶³ãƒ¬ãƒ™ãƒ«ã®å¤§å±€è¦³ï¼ˆ20-40æœ¬ï¼‰ |

### è¨­è¨ˆæ ¹æ‹ 

1. **M1 (480æœ¬ = 8æ™‚é–“)**
   - ã‚¹ã‚­ãƒ£ãƒ«ãƒ—ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯é€šå¸¸30-120åˆ†ã§å®Œçµ
   - 8æ™‚é–“çª“ã§è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½
   
2. **M5 (288æœ¬ = 24æ™‚é–“)**
   - ãƒ‡ã‚¤ãƒˆãƒ¬ç”¨ã®å°æ³¢å‹•ã¯4-12æ™‚é–“
   - 1æ—¥çª“ã§æœãƒ»æ˜¼ãƒ»å¤•ã®è¤‡æ•°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ã‚«ãƒãƒ¼
   
3. **M15 (192æœ¬ = 48æ™‚é–“)**
   - ã‚¹ã‚¤ãƒ³ã‚°ç”¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯12-36æ™‚é–“
   - 2æ—¥çª“ã§é€±åˆãƒ»é€±å¤®ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–ã‚’æ‰ãˆã‚‹
   
4. **H1 (96æœ¬ = 4æ—¥)**
   - é€±è¶³ãƒ¬ãƒ™ãƒ«ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯2-5æ—¥ã§å½¢æˆ
   - 4æ—¥çª“ã§é€±å…¨ä½“ã®æµã‚Œã‚’æŠŠæ¡
   
5. **H4 (48æœ¬ = 8æ—¥)**
   - æœˆè¶³ãƒ¬ãƒ™ãƒ«ã®å¤§å±€ã¯1-3é€±é–“
   - 8æ—¥çª“ã§ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ‡æ›¿ï¼‰ã‚’æ¤œå‡º

### å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã®ä¾‹

```python
# M5è¶³288æœ¬ã®çª“å†…ã§ç•°ãªã‚‹é•·ã•ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º

# ä¾‹1: çŸ­ã„ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ï¼ˆ50æœ¬ â‰ˆ 4æ™‚é–“ï¼‰
# [0]---[238]---[288(ç¾åœ¨)]
#       â†‘å±±1 â†‘å±±2
# Attention weights: ç›´è¿‘50æœ¬ã«é›†ä¸­ (weight > 0.6)

# ä¾‹2: é•·ã„ãƒ¬ãƒ³ã‚¸ï¼ˆ200æœ¬ â‰ˆ 17æ™‚é–“ï¼‰
# [0]---[88]---[288(ç¾åœ¨)]
#       â†‘ãƒ¬ãƒ³ã‚¸é–‹å§‹
# Attention weights: åºƒç¯„å›²ã«åˆ†æ•£ (weight = 0.3-0.5)

# ä¾‹3: è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³åŒæ™‚æ¤œå‡º
# [0]---[100]---[200]---[288(ç¾åœ¨)]
#       â†‘ä¸‰è§’æŒåˆ  â†‘ãƒ–ãƒ¬ã‚¤ã‚¯
# Attention weights: 2ã¤ã®ãƒ”ãƒ¼ã‚¯ (multi-headå¯¾å¿œ)
```

---

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

### å…¨ä½“æ§‹é€ 

```
å…¥åŠ›: ãƒãƒ«ãƒTFç‰¹å¾´é‡
â”œâ”€ X_M1:  [batch, 480, features]
â”œâ”€ X_M5:  [batch, 288, features]
â”œâ”€ X_M15: [batch, 192, features]
â”œâ”€ X_H1:  [batch, 96, features]
â””â”€ X_H4:  [batch, 48, features]

â†“ å„TFç‹¬ç«‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰

TFã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å±¤ï¼ˆLSTMï¼‰
â”œâ”€ LSTM_M1:  [batch, 480, features] â†’ [batch, 480, d_model]
â”œâ”€ LSTM_M5:  [batch, 288, features] â†’ [batch, 288, d_model]
â”œâ”€ LSTM_M15: [batch, 192, features] â†’ [batch, 192, d_model]
â”œâ”€ LSTM_H1:  [batch, 96, features] â†’ [batch, 96, d_model]
â””â”€ LSTM_H4:  [batch, 48, features] â†’ [batch, 48, d_model]

â†“ Self-Attentionï¼ˆå„TFå†…ã§ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºï¼‰

Self-Attentionå±¤
â”œâ”€ SelfAttn_M1:  [batch, 480, d_model] â†’ [batch, d_model]
â”œâ”€ SelfAttn_M5:  [batch, 288, d_model] â†’ [batch, d_model]
â”œâ”€ SelfAttn_M15: [batch, 192, d_model] â†’ [batch, d_model]
â”œâ”€ SelfAttn_H1:  [batch, 96, d_model] â†’ [batch, d_model]
â””â”€ SelfAttn_H4:  [batch, 48, d_model] â†’ [batch, d_model]

â†“ TFã‚µãƒãƒªãƒ¼ãƒ™ã‚¯ãƒˆãƒ«çµ±åˆ

Cross-TF Fusion
â””â”€ H_all: [batch, 5, d_model]  # 5 TFs

â†“ TFé–“ã®é–¢ä¿‚å­¦ç¿’

Cross-TF Attention
â””â”€ fused: [batch, d_model]

â†“ ãƒ¢ãƒ¼ãƒ‰åˆ¥é‡ã¿ä»˜ã‘ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

Mode-Specific Weighting
â”œâ”€ Scalp Mode:  [0.35, 0.30, 0.20, 0.10, 0.05]
â””â”€ Swing Mode:  [0.20, 0.20, 0.25, 0.20, 0.15]

â†“ æœ€çµ‚å‡ºåŠ›

Multi-Head Output
â”œâ”€ Direction Head:      [batch, 3]  # UP/DOWN/NEUTRAL
â”œâ”€ Magnitude_Scalp Head: [batch, 1]  # 0.5-2.0 pips
â”œâ”€ Magnitude_Swing Head: [batch, 1]  # 2.0-5.0 pips
â””â”€ Trend_Strength Head:  [batch, 1]  # 0-1
```

---

## ğŸ”§ å®Ÿè£…è©³ç´°

### 1. TFã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆLSTMï¼‰

```python
class TimeframeEncoder(nn.Module):
    """å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ç”¨ã®LSTMã‚¨ãƒ³ã‚³ãƒ¼ãƒ€"""
    
    def __init__(self, input_dim: int, d_model: int = 128, num_layers: int = 2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=d_model,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1,
            bidirectional=False  # æœªæ¥ãƒªãƒ¼ã‚¯é˜²æ­¢
        )
        self.layer_norm = nn.LayerNorm(d_model)
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Args:
            x: [batch, seq_len, input_dim]
        Returns:
            h: [batch, seq_len, d_model]  # å…¨æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹
        """
        h, (h_n, c_n) = self.lstm(x)
        h = self.layer_norm(h)
        return h  # å…¨æ™‚åˆ»ã‚’è¿”ã™ï¼ˆSelf-Attentionç”¨ï¼‰


# å„TFç”¨ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
encoders = {
    "M1":  TimeframeEncoder(input_dim=features_M1, d_model=128),
    "M5":  TimeframeEncoder(input_dim=features_M5, d_model=128),
    "M15": TimeframeEncoder(input_dim=features_M15, d_model=128),
    "H1":  TimeframeEncoder(input_dim=features_H1, d_model=128),
    "H4":  TimeframeEncoder(input_dim=features_H4, d_model=128),
}
```

**è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ**:
- **å…¨æ™‚åˆ»ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä¿æŒ**: Self-Attentionã§å‹•çš„ã«ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã™ã‚‹ãŸã‚
- **Bidirectional=False**: æœªæ¥ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢ï¼ˆæ¨è«–æ™‚ã«æœªæ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ãˆãªã„ï¼‰
- **LayerNorm**: å„TFã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æƒãˆã‚‹

---

### 2. Self-Attentionï¼ˆTFå†…ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºï¼‰

```python
class SelfAttentionPooling(nn.Module):
    """å„TFå†…ã§é‡è¦ãªæ™‚åˆ»ã‚’å‹•çš„ã«æŠ½å‡º"""
    
    def __init__(self, d_model: int = 128, num_heads: int = 4):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        self.query_token = nn.Parameter(torch.randn(1, 1, d_model))  # å­¦ç¿’å¯èƒ½
        self.layer_norm = nn.LayerNorm(d_model)
    
    def forward(self, h: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Args:
            h: [batch, seq_len, d_model]  # LSTMã®å…¨æ™‚åˆ»å‡ºåŠ›
        Returns:
            summary: [batch, d_model]     # TFã‚µãƒãƒªãƒ¼ãƒ™ã‚¯ãƒˆãƒ«
            weights: [batch, seq_len]     # Attention weightsï¼ˆå¯è¦–åŒ–ç”¨ï¼‰
        """
        batch_size = h.size(0)
        query = self.query_token.expand(batch_size, -1, -1)  # [batch, 1, d_model]
        
        # Attention: Query=å­¦ç¿’ãƒˆãƒ¼ã‚¯ãƒ³, Key/Value=å…¨æ™‚åˆ»
        attn_output, attn_weights = self.attention(
            query=query,
            key=h,
            value=h,
            need_weights=True
        )
        
        summary = self.layer_norm(attn_output.squeeze(1))  # [batch, d_model]
        weights = attn_weights.squeeze(1)  # [batch, seq_len]
        
        return summary, weights


# ä½¿ç”¨ä¾‹
self_attn_M5 = SelfAttentionPooling(d_model=128, num_heads=4)
h_M5 = encoders["M5"](X_M5)  # [batch, 288, 128]
summary_M5, weights_M5 = self_attn_M5(h_M5)  # summary: [batch, 128]

# weights_M5 ã®å¯è¦–åŒ–ã§ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ã®2ã¤ã®å±±ãŒé«˜weightã«ãªã‚‹
```

**è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ**:
- **å­¦ç¿’å¯èƒ½ãªã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³**: ã€Œã©ã®æ™‚åˆ»ãŒé‡è¦ã‹ã€ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’
- **Multi-Head (4 heads)**: è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³åŒæ™‚æ¤œå‡ºï¼ˆä¾‹: å±±1ã€å±±2ã€ç¾åœ¨ä¾¡æ ¼ï¼‰
- **Attention weightsä¿å­˜**: ãƒ‡ãƒãƒƒã‚°ãƒ»è§£é‡ˆæ€§å‘ä¸Šã®ãŸã‚

---

### 3. Cross-TF Attentionï¼ˆTFé–“èåˆï¼‰

```python
class CrossTimeframeFusion(nn.Module):
    """è¤‡æ•°TFã®ã‚µãƒãƒªãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’çµ±åˆ"""
    
    def __init__(self, d_model: int = 128, num_heads: int = 4):
        super().__init__()
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model * 4, d_model),
        )
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)
    
    def forward(self, tf_summaries: Tensor) -> Tensor:
        """
        Args:
            tf_summaries: [batch, 5, d_model]  # [M1, M5, M15, H1, H4]
        Returns:
            fused: [batch, d_model]  # çµ±åˆãƒ™ã‚¯ãƒˆãƒ«
        """
        # Self-Attention: TFé–“ã®é–¢ä¿‚ã‚’å­¦ç¿’
        attn_output, _ = self.cross_attention(
            query=tf_summaries,
            key=tf_summaries,
            value=tf_summaries
        )
        attn_output = self.layer_norm1(attn_output + tf_summaries)  # Residual
        
        # Feed Forward
        ff_output = self.feed_forward(attn_output)
        output = self.layer_norm2(ff_output + attn_output)  # Residual
        
        # Global pooling: 5 TFs â†’ 1 vector
        fused = output.mean(dim=1)  # [batch, d_model]
        
        return fused


# ä½¿ç”¨ä¾‹
tf_summaries = torch.stack([
    summary_M1,   # [batch, 128]
    summary_M5,   # [batch, 128]
    summary_M15,  # [batch, 128]
    summary_H1,   # [batch, 128]
    summary_H4,   # [batch, 128]
], dim=1)  # [batch, 5, 128]

fusion = CrossTimeframeFusion(d_model=128, num_heads=4)
fused = fusion(tf_summaries)  # [batch, 128]
```

**è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ**:
- **Self-Attention on TFs**: M1ã¨H1ã®ç›¸é–¢ãªã©ã€TFé–“ã®é–¢ä¿‚ã‚’å­¦ç¿’
- **Residual + LayerNorm**: å­¦ç¿’å®‰å®šåŒ–
- **Mean pooling**: å…¨TFã®æƒ…å ±ã‚’å‡ç­‰ã«çµ±åˆï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥é‡ã¿ä»˜ã‘ã¯å¾Œæ®µï¼‰

**é …ç›®64å¯¾å¿œ: Cross-TF Attentionå¾Œã®é™çš„ãƒ¢ãƒ¼ãƒ‰é‡ã¿é©ç”¨æ™‚ã®äºŒé‡é‡ã¿åŒ–é˜²æ­¢**:

Cross-TF Attentionã§ã¯ã€å„TFã«å¯¾ã—ã¦å‹•çš„ãªé‡ã¿ï¼ˆattention weightsï¼‰ãŒå­¦ç¿’ã•ã‚Œã‚‹ã€‚ãã®å¾Œã€ModeSpecificWeightingã§é™çš„ãªé‡ã¿ã‚’é©ç”¨ã™ã‚‹ã¨ã€åŒã˜TFã«å¯¾ã—ã¦äºŒé‡ã«é‡ã¿ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

**äºŒé‡é‡ã¿åŒ–ãƒªã‚¹ã‚¯**:
```python
# NGä¾‹: äºŒé‡é‡ã¿åŒ–
attn_fused = cross_tf_attention(tf_summaries)  # å‹•çš„é‡ã¿A
weighted = mode_weighting(tf_summaries, attn_fused)  # é™çš„é‡ã¿B
# â†’ M1ã«å¯¾ã—ã¦: A_M1 * B_M1 ï¼ˆç©ãŒå¤§ãã™ãã‚‹ï¼‰
```

**è§£æ±ºç­–: å¾Œæ®µé‡ã¿ã®æ­£è¦åŒ–**:
```python
class ModeSpecificWeighting(nn.Module):
    def forward(self, tf_summaries: Tensor, fused: Tensor) -> Tensor:
        # ... (ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šãƒ»é‡ã¿è¨ˆç®—) ...
        
        # ã€é‡è¦ã€‘Cross-TF Attentionã®å½±éŸ¿ã‚’è€ƒæ…®ã—ãŸæ­£è¦åŒ–
        # Option 1: Gatingï¼ˆæ¨å¥¨ï¼‰
        gate = torch.sigmoid(self.gate_proj(fused))  # [batch, 1]
        weighted = gate * fused + (1 - gate) * weighted_static
        
        # Option 2: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é‡ã¿ç·©å’Œ
        temperature = 2.0  # é™çš„é‡ã¿ã‚’å¹³æ»‘åŒ–
        weights_soft = torch.softmax(weights / temperature, dim=-1)
        
        # Option 3: å®Œå…¨ã«Attentionå„ªå…ˆï¼ˆé™çš„é‡ã¿ã¯è£œåŠ©ï¼‰
        # é™çš„é‡ã¿ã¯å­¦ç¿’åˆæœŸã®ã¿ä½¿ç”¨ã€å¾Œã¯ç„¡åŠ¹åŒ–
        
        return weighted
```

**å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆé …ç›®64ï¼‰**:
1. **Phase 1**: Cross-TF Attention ã®ã¿ï¼ˆé™çš„é‡ã¿ãªã—ï¼‰
2. **Phase 2è©•ä¾¡æ™‚**: Gatingæ–¹å¼ã§é™çš„é‡ã¿ã‚’è£œåŠ©çš„ã«ä½¿ç”¨
3. **æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 2.0ä»¥ä¸Šã§é‡ã¿é›†ä¸­ã‚’ç·©å’Œ
4. **æˆåŠŸæŒ‡æ¨™**: M1ã¸ã®é‡ã¿é›†ä¸­åº¦ < 0.5ï¼ˆå…¨ä½“ã®50%æœªæº€ï¼‰

---

### 4. ãƒ¢ãƒ¼ãƒ‰åˆ¥é‡ã¿ä»˜ã‘ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

```python
class ModeSpecificWeighting(nn.Module):
    """ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®TFé‡ã¿é©ç”¨ï¼ˆé …ç›®59å¯¾å¿œ: Age-weightæ­£è¦åŒ–ï¼‰"""
    
    def __init__(self, d_model: int = 128):
        super().__init__()
        # é™çš„é‡ã¿ï¼ˆå­¦ç¿’ã—ãªã„ï¼‰- æ­£è¦åŒ–æ¸ˆã¿ï¼ˆsum=1.0ï¼‰
        self.scalp_weights = torch.tensor([0.35, 0.30, 0.20, 0.10, 0.05])
        self.swing_weights = torch.tensor([0.20, 0.20, 0.25, 0.20, 0.15])
        
        # æ­£è¦åŒ–æ¤œè¨¼
        assert abs(self.scalp_weights.sum() - 1.0) < 1e-6, "Scalp weights must sum to 1.0"
        assert abs(self.swing_weights.sum() - 1.0) < 1e-6, "Swing weights must sum to 1.0"
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šç”¨ï¼ˆtrend_strengthã‹ã‚‰ï¼‰
        self.mode_gate = nn.Linear(d_model, 1)
    
    def forward(self, tf_summaries: Tensor, fused: Tensor) -> Tensor:
        """
        Args:
            tf_summaries: [batch, 5, d_model]  # [M1, M5, M15, H1, H4]
            fused: [batch, d_model]  # Cross-TF Attentionå‡ºåŠ›
        Returns:
            weighted: [batch, d_model]  # ãƒ¢ãƒ¼ãƒ‰åˆ¥é‡ã¿é©ç”¨å¾Œ
        """
        # ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆtrend_strengthã®ä»£ç†ï¼‰
        mode_score = torch.sigmoid(self.mode_gate(fused))  # [batch, 1]
        # mode_score < 0.3 â†’ Scalp, > 0.7 â†’ Swing, 0.3-0.7 â†’ Mix
        
        # é‡ã¿è£œé–“
        weights = (
            (1 - mode_score) * self.scalp_weights.to(fused.device) +
            mode_score * self.swing_weights.to(fused.device)
        )  # [batch, 5]
        
        # æ­£è¦åŒ–ä¿è¨¼ï¼ˆè£œé–“å¾Œã‚‚ sum=1.0ï¼‰
        weights = weights / weights.sum(dim=-1, keepdim=True)
        
        # Age-weight KPIè¨ˆç®—ï¼ˆæœ€æ–°TF = M1ã®æœ€å°é‡ã¿ä¿è¨¼ï¼‰
        min_fresh_ratio = weights[:, 0].min().item()  # M1é‡ã¿æœ€å°å€¤
        if min_fresh_ratio < 0.15:  # é–¾å€¤: Scalpæœ€ä½15%, Swingæœ€ä½10%
            logging.warning(f"M1 weight too low: {min_fresh_ratio:.3f}")
        
        # åŠ é‡å¹³å‡
        weights_expanded = weights.unsqueeze(-1)  # [batch, 5, 1]
        weighted = (tf_summaries * weights_expanded).sum(dim=1)  # [batch, d_model]
        
        return weighted


# ä½¿ç”¨ä¾‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ï¼‰
mode_weighting = ModeSpecificWeighting(d_model=128)
weighted_fused = mode_weighting(tf_summaries, fused)
```

**è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ**:
- **é™çš„é‡ã¿æ­£è¦åŒ–**: åˆæœŸåŒ–æ™‚ã« sum=1.0 ã‚’ assert ã§æ¤œè¨¼
- **è£œé–“å¾Œæ­£è¦åŒ–**: å‹•çš„é‡ã¿ã‚‚å¿…ãšæ­£è¦åŒ–ï¼ˆä»–ã®æ³¨æ„æ©Ÿæ§‹ã¨å¹²æ¸‰é˜²æ­¢ï¼‰
- **min_fresh_ratio KPI**: M1é‡ã¿ã®æœ€å°å€¤ã‚’ç›£è¦–
  - Scalp: min_fresh_ratio >= 0.15ï¼ˆçŸ­æœŸé‡è¦–ï¼‰
  - Swing: min_fresh_ratio >= 0.10ï¼ˆä¸­é•·æœŸé‡è¦–ï¼‰
- **è­¦å‘Šãƒ­ã‚°**: é–¾å€¤æœªæº€æ™‚ã«è‡ªå‹•æ¤œå‡º

**Age-weightæ­£è¦åŒ–ã®åŠ¹æœ**:
- é‡ã¿éå¤§ã«ã‚ˆã‚‹å‹¾é…åã‚Šã‚’é˜²æ­¢
- ä»–ã®Attentionå±¤ã¨ã®é‡ã¿ç«¶åˆã‚’å›é¿
- ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿æ™‚ã®å®‰å®šæ€§å‘ä¸Š

---

## ğŸ“Š Attentionå¯è¦–åŒ–ä¾‹

### ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã®ç¢ºèª

```python
# M5è¶³ã§ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—æ¤œå‡ºæ™‚ã®Attention weights
# Xè»¸: æœ¬æ•°ï¼ˆ0-288ï¼‰ã€Yè»¸: Attention weight

weights_M5: [0.001, 0.001, ..., 0.15(å±±1), ..., 0.20(å±±2), ..., 0.35(ç¾åœ¨)]
                              â†‘188æœ¬ç›®      â†‘238æœ¬ç›®        â†‘288æœ¬ç›®

# è§£é‡ˆ:
# - å±±1 (188æœ¬ç›® = ç´„16æ™‚é–“å‰): weight=0.15 â†’ 1ã¤ç›®ã®ãƒ”ãƒ¼ã‚¯æ¤œå‡º
# - å±±2 (238æœ¬ç›® = ç´„4æ™‚é–“å‰):  weight=0.20 â†’ 2ã¤ç›®ã®ãƒ”ãƒ¼ã‚¯æ¤œå‡º
# - ç¾åœ¨ (288æœ¬ç›®):             weight=0.35 â†’ ã‚¨ãƒ³ãƒˆãƒªãƒ¼åˆ¤æ–­ã®åŸºæº–ç‚¹

# â†’ ãƒ¢ãƒ‡ãƒ«ã¯ã€Œ50æœ¬é–“éš”ã®ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ã€ã‚’è‡ªå‹•æ¤œå‡º
```

### TFé–“ã®å”èª¿

```python
# Cross-TF Attentionã§ã®å„TFã®å¯„ä¸åº¦

TF Attention weights (Scalp Mode):
M1:  0.32  # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¿ã‚¤ãƒŸãƒ³ã‚°é‡è¦–
M5:  0.28  # çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰
M15: 0.22  # ä¸­æœŸãƒã‚¤ã‚¢ã‚¹
H1:  0.12  # å¤§å±€ç¢ºèª
H4:  0.06  # ãƒ¬ã‚¸ãƒ¼ãƒ å‚è€ƒ

TF Attention weights (Swing Mode):
M1:  0.18  # ã‚¿ã‚¤ãƒŸãƒ³ã‚°é‡è¦åº¦ä½ä¸‹
M5:  0.20
M15: 0.26  # ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰é‡è¦–
H1:  0.22  # å¤§å±€é‡è¦åº¦ä¸Šæ˜‡
H4:  0.14  # ãƒ¬ã‚¸ãƒ¼ãƒ å½±éŸ¿å¢—åŠ 

# â†’ ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦AttentionãŒè‡ªå‹•èª¿æ•´ã•ã‚Œã‚‹å¯èƒ½æ€§
#    ï¼ˆé™çš„é‡ã¿ã‚ˆã‚Šã‚‚æŸ”è»Ÿï¼‰
```

---

## âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### æ¨å¥¨è¨­å®š

```yaml
model:
  tf_encoder:
    d_model: 128           # LSTMéš ã‚Œå±¤æ¬¡å…ƒ
    num_layers: 2          # LSTMå±¤æ•°
    dropout: 0.1
    
  self_attention:
    num_heads: 4           # Multi-headæ•°
    dropout: 0.1
    
  cross_tf_fusion:
    num_heads: 4
    dropout: 0.1
    ff_dim: 512            # Feed Forwardä¸­é–“å±¤ (d_model * 4)
    
  mode_weighting:
    enabled: false         # Phase 1ã§ã¯ç„¡åŠ¹
    static_weights:
      scalp: [0.35, 0.30, 0.20, 0.10, 0.05]
      swing: [0.20, 0.20, 0.25, 0.20, 0.15]

sequence_lengths:
  M1:  480   # 8æ™‚é–“
  M5:  288   # 24æ™‚é–“
  M15: 192   # 48æ™‚é–“
  H1:  96    # 96æ™‚é–“
  H4:  48    # 192æ™‚é–“
```

### ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æŒ‡é‡

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å°ã•ã„å€¤ã®åŠ¹æœ | å¤§ãã„å€¤ã®åŠ¹æœ | æ¨å¥¨ç¯„å›² |
|----------|--------------|--------------|---------|
| `d_model` | å­¦ç¿’é€Ÿåº¦â†‘ã€è¡¨ç¾åŠ›â†“ | è¡¨ç¾åŠ›â†‘ã€éå­¦ç¿’ãƒªã‚¹ã‚¯â†‘ | 64-256 |
| `num_heads` | å˜ç´”ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¦– | è¤‡é›‘ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º | 2-8 |
| `num_layers` (LSTM) | æµ…ã„ç‰¹å¾´æŠ½å‡º | æ·±ã„æŠ½è±¡åŒ– | 1-3 |
| `dropout` | éå­¦ç¿’ã—ã‚„ã™ã„ | æ±åŒ–æ€§èƒ½â†‘ã€å­¦ç¿’é…å»¶ | 0.1-0.3 |

---

## ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ»æ¤œè¨¼

### 1. Attention weightsä¿å­˜

```python
# å­¦ç¿’æ™‚ã«Attention weightsã‚’ãƒ­ã‚°ä¿å­˜
def log_attention_weights(batch_idx: int, weights: Dict[str, Tensor]):
    """
    Args:
        weights: {
            "M1": [batch, 480],
            "M5": [batch, 288],
            ...
        }
    """
    if batch_idx % 100 == 0:  # 100ãƒãƒƒãƒã”ã¨
        for tf_name, w in weights.items():
            # ã‚µãƒ³ãƒ—ãƒ«0ã®weightsã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            plt.figure(figsize=(12, 3))
            plt.plot(w[0].cpu().numpy())
            plt.title(f"{tf_name} Attention Weights (batch {batch_idx})")
            plt.xlabel("Time Step")
            plt.ylabel("Weight")
            plt.savefig(f"logs/attention_{tf_name}_batch{batch_idx}.png")
            plt.close()
```

### 2. ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºç²¾åº¦

```python
# ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ãŒå­˜åœ¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã§AttentionãŒæ­£ã—ãå±±ã‚’æ¤œå‡ºã§ãã¦ã„ã‚‹ã‹
def evaluate_pattern_detection(
    model: nn.Module,
    test_data: Dataset,
    pattern_labels: Dict[int, List[int]]  # {sample_id: [peak1_idx, peak2_idx]}
):
    """
    Args:
        pattern_labels: äººé–“ãŒæ‰‹å‹•ã§ãƒ©ãƒ™ãƒ«ä»˜ã‘ã—ãŸå±±ã®ä½ç½®
    """
    for sample_id, peak_indices in pattern_labels.items():
        x = test_data[sample_id]
        _, attention_weights = model.forward_with_attention(x)
        
        # Attention weightsã®ä¸Šä½Kå€‹ã®ä½ç½®
        top_k_indices = attention_weights.topk(k=5).indices
        
        # å±±ã®ä½ç½®ã¨ã©ã‚Œã ã‘ä¸€è‡´ã™ã‚‹ã‹
        hit_count = sum(1 for idx in peak_indices if idx in top_k_indices)
        precision = hit_count / len(peak_indices)
        
        print(f"Sample {sample_id}: Precision={precision:.2f}")
```

### 3. TFå¯„ä¸åº¦åˆ†æ

```python
# å„TFãŒã©ã‚Œã ã‘äºˆæ¸¬ã«å¯„ä¸ã—ã¦ã„ã‚‹ã‹ï¼ˆAblation Studyï¼‰
def tf_ablation_study(model: nn.Module, test_loader: DataLoader):
    """å„TFã‚’ç„¡åŠ¹åŒ–ã—ãŸæ™‚ã®æ€§èƒ½å¤‰åŒ–"""
    baseline_acc = evaluate(model, test_loader)
    
    for tf_name in ["M1", "M5", "M15", "H1", "H4"]:
        # ç‰¹å®šTFã‚’ã‚¼ãƒ­ãƒã‚¹ã‚¯
        model.mask_tf(tf_name)
        masked_acc = evaluate(model, test_loader)
        impact = baseline_acc - masked_acc
        
        print(f"TF={tf_name} removed: Accuracy drop = {impact:.3f}")
        model.unmask_tf(tf_name)
    
    # å‡ºåŠ›ä¾‹:
    # TF=M1 removed: Accuracy drop = 0.082  # M1ãŒæœ€é‡è¦
    # TF=H4 removed: Accuracy drop = 0.015  # H4ã¯è£œåŠ©çš„
```

---

## ğŸš€ å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º1: åŸºæœ¬å®Ÿè£…

1. **TFã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ + Self-Attention**
   - å„TFã‚’ç‹¬ç«‹ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
   - Self-Attentionã§ã‚µãƒãƒªãƒ¼æŠ½å‡º
   
2. **Cross-TF Fusionï¼ˆSimpleç‰ˆï¼‰**
   - Mean poolingã®ã¿ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥é‡ã¿ä»˜ã‘ãªã—ï¼‰
   
3. **æ¤œè¨¼**
   - Attention weightså¯è¦–åŒ–
   - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºç²¾åº¦è©•ä¾¡

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º2: é«˜åº¦åŒ–

1. **ãƒ¢ãƒ¼ãƒ‰åˆ¥é‡ã¿ä»˜ã‘**
   - trend_strengthãƒ™ãƒ¼ã‚¹ã®å‹•çš„é‡ã¿
   
2. **Hierarchical Attention**
   - TFå†… â†’ TFé–“ã®2æ®µéšAttention
   
3. **Positional Encoding**
   - æ™‚åˆ»æƒ…å ±ã®æ˜ç¤ºçš„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º3: æœ€é©åŒ–

1. **ONNXå¤‰æ›å¯¾å¿œ**
   - Attentionæ¼”ç®—ã®æœ€é©åŒ–

2. **æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›**
   - Incremental updateï¼ˆæ–°è¦1æœ¬ã®ã¿å‡¦ç†ï¼‰

3. **Multi-GPUå­¦ç¿’**
   - ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—åŒ–

---

## Cross-TF Attentionå¾Œã®é‡ã¿æ­£è¦åŒ–

**ç›®çš„**: åŒä¸€TFã¸æ³¨æ„é‡ã¿+é™çš„ãƒ¢ãƒ¼ãƒ‰é‡ã¿ã®é›†ä¸­ã«ã‚ˆã‚Šå‹¾é…åé‡ãƒ»éå­¦ç¿’ï¼ˆç‰¹å®šTFéå‰°å¼·èª¿ï¼‰ãƒªã‚¹ã‚¯ã€‚

**å•é¡Œè©³ç´°**:
```python
# äºŒé‡é‡ã¿åŒ–ã®ä¾‹
# Step 1: Cross-TF Attention
attention_weights = softmax(Q @ K.T)  # [0.4, 0.3, 0.2, 0.05, 0.05] (M1å„ªä½)

# Step 2: ãƒ¢ãƒ¼ãƒ‰åˆ¥é™çš„é‡ã¿é©ç”¨
mode_weights = [0.35, 0.30, 0.20, 0.10, 0.05]  # M1å„ªå…ˆ (Scalp mode)

# çµæœ: M1ãŒäºŒé‡ã«å¼·èª¿ã•ã‚Œã‚‹
# M1ã¸ã®å®ŸåŠ¹é‡ã¿ â‰ˆ 0.4 Ã— 0.35 = 0.14
# M5ã¸ã®å®ŸåŠ¹é‡ã¿ â‰ˆ 0.3 Ã— 0.30 = 0.09
# â†’ M1ã®å‹¾é…ãŒéå‰°ã«å¤§ãããªã‚‹
```

**å¯¾å¿œç­–**: å¾Œæ®µé‡ã¿æ­£è¦åŒ–ã¾ãŸã¯ gating

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: å¾Œæ®µæ­£è¦åŒ–ï¼ˆæ¨å¥¨ï¼‰

```python
class CrossTFFusionWithNormalization(nn.Module):
    """
    Cross-TF Attention + å¾Œæ®µé‡ã¿æ­£è¦åŒ–
    """

    def __init__(self, d_model=128, mode='scalp'):
        super().__init__()
        self.attention = CrossTFAttention(d_model)
        self.mode = mode

        # ãƒ¢ãƒ¼ãƒ‰åˆ¥é™çš„é‡ã¿ï¼ˆåˆæœŸå€¤ï¼‰
        self.register_buffer('static_weights', torch.tensor([
            0.35, 0.30, 0.20, 0.10, 0.05  # Scalp mode
        ]))

    def forward(self, tf_embeddings: List[torch.Tensor]) -> torch.Tensor:
        """
        Args:
            tf_embeddings: [M1, M5, M15, H1, H4] embeddings

        Returns:
            èåˆembedding
        """
        # Step 1: Cross-TF Attentionï¼ˆå‹•çš„é‡ã¿ï¼‰
        attended, attn_weights = self.attention(tf_embeddings)
        # attn_weights.shape = [batch, 5]

        # Step 2: é™çš„é‡ã¿é©ç”¨
        static_w = self.static_weights.unsqueeze(0)  # [1, 5]

        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³1a: é‡ã¿ç©ã®æ­£è¦åŒ–
        combined_weights = attn_weights * static_w
        combined_weights = combined_weights / combined_weights.sum(dim=1, keepdim=True)
        # æ­£è¦åŒ–ã«ã‚ˆã‚Š Î£w = 1.0 ã‚’ä¿è¨¼

        # Step 3: èåˆ
        fused = torch.stack(attended, dim=1)  # [batch, 5, d_model]
        output = (fused * combined_weights.unsqueeze(-1)).sum(dim=1)

        return output, combined_weights


# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®æ¤œè¨¼
def validate_weight_distribution(combined_weights):
    """é‡ã¿åˆ†å¸ƒã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
    # 1. åˆè¨ˆãŒ1.0
    assert torch.allclose(combined_weights.sum(dim=1), torch.ones(len(combined_weights)))

    # 2. å˜ä¸€TFã¸ã®éå‰°é›†ä¸­é˜²æ­¢ï¼ˆæœ€å¤§é‡ã¿ < 0.6ï¼‰
    max_weights, _ = combined_weights.max(dim=1)
    if (max_weights > 0.6).any():
        logger.warning(f"TF weight concentration detected: max={max_weights.max():.3f}")

    # 3. å®ŸåŠ¹é‡ã¿ã®å¤šæ§˜æ€§ï¼ˆEntropyï¼‰
    entropy = -(combined_weights * torch.log(combined_weights + 1e-9)).sum(dim=1)
    min_entropy = torch.log(torch.tensor(5.0)) * 0.5  # æœ€å°ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é–¾å€¤
    if (entropy < min_entropy).any():
        logger.warning(f"Low weight diversity: entropy={entropy.min():.3f}")
```

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: Gatingæ©Ÿæ§‹

```python
class GatedCrossTFFusion(nn.Module):
    """
    Gating ã«ã‚ˆã‚‹å‹•çš„ãƒ»é™çš„é‡ã¿ã®çµ±åˆ
    """

    def __init__(self, d_model=128):
        super().__init__()
        self.attention = CrossTFAttention(d_model)

        # Gate network: å‹•çš„é‡ã¿ã¨é™çš„é‡ã¿ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å­¦ç¿’
        self.gate_net = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # [0, 1]
        )

    def forward(self, tf_embeddings, static_weights):
        """
        Args:
            tf_embeddings: [M1, M5, M15, H1, H4]
            static_weights: [0.35, 0.30, 0.20, 0.10, 0.05]
        """
        # Attentioné‡ã¿
        attended, attn_weights = self.attention(tf_embeddings)

        # Gatingå€¤è¨ˆç®—ï¼ˆãƒãƒƒãƒå¹³å‡embedding ã‹ã‚‰ï¼‰
        avg_emb = torch.stack(attended, dim=1).mean(dim=1)  # [batch, d_model]
        gate = self.gate_net(avg_emb)  # [batch, 1]

        # å‹•çš„ãƒ»é™çš„ã®ãƒ–ãƒ¬ãƒ³ãƒ‰
        static_w = static_weights.unsqueeze(0).expand(len(attn_weights), -1)
        blended_weights = gate * attn_weights + (1 - gate) * static_w

        # æ­£è¦åŒ–
        blended_weights = blended_weights / blended_weights.sum(dim=1, keepdim=True)

        # èåˆ
        fused = torch.stack(attended, dim=1)
        output = (fused * blended_weights.unsqueeze(-1)).sum(dim=1)

        return output, blended_weights, gate
```

**æˆåŠŸæŒ‡æ¨™**:
- æœ€å¤§TFé‡ã¿ < 0.6ï¼ˆéå‰°é›†ä¸­é˜²æ­¢ï¼‰
- é‡ã¿ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ > log(5) Ã— 0.5ï¼ˆå¤šæ§˜æ€§ä¿è¨¼ï¼‰
- å‹¾é…ãƒãƒ«ãƒ åˆ†æ•£: TFé–“ã§ CV < 0.5

**æ¤œè¨¼**:
```python
def test_weight_normalization():
    """é‡ã¿æ­£è¦åŒ–ã®æ¤œè¨¼"""
    model = CrossTFFusionWithNormalization(d_model=128, mode='scalp')

    # ãƒ€ãƒŸãƒ¼å…¥åŠ›
    tf_embs = [torch.randn(32, 128) for _ in range(5)]

    output, combined_weights = model(tf_embs)

    # æ¤œè¨¼1: é‡ã¿åˆè¨ˆ=1.0
    assert torch.allclose(combined_weights.sum(dim=1), torch.ones(32))

    # æ¤œè¨¼2: éå‰°é›†ä¸­ãªã—
    max_w = combined_weights.max(dim=1)[0]
    assert (max_w < 0.6).all(), f"éå‰°é›†ä¸­æ¤œå‡º: {max_w.max():.3f}"

    # æ¤œè¨¼3: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¢ºèª
    entropy = -(combined_weights * torch.log(combined_weights + 1e-9)).sum(dim=1)
    assert (entropy > torch.log(torch.tensor(5.0)) * 0.5).all()
```

**å®Ÿè£…æ¨å¥¨**: Phase 1ã§ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³1ï¼ˆå¾Œæ®µæ­£è¦åŒ–ï¼‰ã‚’æ¡ç”¨ã€Phase 2ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³2ï¼ˆGatingï¼‰ã‚’è©•ä¾¡ã€‚

---

## LSTMæœ€é©åŒ–

### é•·çª“é®®åº¦æ¸›è¡°ï¼ˆFreshness Decayï¼‰

**ç›®çš„**: 480æœ¬ï¼ˆM1ï¼‰ã‚„288æœ¬ï¼ˆM5ï¼‰ã®å¤ã„æƒ…å ±ãŒæ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ¼ã—æ½°ã™

**è§£æ±ºç­–**: æ™‚é–“æ¸›è¡°ã«ã‚ˆã‚‹ä½ç½®åŸ‹ã‚è¾¼ã¿æ‹¡å¼µ

```python
class FreshnessDecayEmbedding(nn.Module):
    """é®®åº¦æ¸›è¡°ä½ç½®åŸ‹ã‚è¾¼ã¿"""
    
    def __init__(self, d_model: int, max_len: int = 500, decay_mode: str = "exponential"):
        super().__init__()
        self.d_model = d_model
        self.decay_mode = decay_mode
        
        # æ¨™æº–ä½ç½®åŸ‹ã‚è¾¼ã¿
        self.pos_embedding = nn.Parameter(torch.randn(max_len, d_model))
        
        # æ¸›è¡°ä¿‚æ•°ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.decay_alpha = nn.Parameter(torch.tensor(0.995))  # åˆæœŸå€¤: 480æœ¬ã§~0.1å€
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        é®®åº¦æ¸›è¡°é©ç”¨
        
        Args:
            x: å…¥åŠ›ç³»åˆ— (batch, seq_len, d_model)
        
        Returns:
            x_decayed: æ¸›è¡°é©ç”¨å¾Œ (batch, seq_len, d_model)
        """
        batch, seq_len, d_model = x.shape
        
        # ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå¤ã„=0, æ–°ã—ã„=seq_len-1ï¼‰
        positions = torch.arange(seq_len, device=x.device)
        
        # æ¸›è¡°ä¿‚æ•°è¨ˆç®—
        if self.decay_mode == "exponential":
            # Î±^(seq_len - 1 - pos)
            decay_weights = self.decay_alpha ** (seq_len - 1 - positions)
        elif self.decay_mode == "linear":
            # (pos + 1) / seq_len
            decay_weights = (positions + 1) / seq_len
        else:
            raise ValueError(f"Unknown decay_mode: {self.decay_mode}")
        
        decay_weights = decay_weights.view(1, seq_len, 1)  # (1, seq_len, 1)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿è¿½åŠ 
        pos_emb = self.pos_embedding[:seq_len]  # (seq_len, d_model)
        x = x + pos_emb.unsqueeze(0)
        
        # æ¸›è¡°é©ç”¨
        x_decayed = x * decay_weights
        
        return x_decayed


class MultiTFLSTMWithFreshness(nn.Module):
    """é®®åº¦æ¸›è¡°å¯¾å¿œãƒãƒ«ãƒTF LSTM"""
    
    def __init__(self, config: dict):
        super().__init__()
        
        d_model = config['d_model']
        
        # å„TFç”¨é®®åº¦æ¸›è¡°
        self.freshness_m1 = FreshnessDecayEmbedding(d_model, max_len=480)
        self.freshness_m5 = FreshnessDecayEmbedding(d_model, max_len=288)
        self.freshness_m15 = FreshnessDecayEmbedding(d_model, max_len=192)
        self.freshness_h1 = FreshnessDecayEmbedding(d_model, max_len=96)
        self.freshness_h4 = FreshnessDecayEmbedding(d_model, max_len=48)
        
        # LSTMå±¤
        self.lstm_m1 = nn.LSTM(d_model, d_model, batch_first=True)
        self.lstm_m5 = nn.LSTM(d_model, d_model, batch_first=True)
        # ... ä»–ã®TF
    
    def forward(self, multi_tf_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        é®®åº¦æ¸›è¡°é©ç”¨å¾Œã«LSTMå‡¦ç†
        
        Args:
            multi_tf_features: {
                "m1": (batch, 480, d_model),
                "m5": (batch, 288, d_model),
                ...
            }
        
        Returns:
            fused_output: (batch, d_model)
        """
        # é®®åº¦æ¸›è¡°é©ç”¨
        m1_decayed = self.freshness_m1(multi_tf_features["m1"])
        m5_decayed = self.freshness_m5(multi_tf_features["m5"])
        # ...
        
        # LSTMå‡¦ç†
        m1_out, _ = self.lstm_m1(m1_decayed)
        m5_out, _ = self.lstm_m5(m5_decayed)
        # ...
        
        # æœ€çµ‚çŠ¶æ…‹å–å¾—
        m1_final = m1_out[:, -1, :]  # (batch, d_model)
        m5_final = m5_out[:, -1, :]
        # ...
        
        # Fusionï¼ˆæ—¢å­˜ã®CrossTFFusionä½¿ç”¨ï¼‰
        fused = self.fusion([m1_final, m5_final, ...])
        
        return fused


# æ¸›è¡°æ›²ç·šã®å¯è¦–åŒ–
def visualize_freshness_decay():
    """æ¸›è¡°ä¿‚æ•°ã®å¯è¦–åŒ–"""
    seq_len = 480
    alpha = 0.995
    
    positions = np.arange(seq_len)
    decay = alpha ** (seq_len - 1 - positions)
    
    plt.figure(figsize=(10, 4))
    plt.plot(positions, decay)
    plt.xlabel("Position (0=oldest, 479=newest)")
    plt.ylabel("Weight Multiplier")
    plt.title(f"Freshness Decay (Î±={alpha})")
    plt.grid(True)
    plt.savefig("freshness_decay.png")
```

**æ¸›è¡°ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ**:

| Mode | å¼ | 480æœ¬ã§ã®æ¸›è¡° | ç”¨é€” |
|------|---|--------------|------|
| exponential | Î±^(N-1-t) | 0.995^479 â‰ˆ 0.09 | å¤ã„æƒ…å ±ã‚’å¤§å¹…æ¸›è¡° |
| linear | (t+1)/N | 1/480 â‰ˆ 0.002 | ç·šå½¢æ¸›å°‘ |

**KPIï¼ˆé …ç›®19ï¼‰**:
- æ–°ã—ã„50æœ¬vså¤ã„50æœ¬ã®é‡ã¿æ¯”: â‰¥10:1
- å­¦ç¿’æ¸ˆã¿Î±å€¤: 0.99ï½0.999ç¯„å›²
- ç²¾åº¦æ”¹å–„: â‰¥+1%ï¼ˆæ¸›è¡°ãªã—æ¯”è¼ƒï¼‰

---

### é …ç›®65ãƒ»81å¯¾å¿œ: Incremental LSTMæ¨è«–

**ç›®çš„**: æ¯å›å…¨çª“å†è¨ˆç®—ã¯é…å»¶å¢—å¤§ã®åŸå› 

**è§£æ±ºç­–**: LSTMéš ã‚ŒçŠ¶æ…‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹å·®åˆ†æ›´æ–°

```python
class IncrementalLSTMInference:
    """å·®åˆ†æ›´æ–°LSTMæ¨è«–ï¼ˆé …ç›®65ãƒ»81å¯¾å¿œï¼‰"""
    
    def __init__(self, lstm_model: nn.LSTM, max_cache_size: int = 500):
        self.lstm = lstm_model
        self.max_cache_size = max_cache_size
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆTFåˆ¥ï¼‰
        self.hidden_cache = {}  # {tf: (h, c)}
        self.feature_cache = {}  # {tf: deque of features}
    
    def initialize_cache(self, tf: str, initial_sequence: torch.Tensor):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–ï¼ˆèµ·å‹•æ™‚ãƒ»ãƒªã‚»ãƒƒãƒˆæ™‚ï¼‰
        
        Args:
            tf: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åï¼ˆ"m1", "m5", ...ï¼‰
            initial_sequence: åˆæœŸç³»åˆ— (1, seq_len, d_model)
        """
        # å…¨çª“å‡¦ç†ã—ã¦æœ€çµ‚çŠ¶æ…‹ã‚’ä¿å­˜
        output, (h_n, c_n) = self.lstm(initial_sequence)
        
        self.hidden_cache[tf] = (h_n, c_n)
        
        # ç‰¹å¾´é‡å±¥æ­´ï¼ˆdequeï¼‰
        self.feature_cache[tf] = deque(
            initial_sequence[0].cpu().numpy(),
            maxlen=self.max_cache_size
        )
        
        logger.info(f"{tf} ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–: hidden_shape={h_n.shape}")
    
    def update_incremental(
        self,
        tf: str,
        new_feature: torch.Tensor  # (1, 1, d_model) æ–°ç€1æœ¬
    ) -> torch.Tensor:
        """
        å·®åˆ†æ›´æ–°ï¼ˆæ–°ç€1æœ¬ã®ã¿å‡¦ç†ï¼‰
        
        Args:
            tf: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
            new_feature: æ–°ç€ç‰¹å¾´é‡ (1, 1, d_model)
        
        Returns:
            new_output: æ›´æ–°å¾Œã®å‡ºåŠ› (1, d_model)
        """
        if tf not in self.hidden_cache:
            raise ValueError(f"{tf} ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœªåˆæœŸåŒ–")
        
        # å‰å›ã®éš ã‚ŒçŠ¶æ…‹å–å¾—
        h_prev, c_prev = self.hidden_cache[tf]
        
        # æ–°ç€1æœ¬ã®ã¿LSTMå‡¦ç†
        output, (h_new, c_new) = self.lstm(new_feature, (h_prev, c_prev))
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
        self.hidden_cache[tf] = (h_new, c_new)
        self.feature_cache[tf].append(new_feature[0, 0].cpu().numpy())
        
        return output[:, -1, :]  # (1, d_model)
    
    def reset_if_stale(self, tf: str, threshold_seconds: int = 300):
        """
        å¤ã™ãã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ¥ç¶šæ–­å¾Œãªã©ï¼‰
        
        Args:
            tf: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
            threshold_seconds: ãƒªã‚»ãƒƒãƒˆé–¾å€¤ï¼ˆç§’ï¼‰
        """
        # å®Ÿè£…æ™‚ã¯æœ€çµ‚æ›´æ–°æ™‚åˆ»ã‚’ä¿å­˜ã—ã¦ãƒã‚§ãƒƒã‚¯
        pass


# ä½¿ç”¨ä¾‹
incremental_lstm = IncrementalLSTMInference(
    lstm_model=model.lstm_m1,
    max_cache_size=500
)

# åˆæœŸåŒ–ï¼ˆèµ·å‹•æ™‚ï¼‰
initial_seq = load_initial_480_bars("m1")  # (1, 480, d_model)
incremental_lstm.initialize_cache("m1", initial_seq)

# æ¨è«–ãƒ«ãƒ¼ãƒ—ï¼ˆæ–°ç€tickæ¯ï¼‰
while True:
    new_tick = get_new_tick_feature("m1")  # (1, 1, d_model)
    
    # å·®åˆ†æ›´æ–°ï¼ˆO(1)è¨ˆç®—é‡ï¼‰
    output = incremental_lstm.update_incremental("m1", new_tick)
    
    # äºˆæ¸¬å®Ÿè¡Œ
    prediction = model.direction_head(output)
```

**APIä»•æ§˜**:

| ãƒ¡ã‚½ãƒƒãƒ‰ | è¨ˆç®—é‡ | ç”¨é€” |
|---------|-------|------|
| `initialize_cache()` | O(N) | èµ·å‹•æ™‚ãƒ»ãƒªã‚»ãƒƒãƒˆæ™‚ |
| `update_incremental()` | O(1) | é€šå¸¸æ¨è«–ï¼ˆæ–°ç€1æœ¬ï¼‰ |
| `reset_if_stale()` | O(N) | æ¥ç¶šæ–­å¾©å¸°æ™‚ |

**å®Ÿè£…è©³ç´°ï¼ˆé …ç›®81ï¼‰**:

```python
# éš ã‚ŒçŠ¶æ…‹ã®å½¢çŠ¶
h_n: (num_layers, batch, hidden_size)
c_n: (num_layers, batch, hidden_size)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºè¦‹ç©
hidden_cache_size = num_layers * hidden_size * 2 (h+c) * 4 bytes (float32)
ä¾‹: 2å±¤Ã—128æ¬¡å…ƒÃ—2Ã—4 = 2KB / TF

# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
- CPUä¿å­˜: æ¨è«–ä¸­ã®ã¿GPUã¸è»¢é€
- FP16åŒ–: ãƒ¡ãƒ¢ãƒªåŠæ¸›ï¼ˆç²¾åº¦åŠ£åŒ–<0.1%ï¼‰
```

**KPIï¼ˆé …ç›®65ãƒ»81ï¼‰**:
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›: â‰¥50%ï¼ˆå…¨çª“å†è¨ˆç®—æ¯”è¼ƒï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: <10KB / TFï¼ˆ5TFåˆè¨ˆ<50KBï¼‰
- ãƒªã‚»ãƒƒãƒˆé »åº¦: <1å›/æ—¥ï¼ˆæ¥ç¶šå®‰å®šæ™‚ï¼‰

---

## ğŸ“š é–¢é€£ä»•æ§˜

- [TRAINER_SPEC.md](../TRAINER_SPEC.md) - å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- [PREPROCESSOR_SPEC.md](../PREPROCESSOR_SPEC.md) - å…¥åŠ›ç‰¹å¾´é‡å½¢å¼
- [EXECUTION_LATENCY_SPEC.md](../validator/EXECUTION_LATENCY_SPEC.md) - æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶

---

## ğŸ“ å¤‰æ›´å±¥æ­´

- **2025-10-21**: åˆç‰ˆä½œæˆ
  - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·å®šç¾©ï¼ˆæœ€å¤§å…¬ç´„æ•°çš„å›ºå®šçª“ï¼‰
  - LSTM + Self-Attentionè¨­è¨ˆ
  - Cross-TF Fusionè©³ç´°
  - Attentionå¯è¦–åŒ–ãƒ»æ¤œè¨¼æ–¹æ³•
