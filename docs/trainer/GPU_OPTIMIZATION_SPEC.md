# GPU Optimization Specification

## ç›®çš„

å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã«ãŠã‘ã‚‹GPUæœ€é©åŒ–æ‰‹æ³•ã‚’å®šç¾©ã—ã€å‡¦ç†æ™‚é–“ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚

---

## è¨­è¨ˆæ–¹é‡

### æœ€å„ªå…ˆåŸå‰‡ï¼šãƒ¡ã‚¬ãƒãƒƒãƒæ–¹å¼ï¼ˆChunkDataLoaderï¼‰

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: GPUãƒ¡ãƒ¢ãƒªã«ç›®ä¸€æ¯ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹ã—ã€ãƒ‡ã‚£ã‚¹ã‚¯I/Oãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’è§£æ¶ˆ

**æ—§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆmt5_lstm-modelï¼‰æ¤œè¨¼çµæœ**:
- å­¦ç¿’æ™‚é–“: **59.9%çŸ­ç¸®**ï¼ˆ17åˆ†48ç§’ â†’ 7åˆ†8ç§’ï¼‰
- Issue #263 Phase 3 ã§å®Ÿè¨¼æ¸ˆã¿

**å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
```
1. HDF5ã‹ã‚‰å¤§ããªãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ï¼ˆä¾‹: 12GBï¼‰
2. GPUãƒ¡ãƒ¢ãƒªã«ä¸€æ‹¬è»¢é€
3. GPUãƒ¡ãƒ¢ãƒªä¸Šã§ãƒãƒƒãƒå‡¦ç†å®Œçµ
4. æ¬¡ãƒãƒ£ãƒ³ã‚¯ã¸ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
```

---

## ChunkDataLoader å®Ÿè£…

### åŸºæœ¬è¨­è¨ˆ

```python
class ChunkDataLoader:
    """
    HDF5ã‹ã‚‰å¤§ããªãƒãƒ£ãƒ³ã‚¯å˜ä½ã§GPUãƒ¡ãƒ¢ãƒªã«ç›´æ¥è»¢é€
    
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨ä¾‹:
    - GPUãƒ¡ãƒ¢ãƒª: 16GBï¼ˆä¾‹: NVIDIA RTX 4070ï¼‰
    - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: 12GBï¼ˆGPUãƒ¡ãƒ¢ãƒªã®75%ï¼‰
    - å…¨ãƒ‡ãƒ¼ã‚¿: 51,779ã‚µãƒ³ãƒ—ãƒ« Ã— 360 Ã— 167 Ã— 4bytes â‰ˆ 12.4GB
    - ãƒãƒ£ãƒ³ã‚¯æ•°: 1ãƒãƒ£ãƒ³ã‚¯ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«ä¸€æ‹¬è»¢é€å¯èƒ½ï¼‰
    """
    
    def __init__(
        self,
        hdf5_path: str,
        batch_size: int = 128,
        chunk_size_gb: float = 12.0,
        device: str = 'cuda',
        shuffle: bool = False,
        drop_last: bool = False
    ):
        self.hdf5_path = hdf5_path
        self.batch_size = batch_size
        self.chunk_size_gb = chunk_size_gb
        self.device = torch.device(device)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—
        with h5py.File(hdf5_path, 'r') as f:
            self.num_samples = f['features'].shape[0]
            self.sequence_length = f['features'].shape[1]
            self.feature_count = f['features'].shape[2]
        
        # ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œè¨ˆç®—
        bytes_per_sample = (
            self.sequence_length * self.feature_count * 4 +  # features (float32)
            self.target_length * 4  # targets (float32)
        )
        samples_per_chunk = int((chunk_size_gb * 1024**3) / bytes_per_sample)
        samples_per_chunk = max(batch_size, min(samples_per_chunk, self.num_samples))
        
        # ãƒãƒ£ãƒ³ã‚¯ç¯„å›²è¨ˆç®—
        self.chunk_ranges = []
        for start in range(0, self.num_samples, samples_per_chunk):
            end = min(start + samples_per_chunk, self.num_samples)
            self.chunk_ranges.append((start, end))
    
    def _load_chunk_to_gpu(self, start: int, end: int):
        """HDF5ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿ã€GPUãƒ¡ãƒ¢ãƒªã«è»¢é€"""
        with h5py.File(self.hdf5_path, 'r') as f:
            # HDF5ã‹ã‚‰èª­ã¿è¾¼ã¿
            features = f['features'][start:end]
            targets = f['targets'][start:end]
        
        # GPUã«è»¢é€ï¼ˆãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªçµŒç”±ã§é«˜é€ŸåŒ–ï¼‰
        chunk_features = torch.from_numpy(features).pin_memory().to(self.device, non_blocking=True)
        chunk_targets = torch.from_numpy(targets).pin_memory().to(self.device, non_blocking=True)
        
        return chunk_features, chunk_targets
    
    def __iter__(self):
        """ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿: ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«GPUãƒ¡ãƒ¢ãƒªå±•é–‹ â†’ ãƒãƒƒãƒå‡¦ç†"""
        for start, end in self.chunk_ranges:
            # ãƒãƒ£ãƒ³ã‚¯ã‚’GPUãƒ¡ãƒ¢ãƒªã«å±•é–‹
            chunk_features, chunk_targets = self._load_chunk_to_gpu(start, end)
            
            # GPUãƒ¡ãƒ¢ãƒªä¸Šã§ãƒãƒƒãƒå‡¦ç†
            chunk_size = end - start
            for i in range(0, chunk_size, self.batch_size):
                batch_end = min(i + self.batch_size, chunk_size)
                
                # GPUãƒ¡ãƒ¢ãƒªä¸Šã§ã‚¹ãƒ©ã‚¤ã‚¹ï¼ˆé«˜é€Ÿï¼‰
                batch_features = chunk_features[i:batch_end]
                batch_targets = chunk_targets[i:batch_end]
                
                yield batch_features, batch_targets
            
            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†å¾Œã€ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del chunk_features, chunk_targets
            torch.cuda.empty_cache()
```

### ä½¿ç”¨æ–¹æ³•

```python
# ChunkDataLoaderä½œæˆ
train_loader = ChunkDataLoader(
    hdf5_path='data/aligned_train.h5',
    batch_size=128,
    chunk_size_gb=12.0,  # GPUãƒ¡ãƒ¢ãƒªã®75%
    device='cuda',
    shuffle=False,  # ãƒãƒ«ãƒTFã§ã¯æ™‚ç³»åˆ—ç¶­æŒå¿…é ˆ
    drop_last=False
)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for epoch in range(num_epochs):
    for batch_features, batch_targets in train_loader:
        # æ—¢ã«GPUä¸Šã«ã‚ã‚‹ãŸã‚ã€to(device)ä¸è¦
        outputs = model(batch_features)
        loss = criterion(outputs, batch_targets)
        loss.backward()
        optimizer.step()
```

---

## GPU ãƒ¡ãƒ¢ãƒªåˆ¥æ¨å¥¨è¨­å®š

### ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—å¼

```python
# å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³: GPUãƒ¡ãƒ¢ãƒªã®75%ã‚’ä½¿ç”¨
chunk_size_gb = gpu_memory_gb * 0.75

# ã‚µãƒ³ãƒ—ãƒ«1ã¤ã‚ãŸã‚Šã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
bytes_per_sample = (
    sequence_length * feature_count * 4 +  # features (float32)
    target_length * 4  # targets (float32)
)

# ãƒãƒ£ãƒ³ã‚¯å†…ã‚µãƒ³ãƒ—ãƒ«æ•°
samples_per_chunk = int((chunk_size_gb * 1024**3) / bytes_per_sample)
```

### GPU ãƒ¡ãƒ¢ãƒªåˆ¥æ¨å¥¨å€¤

| GPU ãƒ¡ãƒ¢ãƒª | ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º | ãƒãƒƒãƒã‚µã‚¤ã‚º | å‚™è€ƒ |
|-----------|--------------|------------|------|
| **8GB** | 6.0 GB | 64 | ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒ¬ãƒ™ãƒ« |
| **12GB** | 9.0 GB | 96 | ãƒŸãƒ‰ãƒ«ãƒ¬ãƒ³ã‚¸ |
| **16GB** | 12.0 GB | 128 | æ¨å¥¨æ§‹æˆ â­ |
| **24GB** | 18.0 GB | 192 | ãƒã‚¤ã‚¨ãƒ³ãƒ‰ |

**ãƒãƒ«ãƒTF ã®è¿½åŠ è€ƒæ…®**:
- M1: 480 bars Ã— F features
- M5: 288 bars Ã— F features
- M15: 192 bars Ã— F features
- H1: 96 bars Ã— F features
- H4: 48 bars Ã— F features

åˆè¨ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 480+288+192+96+48 = **1104 bars**

**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ä¾‹**ï¼ˆF=128, float32ï¼‰:
```
features: 1104 Ã— 128 Ã— 4 bytes = 565 KB/sample
targets: 36 Ã— 1 Ã— 4 bytes = 144 bytes/sample
total: ~565 KB/sample

16GB GPU ã®å ´åˆ:
chunk_samples = (12 * 1024^3) / (565 * 1024) â‰ˆ 21,000 samples
```

---

## DataLoader æœ€é©åŒ–è¨­å®š

### é‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
# æ¨å¥¨è¨­å®šï¼ˆæ—§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œè¨¼æ¸ˆã¿ï¼‰
dataloader_config = {
    'batch_size': 128,
    'num_workers': 0,        # ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆCOWå•é¡Œå›é¿ï¼‰
    'pin_memory': True,      # GPUè»¢é€é«˜é€ŸåŒ–
    'prefetch_factor': None, # num_workers=0 æ™‚ã¯ä¸è¦
    'persistent_workers': False,  # num_workers=0 æ™‚ã¯ä¸è¦
    'drop_last': False       # å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
}
```

### num_workers=0 ã®ç†ç”±

**Copy-on-Write (COW) å•é¡Œå›é¿**:
- ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æ™‚ã€å­ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ â†’ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—å¤§
- HDF5ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«ã®å…±æœ‰å•é¡Œ
- ãƒ¡ã‚¬ãƒãƒƒãƒæ–¹å¼ã§ã¯ã€ãƒ‡ã‚£ã‚¹ã‚¯I/OãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã§ãªã„ãŸã‚ä¸è¦

**ä¾‹å¤–**: å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆGPU ã«ä¸€æ‹¬å±•é–‹ä¸å¯ï¼‰ã®å ´åˆã¯ `num_workers=2ã€œ4` ã‚‚æ¤œè¨

### pin_memory=True ã®åŠ¹æœ

**CPU â†’ GPU è»¢é€é«˜é€ŸåŒ–**:
- ãƒšãƒ¼ã‚¸ãƒ­ãƒƒã‚¯ï¼ˆãƒ”ãƒ³ç•™ã‚ï¼‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨
- DMAï¼ˆDirect Memory Accessï¼‰ã«ã‚ˆã‚‹é«˜é€Ÿè»¢é€
- ç´„ 10ã€œ30% é«˜é€ŸåŒ–

**å®Ÿè£…ä¾‹**:
```python
# NumPy â†’ Tensor å¤‰æ›æ™‚ã«ãƒ”ãƒ³ç•™ã‚
tensor = torch.from_numpy(data).pin_memory().to(device, non_blocking=True)
```

---

## Mixed Precision (FP16/AMP)

### åŠ¹æœ

- **GPUè¨ˆç®—é€Ÿåº¦**: 2ã€œ3å€é«˜é€ŸåŒ–
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ç´„50%å‰Šæ¸›
- **ç²¾åº¦**: ã»ã¼åŠ£åŒ–ãªã—ï¼ˆå‹•çš„Loss Scalingä½¿ç”¨æ™‚ï¼‰

### PyTorchå®Ÿè£…

```python
from torch.cuda.amp import autocast, GradScaler

# ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼åˆæœŸåŒ–
scaler = GradScaler()

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for batch_features, batch_targets in train_loader:
    optimizer.zero_grad()
    
    # Mixed Precision ã§ forward
    with autocast():
        outputs = model(batch_features)
        loss = criterion(outputs, batch_targets)
    
    # ã‚¹ã‚±ãƒ¼ãƒ«ã—ãŸå‹¾é…ã§ backward
    scaler.scale(loss).backward()
    
    # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚¹ãƒ†ãƒƒãƒ—
    scaler.step(optimizer)
    scaler.update()
```

### æ•°å€¤å®‰å®šæ€§å¯¾ç­–

**Loss Scaling**:
- å‹¾é…ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
- PyTorch ã® GradScaler ãŒè‡ªå‹•èª¿æ•´
- åˆæœŸã‚¹ã‚±ãƒ¼ãƒ«: 2^16ã€å‹•çš„ã«å¢—æ¸›

**æ³¨æ„ç‚¹**:
- BatchNorm ã¯ FP32 ã§è¨ˆç®—ï¼ˆPyTorch è‡ªå‹•å‡¦ç†ï¼‰
- Loss è¨ˆç®—ã‚‚ FP32 æ¨å¥¨ï¼ˆç²¾åº¦ç¶­æŒï¼‰

---

## Gradient Accumulation

### ç›®çš„

å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§ï¼ˆGPUãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ï¼‰

### å®Ÿè£…

```python
accumulation_steps = 4  # å®ŸåŠ¹batch_size = 128 Ã— 4 = 512

optimizer.zero_grad()

for i, (batch_features, batch_targets) in enumerate(train_loader):
    with autocast():
        outputs = model(batch_features)
        loss = criterion(outputs, batch_targets)
    
    # å‹¾é…ã‚’ç´¯ç©ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼‰
    loss = loss / accumulation_steps
    scaler.scale(loss).backward()
    
    # accumulation_steps ã”ã¨ã«æ›´æ–°
    if (i + 1) % accumulation_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
```

**æ³¨æ„**: ãƒ¡ã‚¬ãƒãƒƒãƒæ–¹å¼ã§ã¯é€šå¸¸ä¸è¦ï¼ˆæ—¢ã«å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºç¢ºä¿å¯èƒ½ï¼‰

---

## ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### 1. Gradient Checkpointing

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: ç´„50%ï¼ˆè¨ˆç®—æ™‚é–“ +20%ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰

```python
from torch.utils.checkpoint import checkpoint

class LSTMWithCheckpointing(nn.Module):
    def forward(self, x):
        # ä¸­é–“å±¤ã§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        x = checkpoint(self.lstm_layer, x)
        return self.output_layer(x)
```

**æ¨å¥¨**: GPUãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®ã¿ä½¿ç”¨

### 2. ãƒ¢ãƒ‡ãƒ«åœ§ç¸®

**é‡å­åŒ–**ï¼ˆæ¨è«–æ™‚ã®ã¿æ¨å¥¨ï¼‰:
```python
# INT8é‡å­åŒ–ï¼ˆæ¨è«–å°‚ç”¨ï¼‰
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.LSTM, torch.nn.Linear}, dtype=torch.qint8
)
```

### 3. ãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–å¯¾ç­–

```python
# ã‚¨ãƒãƒƒã‚¯é–“ã§ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
torch.cuda.empty_cache()

# ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–ï¼ˆå¿…è¦æ™‚ï¼‰
torch.cuda.reset_peak_memory_stats()
```

---

## ãƒãƒ«ãƒGPUå¯¾å¿œï¼ˆå°†æ¥æ‹¡å¼µï¼‰

### ç¾çŠ¶

å˜ç‹¬GPUæ§‹æˆï¼ˆPhase 1ï¼‰

### å°†æ¥å¯¾å¿œï¼ˆPhase 2ä»¥é™ï¼‰

**DistributedDataParallel (DDP)**:
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆæœŸåŒ–
dist.init_process_group(backend='nccl')
local_rank = int(os.environ['LOCAL_RANK'])
torch.cuda.set_device(local_rank)

# ãƒ¢ãƒ‡ãƒ«ã‚’DDPã§ãƒ©ãƒƒãƒ—
model = model.to(local_rank)
model = DDP(model, device_ids=[local_rank])

# DistributedSamplerä½¿ç”¨
train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)
train_loader = DataLoader(dataset, sampler=train_sampler, ...)
```

**æ³¨æ„**: ãƒãƒ«ãƒTFã®æ™‚ç³»åˆ—æ•´åˆæ€§ã‚’ç¶­æŒã™ã‚‹å¿…è¦ã‚ã‚Š

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

### GPUä½¿ç”¨ç‡ç¢ºèª

```python
import torch

def log_gpu_stats(logger):
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        logger.info(f"ğŸ® GPU ãƒ¡ãƒ¢ãƒª:")
        logger.info(f"   å‰²ã‚Šå½“ã¦: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)")
        logger.info(f"   äºˆç´„: {reserved:.2f}GB")
```

### nvidia-smi ç›£è¦–

```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
watch -n 1 nvidia-smi

# ãƒ­ã‚°å‡ºåŠ›
nvidia-smi --query-gpu=timestamp,memory.used,memory.total,utilization.gpu --format=csv -l 1 > gpu_log.csv
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CUDA Out of Memory

**å¯¾å‡¦å„ªå…ˆé †ä½**:
1. **ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºå‰Šæ¸›**: `chunk_size_gb` ã‚’ 75% â†’ 60% ã«
2. **ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›**: 128 â†’ 96 â†’ 64
3. **Gradient Accumulation**: å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºç¶­æŒ
4. **Gradient Checkpointing**: ãƒ¡ãƒ¢ãƒª50%å‰Šæ¸›ï¼ˆé€Ÿåº¦-20%ï¼‰
5. **Mixed Precision**: æ—¢ã«æœ‰åŠ¹ãªå ´åˆã¯åŠ¹æœè–„

### ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º

```python
import gc
import torch

initial_memory = torch.cuda.memory_allocated()

for epoch in range(num_epochs):
    # å­¦ç¿’å‡¦ç†
    ...
    
    # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ãƒã‚§ãƒƒã‚¯
    current_memory = torch.cuda.memory_allocated()
    memory_increase = (current_memory - initial_memory) / 1024**2
    
    if memory_increase > 100:  # 100MBä»¥ä¸Šå¢—åŠ 
        logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç–‘ã„: +{memory_increase:.1f}MB")
        gc.collect()
        torch.cuda.empty_cache()
```

### ä½ã„GPUä½¿ç”¨ç‡

**åŸå› **:
- CPUå‰å‡¦ç†ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ â†’ ãƒ¡ã‚¬ãƒãƒƒãƒæ–¹å¼ã§è§£æ±º
- å°ã•ã™ãã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚º â†’ å¢—åŠ 
- ãƒ‡ã‚£ã‚¹ã‚¯I/Oå¾…ã¡ â†’ ChunkDataLoader ã§è§£æ±º

---

## è¨­å®šä¾‹ï¼ˆconfig/training.yamlï¼‰

```yaml
gpu_optimization:
  # ãƒ¡ã‚¬ãƒãƒƒãƒæ–¹å¼ï¼ˆæ¨å¥¨ï¼‰
  use_chunk_loader: true
  chunk_size_gb: 12.0        # GPU 16GB ã®å ´åˆ
  
  # DataLoaderè¨­å®š
  batch_size: 128
  num_workers: 0             # ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹
  pin_memory: true           # GPUè»¢é€é«˜é€ŸåŒ–
  
  # Mixed Precision
  use_amp: true              # FP16æœ‰åŠ¹åŒ–
  
  # Gradient Accumulationï¼ˆé€šå¸¸ä¸è¦ï¼‰
  gradient_accumulation_steps: 1
  
  # ãƒ¡ãƒ¢ãƒªç®¡ç†
  empty_cache_every_n_epochs: 1
  
  # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
  log_gpu_stats_every_n_batches: 100
```

---

## æœŸå¾…åŠ¹æœï¼ˆæ—§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿç¸¾ï¼‰

| æœ€é©åŒ–æ‰‹æ³• | å­¦ç¿’æ™‚é–“çŸ­ç¸® | å‚™è€ƒ |
|-----------|------------|------|
| å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º1: batch_size=128 | **4.1%** | 17åˆ†48ç§’ â†’ 17åˆ†3ç§’ |
| å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º2: hiddençŠ¶æ…‹å¼•ãç¶™ã | - | æ™‚ç³»åˆ—é€£ç¶šæ€§ç¢ºä¿ |
| å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º3: ChunkDataLoader | **59.9%** | 17åˆ†48ç§’ â†’ **7åˆ†8ç§’** â­ |
| Mixed Precision (AMP) | **2ã€œ3å€** | è¿½åŠ çš„åŠ¹æœ |

**åˆè¨ˆåŠ¹æœ**: å¾“æ¥æ¯” **ç´„80%çŸ­ç¸®** å¯èƒ½

---

## å‚è€ƒè³‡æ–™

- **å­¦ç¿’ä»•æ§˜**: `docs/TRAINER_SPEC.md`
- **ãƒãƒ«ãƒTFèåˆ**: `docs/trainer/MULTI_TF_FUSION_SPEC.md`
- **æ—§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿè£…**: `mt5_lstm-model/src/train/chunk_dataloader.py`
- **æ—§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œè¨¼**: `mt5_lstm-model/docs/TRAIN_SPEC.md` (Issue #263)
