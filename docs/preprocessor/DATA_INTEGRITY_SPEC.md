# ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼ä»•æ§˜ (DATA_INTEGRITY_SPEC)

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**æ›´æ–°æ—¥**: 2025-10-22
**è²¬ä»»è€…**: core-team
**ã‚«ãƒ†ã‚´ãƒª**: å‰å‡¦ç†ã‚µãƒ–ä»•æ§˜

---

## ğŸ“‹ ç›®çš„

å‰å‡¦ç†æ®µéšã§ã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’æ¤œè¨¼ã—ã€åˆ—é †åºä¸ä¸€è‡´ã‚„TFãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—ã«å¯¾ã™ã‚‹å‡¦ç†ã‚’å®šç¾©ã™ã‚‹ã€‚

**å¯¾è±¡é …ç›®**:
- åˆ—é †åºãƒãƒƒã‚·ãƒ¥æ¤œè¨¼
- TFãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

---

## é …ç›®34å¯¾å¿œ: åˆ—é †åºãƒãƒƒã‚·ãƒ¥æ¤œè¨¼

**ç›®çš„**: ç‰¹å¾´é‡åˆ—é †åºå¤‰æ›´ã¯attention weightèª¤é©ç”¨ã‚„ã‚¹ã‚±ãƒ¼ãƒ«æ··åœ¨ã‚’èª˜ç™ºã€‚Runtimeæ¤œè¨¼ã§ãƒŸã‚¹ãƒãƒƒãƒæ¤œå‡ºã™ã‚‹ã€‚

**è§£æ±ºç­–**: ordered_feature_names_hash ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ + Runtimeæ¤œè¨¼

```python
import hashlib
import json
from typing import List

class ColumnOrderValidator:
    """åˆ—é †åºãƒãƒƒã‚·ãƒ¥æ¤œè¨¼"""
    
    def compute_ordered_hash(self, feature_names: List[str]) -> str:
        """
        é †åºä»˜ãç‰¹å¾´é‡åã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        
        Args:
            feature_names: é †åºã‚’ä¿æŒã—ãŸç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        
        Returns:
            hash: SHA-256ã®å…ˆé ­16æ–‡å­—
        """
        # é †åºä¿æŒã®ãŸã‚ãƒªã‚¹ãƒˆã‚’ãã®ã¾ã¾JSONåŒ–
        ordered_str = json.dumps(feature_names, ensure_ascii=False)
        hash_full = hashlib.sha256(ordered_str.encode()).hexdigest()
        return hash_full[:16]
    
    def save_column_order_metadata(
        self,
        h5_file,
        feature_names: List[str]
    ):
        """
        HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ—é †åºãƒãƒƒã‚·ãƒ¥ä¿å­˜
        
        Args:
            h5_file: h5py.File object
            feature_names: é †åºä»˜ãç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        """
        metadata_group = h5_file.require_group("metadata")
        
        # ç‰¹å¾´é‡åä¿å­˜
        if "feature_names" in metadata_group:
            del metadata_group["feature_names"]
        metadata_group.create_dataset(
            "feature_names",
            data=[name.encode() for name in feature_names]
        )
        
        # åˆ—é †åºãƒãƒƒã‚·ãƒ¥ä¿å­˜
        ordered_hash = self.compute_ordered_hash(feature_names)
        metadata_group.attrs["feature_names_hash"] = ordered_hash
        
        logger.info(f"åˆ—é †åºãƒãƒƒã‚·ãƒ¥ä¿å­˜: {ordered_hash}")
    
    def verify_column_order_at_runtime(
        self,
        expected_feature_names: List[str],
        runtime_feature_names: List[str]
    ) -> Dict[str, Any]:
        """
        Runtimeæ™‚ã®åˆ—é †åºæ¤œè¨¼
        
        Args:
            expected_feature_names: å­¦ç¿’æ™‚ã®é †åºï¼ˆHDF5ã‹ã‚‰èª­è¾¼ï¼‰
            runtime_feature_names: æ¨è«–æ™‚ã®é †åº
        
        Returns:
            {
                "valid": bool,
                "expected_hash": str,
                "runtime_hash": str,
                "mismatches": List[Dict],  # ãƒŸã‚¹ãƒãƒƒãƒè©³ç´°
                "error_message": str
            }
        """
        expected_hash = self.compute_ordered_hash(expected_feature_names)
        runtime_hash = self.compute_ordered_hash(runtime_feature_names)
        
        if expected_hash == runtime_hash:
            return {
                "valid": True,
                "expected_hash": expected_hash,
                "runtime_hash": runtime_hash,
                "mismatches": [],
                "error_message": ""
            }
        
        # ãƒŸã‚¹ãƒãƒƒãƒè©³ç´°åˆ†æ
        mismatches = []
        
        # é•·ã•ä¸ä¸€è‡´
        if len(expected_feature_names) != len(runtime_feature_names):
            mismatches.append({
                "type": "length_mismatch",
                "expected": len(expected_feature_names),
                "runtime": len(runtime_feature_names)
            })
        
        # é †åºãƒŸã‚¹ãƒãƒƒãƒæ¤œå‡º
        max_len = max(len(expected_feature_names), len(runtime_feature_names))
        for i in range(max_len):
            exp_name = expected_feature_names[i] if i < len(expected_feature_names) else "<missing>"
            run_name = runtime_feature_names[i] if i < len(runtime_feature_names) else "<missing>"
            
            if exp_name != run_name:
                mismatches.append({
                    "type": "order_mismatch",
                    "index": i,
                    "expected": exp_name,
                    "runtime": run_name
                })
        
        error_msg = (
            f"åˆ—é †åºãƒãƒƒã‚·ãƒ¥ä¸ä¸€è‡´: expected={expected_hash}, runtime={runtime_hash}\n"
            f"ãƒŸã‚¹ãƒãƒƒãƒæ•°: {len(mismatches)}"
        )
        
        return {
            "valid": False,
            "expected_hash": expected_hash,
            "runtime_hash": runtime_hash,
            "mismatches": mismatches[:10],  # æœ€åˆã®10ä»¶ã®ã¿
            "error_message": error_msg
        }


# ä½¿ç”¨ä¾‹: å‰å‡¦ç†å®Œäº†æ™‚
validator = ColumnOrderValidator()

with h5py.File("preprocessed.h5", "a") as h5f:
    feature_names = ["rsi_m1", "macd_m1", "atr_m5", ...]  # é †åºé‡è¦
    validator.save_column_order_metadata(h5f, feature_names)


# ä½¿ç”¨ä¾‹: æ¨è«–æ™‚
def load_and_validate_features(preprocessed_path: str, runtime_features: pd.DataFrame):
    """
    æ¨è«–æ™‚ã®åˆ—é †åºæ¤œè¨¼
    
    Args:
        preprocessed_path: å­¦ç¿’æ¸ˆã¿HDF5ãƒ‘ã‚¹
        runtime_features: æ¨è«–æ™‚ã®ç‰¹å¾´é‡DataFrame
    
    Raises:
        ValueError: åˆ—é †åºä¸ä¸€è‡´æ™‚
    """
    validator = ColumnOrderValidator()
    
    # å­¦ç¿’æ™‚ã®åˆ—é †åºèª­è¾¼
    with h5py.File(preprocessed_path, "r") as h5f:
        expected_names = [
            name.decode() for name in h5f["metadata/feature_names"][:]
        ]
    
    # Runtimeåˆ—é †åº
    runtime_names = list(runtime_features.columns)
    
    # æ¤œè¨¼å®Ÿè¡Œ
    result = validator.verify_column_order_at_runtime(
        expected_feature_names=expected_names,
        runtime_feature_names=runtime_names
    )
    
    if not result["valid"]:
        logger.error(f"âŒ åˆ—é †åºæ¤œè¨¼å¤±æ•—: {result['error_message']}")
        
        # ãƒŸã‚¹ãƒãƒƒãƒè©³ç´°ãƒ­ã‚°
        for mismatch in result["mismatches"][:5]:
            logger.error(f"   - {mismatch}")
        
        raise ValueError(
            f"Feature column order mismatch: {result['error_message']}"
        )
    
    logger.info(f"âœ… åˆ—é †åºæ¤œè¨¼æˆåŠŸ: hash={result['expected_hash']}")
    
    return runtime_features[expected_names]  # æ­£ã—ã„é †åºã§è¿”ã™


# ä½¿ç”¨ä¾‹: ãƒãƒƒãƒæ¨è«–
try:
    validated_features = load_and_validate_features(
        "models/fx_mtf_20251022_preprocessed.h5",
        runtime_features
    )
    predictions = model.predict(validated_features)
except ValueError as e:
    logger.error(f"æ¨è«–å¤±æ•—: {e}")
    # åˆ—é †åºä¿®æ­£ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼é€šçŸ¥
```

**åˆ—é †åºãƒãƒƒã‚·ãƒ¥ä»•æ§˜**:
- **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: SHA-256ï¼ˆå…ˆé ­16æ–‡å­—ä½¿ç”¨ï¼‰
- **ä¿å­˜å ´æ‰€**: `/metadata/feature_names_hash`ï¼ˆHDF5 attributeï¼‰
- **æ¤œè¨¼ã‚¿ã‚¤ãƒŸãƒ³ã‚°**: 
  - å‰å‡¦ç†å®Œäº†æ™‚: ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆãƒ»ä¿å­˜
  - æ¨è«–é–‹å§‹æ™‚: Runtimeåˆ—é †åºã¨ãƒãƒƒã‚·ãƒ¥æ¯”è¼ƒ
  - ONNXå¤‰æ›æ™‚: å¥‘ç´„ãƒãƒƒã‚·ãƒ¥ã¨ã®æ•´åˆæ€§ç¢ºèª
- **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: ãƒŸã‚¹ãƒãƒƒãƒæ™‚ã¯å³åº§ã«ValueError raiseï¼ˆæ¨è«–ç¶šè¡Œç¦æ­¢ï¼‰

**æˆåŠŸæŒ‡æ¨™**:
- åˆ—é †åºãƒŸã‚¹ãƒãƒƒãƒæ¤œå‡ºç‡: 100%
- False Positiveç‡: 0%ï¼ˆæ­£ã—ã„é †åºã§èª¤æ¤œå‡ºãªã—ï¼‰
- Runtimeæ¤œè¨¼ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: <10ms

**æ¤œè¨¼**:
```python
def test_column_order_validation():
    """åˆ—é †åºãƒãƒƒã‚·ãƒ¥æ¤œè¨¼ã®å‹•ä½œç¢ºèª"""
    validator = ColumnOrderValidator()
    
    # ã‚±ãƒ¼ã‚¹1: æ­£ã—ã„é †åº
    expected = ["feat1", "feat2", "feat3"]
    runtime = ["feat1", "feat2", "feat3"]
    result = validator.verify_column_order_at_runtime(expected, runtime)
    assert result["valid"] == True
    
    # ã‚±ãƒ¼ã‚¹2: é †åºå…¥ã‚Œæ›¿ã‚ã‚Š
    runtime = ["feat1", "feat3", "feat2"]
    result = validator.verify_column_order_at_runtime(expected, runtime)
    assert result["valid"] == False
    assert any(m["type"] == "order_mismatch" for m in result["mismatches"])
    
    # ã‚±ãƒ¼ã‚¹3: åˆ—æ•°ä¸ä¸€è‡´
    runtime = ["feat1", "feat2"]
    result = validator.verify_column_order_at_runtime(expected, runtime)
    assert result["valid"] == False
    assert any(m["type"] == "length_mismatch" for m in result["mismatches"])
```

---


---

## é …ç›®46å¯¾å¿œ: TFãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

**ç›®çš„**: H1/H4é–‹å§‹ãƒãƒ¼æ¬ ææ™‚ã®å‡¦ç†ãŒæœªå®šç¾© â†’ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é…å»¶ã‚„æœªæ¥æ¨å®šèª¤ã‚Š

**è§£æ±ºç­–**: `fallback="skip"` or `"use_last_closed"` ã®ãƒãƒªã‚·ãƒ¼ãƒ•ãƒ©ã‚°åŒ–

```python
class TFMappingFallbackPolicy:
    """TFãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    
    def __init__(self, config: dict):
        self.fallback_policy = config.get("tf_mapping_fallback", "skip")  # "skip" | "use_last_closed"
        self.max_lookback_seconds = config.get("max_lookback_seconds", {
            "H1": 3600,   # H1ã¯1æ™‚é–“å‰ã¾ã§è¨±å®¹
            "H4": 7200    # H4ã¯2æ™‚é–“å‰ã¾ã§è¨±å®¹
        })
    
    def map_lower_to_higher_tf(
        self,
        lower_tf_timestamps: np.ndarray,
        higher_tf_data: pd.DataFrame,
        higher_tf_timestamps: np.ndarray,
        tf_name: str
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        ä¸‹ä½TF â†’ ä¸Šä½TFãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
        
        Args:
            lower_tf_timestamps: M1/M5ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            higher_tf_data: H1/H4ã®ãƒ‡ãƒ¼ã‚¿
            higher_tf_timestamps: H1/H4ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            tf_name: ä¸Šä½TFåï¼ˆ"H1" or "H4"ï¼‰
        
        Returns:
            (mapped_indices, mapping_info)
        """
        N = len(lower_tf_timestamps)
        mapped_indices = np.full(N, -1, dtype=int)  # -1=ãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—
        
        fallback_count = 0
        skip_count = 0
        
        for i, ts in enumerate(lower_tf_timestamps):
            # å®Œå…¨ä¸€è‡´æ¤œç´¢
            exact_match = np.where(higher_tf_timestamps <= ts)[0]
            
            if len(exact_match) > 0:
                # æœ€ã‚‚è¿‘ã„éå»ã®ãƒãƒ¼ã‚’ä½¿ç”¨
                mapped_indices[i] = exact_match[-1]
            else:
                # ãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•— â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é©ç”¨
                if self.fallback_policy == "skip":
                    # ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒã‚¹ã‚¯å¯¾è±¡ï¼‰
                    mapped_indices[i] = -1
                    skip_count += 1
                
                elif self.fallback_policy == "use_last_closed":
                    # è¨±å®¹æ™‚é–“å†…ã®æœ€è¿‘éå»ãƒãƒ¼ã‚’ä½¿ç”¨
                    max_lookback = self.max_lookback_seconds.get(tf_name, 3600)
                    
                    # æ™‚é–“å·®è¨ˆç®—
                    time_diffs = ts - higher_tf_timestamps
                    valid_lookback = (time_diffs > 0) & (time_diffs <= max_lookback)
                    
                    if valid_lookback.any():
                        # æœ€ã‚‚è¿‘ã„éå»ãƒãƒ¼
                        valid_indices = np.where(valid_lookback)[0]
                        mapped_indices[i] = valid_indices[-1]
                        fallback_count += 1
                    else:
                        # è¨±å®¹ç¯„å›²å¤– â†’ ã‚¹ã‚­ãƒƒãƒ—
                        mapped_indices[i] = -1
                        skip_count += 1
        
        mapping_info = {
            "policy": self.fallback_policy,
            "total_mappings": N,
            "successful_mappings": int((mapped_indices >= 0).sum()),
            "fallback_used": fallback_count,
            "skipped": skip_count,
            "success_rate": float((mapped_indices >= 0).mean())
        }
        
        logger.info(
            f"{tf_name}ãƒãƒƒãƒ”ãƒ³ã‚°: æˆåŠŸç‡={mapping_info['success_rate']:.2%}, "
            f"fallback={fallback_count}, skip={skip_count}"
        )
        
        return mapped_indices, mapping_info


# ä½¿ç”¨ä¾‹: ãƒãƒ«ãƒTFçµ±åˆæ™‚
fallback_policy = TFMappingFallbackPolicy({
    "tf_mapping_fallback": "use_last_closed",  # or "skip"
    "max_lookback_seconds": {"H1": 3600, "H4": 7200}
})

# M5 â†’ H1ãƒãƒƒãƒ”ãƒ³ã‚°
h1_indices, mapping_info = fallback_policy.map_lower_to_higher_tf(
    lower_tf_timestamps=timestamps_m5,
    higher_tf_data=features_h1,
    higher_tf_timestamps=timestamps_h1,
    tf_name="H1"
)

# ãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—ç®‡æ‰€ã‚’ãƒã‚¹ã‚¯
mask = (h1_indices >= 0).astype(float)
```

**KPIï¼ˆé …ç›®46ï¼‰**:
- ãƒãƒƒãƒ”ãƒ³ã‚°æˆåŠŸç‡: â‰¥95%
- fallbackä½¿ç”¨ç‡: <5%
- skipç‡: <2%

---



---

## ğŸ”— é–¢é€£ä»•æ§˜

- [PREPROCESSOR_SPEC.md](../PREPROCESSOR_SPEC.md) - å‰å‡¦ç†ãƒ¡ã‚¤ãƒ³ä»•æ§˜
- [MODEL_IO_CONTRACT_SPEC.md](../common/MODEL_IO_CONTRACT_SPEC.md) - å…¥åŠ›å¥‘ç´„å®šç¾©

---

**ä½œæˆæ—¥**: 2025-10-22
