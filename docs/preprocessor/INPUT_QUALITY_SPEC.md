# å…¥åŠ›å“è³ªç®¡ç†ä»•æ§˜ (INPUT_QUALITY_SPEC)

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**æ›´æ–°æ—¥**: 2025-10-22
**è²¬ä»»è€…**: core-team
**ã‚«ãƒ†ã‚´ãƒª**: å‰å‡¦ç†ã‚µãƒ–ä»•æ§˜

---

## ğŸ“‹ ç›®çš„

å‰å‡¦ç†æ®µéšã§ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ç®¡ç†ã—ã€æ¬ æãƒ»ã‚®ãƒ£ãƒƒãƒ—ãƒ»å“è³ªåŠ£åŒ–ã«å¯¾ã™ã‚‹é©åˆ‡ãªå‡¦ç†ã‚’å®šç¾©ã™ã‚‹ã€‚

**å¯¾è±¡é …ç›®**:
- å…¥åŠ›å“è³ªåŠ£åŒ–æ™‚ã®ä¿¡é ¼åº¦èª¿æ•´
- æ¬ æåˆ¤å®šã®åˆ—åˆ†é›¢
- é•·æœŸæ¬ æå¾Œã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†æ–­
- é€£ç¶šã‚®ãƒ£ãƒƒãƒ—é™¤å¤–åŸºæº–

---

## é …ç›®105å¯¾å¿œ: å…¥åŠ›å“è³ªåŠ£åŒ–è¨­è¨ˆ

**ç›®çš„**: é«˜é »åº¦æ¬ è½ãƒãƒ¼æ··å…¥æ™‚ã€æ®ç™ºæ€§æŒ‡æ¨™ï¼ˆRSI, ATRç­‰ï¼‰ã®èª¤å·®å¢—å¤§ â†’ èª¤ã‚·ã‚°ãƒŠãƒ«ç™ºç”Ÿ

**è§£æ±ºç­–**: å…¥åŠ›å“è³ªã‚¹ã‚³ã‚¢ã§ä¿¡é ¼åº¦ã‚’èª¿æ•´

```python
class InputQualityScorer:
    """å…¥åŠ›å“è³ªã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´"""
    
    def __init__(self, config: dict):
        self.max_gap_bars = config.get("max_gap_bars", 5)  # è¨±å®¹æ¬ è½ãƒãƒ¼æ•°
        self.gap_penalty = config.get("gap_penalty", 0.2)  # 1ãƒãƒ¼æ¬ è½ã§20%æ¸›
        self.min_quality_score = config.get("min_quality_score", 0.3)
    
    def calculate_quality_score(self, sequence_timestamps: np.ndarray,
                                expected_interval_seconds: int) -> float:
        """
        å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        
        Args:
            sequence_timestamps: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é…åˆ—
            expected_interval_seconds: æœŸå¾…é–“éš”ï¼ˆM1=60, M5=300, etc.ï¼‰
        
        Returns:
            quality_score: 0.0ï¼ˆæœ€æ‚ªï¼‰ï½ 1.0ï¼ˆå®Œç’§ï¼‰
        """
        # 1. æ™‚é–“é–“éš”å·®åˆ†è¨ˆç®—
        intervals = np.diff(sequence_timestamps)
        expected_interval = expected_interval_seconds
        
        # 2. æ¬ è½ãƒãƒ¼æ¤œå‡º
        gap_bars = (intervals - expected_interval) // expected_interval
        gap_bars = np.clip(gap_bars, 0, None)  # è² å€¤ã¯0ã«
        
        total_gaps = np.sum(gap_bars)
        max_single_gap = np.max(gap_bars) if len(gap_bars) > 0 else 0
        
        # 3. å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        quality_score = 1.0
        
        # ç·æ¬ è½ãƒšãƒŠãƒ«ãƒ†ã‚£
        quality_score -= total_gaps * self.gap_penalty
        
        # é€£ç¶šæ¬ è½ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆã‚ˆã‚Šé‡ã„ï¼‰
        if max_single_gap > self.max_gap_bars:
            quality_score -= (max_single_gap - self.max_gap_bars) * self.gap_penalty * 2
        
        # ä¸‹é™ã‚¯ãƒªãƒƒãƒ—
        quality_score = max(quality_score, self.min_quality_score)
        
        return quality_score
    
    def apply_confidence_scaling(self, predictions: dict, 
                                 quality_score: float) -> dict:
        """
        å“è³ªã‚¹ã‚³ã‚¢ã«å¿œã˜ã¦äºˆæ¸¬ä¿¡é ¼åº¦ã‚’èª¿æ•´
        
        Args:
            predictions: {
                "direction_probs": [UP, DOWN, NEUTRAL],
                "magnitude": float,
                "confidence": float
            }
            quality_score: å…¥åŠ›å“è³ªã‚¹ã‚³ã‚¢
        
        Returns:
            adjusted_predictions: ä¿¡é ¼åº¦èª¿æ•´å¾Œã®äºˆæ¸¬
        """
        adjusted = predictions.copy()
        
        # 1. ä¿¡é ¼åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        adjusted["confidence"] *= quality_score
        
        # 2. NEUTRALç¢ºç‡ã®å¢—åŠ ï¼ˆä½å“è³ªæ™‚ã¯æ…é‡ï¼‰
        if quality_score < 0.7:
            # NEUTRALæ¯”ç‡ã‚’å¢—åŠ ï¼ˆUP/DOWNã‚’æŠ‘åˆ¶ï¼‰
            neutral_boost = (1.0 - quality_score) * 0.3
            
            direction_probs = adjusted["direction_probs"]
            direction_probs[2] += neutral_boost  # NEUTRAL index=2
            
            # æ­£è¦åŒ–
            direction_probs = direction_probs / direction_probs.sum()
            adjusted["direction_probs"] = direction_probs
        
        # 3. ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰ç¸®å°ï¼ˆä½å“è³ªæ™‚ã¯ä¿å®ˆçš„ï¼‰
        adjusted["magnitude"] *= quality_score
        
        logger.debug(
            f"ä¿¡é ¼åº¦èª¿æ•´: quality={quality_score:.2f}, "
            f"confidence={predictions['confidence']:.2f} â†’ {adjusted['confidence']:.2f}"
        )
        
        return adjusted
    
    def log_quality_statistics(self, quality_scores: list):
        """å“è³ªã‚¹ã‚³ã‚¢çµ±è¨ˆãƒ­ã‚°å‡ºåŠ›"""
        stats = {
            "mean": np.mean(quality_scores),
            "std": np.std(quality_scores),
            "min": np.min(quality_scores),
            "p10": np.percentile(quality_scores, 10),
            "median": np.median(quality_scores),
        }
        
        logger.info(f"å…¥åŠ›å“è³ªçµ±è¨ˆ: {stats}")
        
        # è­¦å‘Šé–¾å€¤
        if stats["p10"] < 0.5:  # ä¸‹ä½10%ãŒ0.5æœªæº€
            logger.warning(
                f"å…¥åŠ›å“è³ªä½ä¸‹: p10={stats['p10']:.2f}, "
                f"ãƒ‡ãƒ¼ã‚¿åé›†ç¢ºèªæ¨å¥¨"
            )


# ä½¿ç”¨ä¾‹: æ¨è«–æ™‚ã®å“è³ªãƒã‚§ãƒƒã‚¯
quality_scorer = InputQualityScorer({
    "max_gap_bars": 5,
    "gap_penalty": 0.2,
    "min_quality_score": 0.3
})

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
sequence_timestamps = preprocessed_data["timestamps"][-480:]  # M1: ç›´è¿‘480æœ¬
quality_score = quality_scorer.calculate_quality_score(
    sequence_timestamps,
    expected_interval_seconds=60  # M1
)

# äºˆæ¸¬å®Ÿè¡Œ
predictions = model.predict(sequence_data)

# å“è³ªã‚¹ã‚³ã‚¢ã§ä¿¡é ¼åº¦èª¿æ•´
adjusted_predictions = quality_scorer.apply_confidence_scaling(
    predictions,
    quality_score
)

# èª¿æ•´å¾Œã®äºˆæ¸¬ã‚’ä½¿ç”¨
if adjusted_predictions["confidence"] < 0.5:
    logger.info("ä¿¡é ¼åº¦ä¸è¶³ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼è¦‹é€ã‚Š")
else:
    execute_signal(adjusted_predictions)
```

**å…¥åŠ›å“è³ªã‚¹ã‚³ã‚¢ä»•æ§˜**:
- **max_gap_bars**: 5æœ¬ï¼ˆè¨±å®¹æ¬ è½ãƒãƒ¼æ•°ï¼‰
- **gap_penalty**: 0.2ï¼ˆ1ãƒãƒ¼æ¬ è½ã§20%æ¸›ç‚¹ï¼‰
- **min_quality_score**: 0.3ï¼ˆæœ€ä½ã‚¹ã‚³ã‚¢ï¼‰
- **ä¿¡é ¼åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: confidence Ã— quality_score
- **NEUTRAL boost**: quality < 0.7 æ™‚ã« NEUTRAL ç¢ºç‡ +30%
- **æˆåŠŸæŒ‡æ¨™**: æ¬ è½æœŸ NetLoss æ¸›å°‘ >= 20%

**åŠ¹æœ**:
- æ¬ è½ãƒãƒ¼æ··å…¥æ™‚ã®èª¤ã‚·ã‚°ãƒŠãƒ«æŠ‘åˆ¶
- ä½å“è³ªå…¥åŠ›ã®è‡ªå‹•æ¤œå‡º
- æ¨è«–ä¿¡é ¼åº¦ã®é€æ˜æ€§å‘ä¸Š

---


---

## é …ç›®44å¯¾å¿œ: æ¬ æåˆ¤å®šã®åˆ—åˆ†é›¢

**ç›®çš„**: `df.isna().any(axis=1)` ã¯ä¸€éƒ¨åˆ—æ¬ æã§è¡Œå…¨ä½“ã‚’æ¬ ææ‰±ã„ â†’ éå‰°ãªforward fillç™ºç”Ÿ

**è§£æ±ºç­–**: primary/auxiliaryåˆ—ã‚’åˆ†é›¢ã—ã€primaryåˆ—æ¬ æã®ã¿fillå¯¾è±¡

```python
class MissingDataClassifier:
    """æ¬ æåˆ¤å®šã®åˆ—åˆ†é›¢"""
    
    def __init__(self, config: dict):
        # ä¸»è¦åˆ—: OHLCV + ä¸»è¦ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        self.primary_columns = config.get("primary_columns", [
            "open", "high", "low", "close", "volume",
            "rsi_14", "macd_line", "atr_14"
        ])
        
        # è£œåŠ©åˆ—: ãã®ä»–ã®æ´¾ç”Ÿç‰¹å¾´é‡
        self.auxiliary_columns = None  # åˆæœŸåŒ–æ™‚ã«è‡ªå‹•åˆ¤å®š
        
        self.primary_missing_threshold = config.get("primary_missing_threshold", 0.05)
        self.auxiliary_missing_threshold = config.get("auxiliary_missing_threshold", 0.20)
    
    def classify_missing(
        self,
        df: pd.DataFrame
    ) -> Dict[str, Any]:
        """
        æ¬ æã‚’ä¸»è¦/è£œåŠ©åˆ—ã§åˆ†é¡åˆ¤å®š
        
        Args:
            df: ç‰¹å¾´é‡DataFrame
        
        Returns:
            {
                "primary_missing_mask": np.ndarray,  # ä¸»è¦åˆ—æ¬ æè¡Œ
                "auxiliary_missing_mask": np.ndarray,  # è£œåŠ©åˆ—ã®ã¿æ¬ æè¡Œ
                "complete_mask": np.ndarray,  # å®Œå…¨ãƒ‡ãƒ¼ã‚¿è¡Œ
                "statistics": dict
            }
        """
        # è£œåŠ©åˆ—ã‚’è‡ªå‹•åˆ¤å®šï¼ˆä¸»è¦åˆ—ä»¥å¤–ï¼‰
        if self.auxiliary_columns is None:
            self.auxiliary_columns = [
                col for col in df.columns 
                if col not in self.primary_columns
            ]
        
        # ä¸»è¦åˆ—æ¬ æåˆ¤å®š
        primary_df = df[self.primary_columns]
        primary_missing_mask = primary_df.isna().any(axis=1).values
        
        # è£œåŠ©åˆ—æ¬ æåˆ¤å®š
        if len(self.auxiliary_columns) > 0:
            auxiliary_df = df[self.auxiliary_columns]
            auxiliary_missing_mask = auxiliary_df.isna().any(axis=1).values
        else:
            auxiliary_missing_mask = np.zeros(len(df), dtype=bool)
        
        # å®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸¡æ–¹æ¬ æãªã—ï¼‰
        complete_mask = ~(primary_missing_mask | auxiliary_missing_mask)
        
        # è£œåŠ©åˆ—ã®ã¿æ¬ æï¼ˆä¸»è¦åˆ—ã¯å®Œå…¨ï¼‰
        auxiliary_only_missing = auxiliary_missing_mask & ~primary_missing_mask
        
        # çµ±è¨ˆè¨ˆç®—
        total_rows = len(df)
        stats = {
            "total_rows": total_rows,
            "primary_missing_count": int(primary_missing_mask.sum()),
            "primary_missing_rate": float(primary_missing_mask.mean()),
            "auxiliary_only_missing_count": int(auxiliary_only_missing.sum()),
            "auxiliary_only_missing_rate": float(auxiliary_only_missing.mean()),
            "complete_count": int(complete_mask.sum()),
            "complete_rate": float(complete_mask.mean())
        }
        
        return {
            "primary_missing_mask": primary_missing_mask,
            "auxiliary_missing_mask": auxiliary_missing_mask,
            "auxiliary_only_missing": auxiliary_only_missing,
            "complete_mask": complete_mask,
            "statistics": stats
        }
    
    def apply_selective_fill(
        self,
        df: pd.DataFrame
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        é¸æŠçš„forward fillé©ç”¨
        
        - ä¸»è¦åˆ—æ¬ æ: forward fillé©ç”¨
        - è£œåŠ©åˆ—ã®ã¿æ¬ æ: æ¬ æã®ã¾ã¾ï¼ˆå­¦ç¿’æ™‚ãƒã‚¹ã‚¯å¯¾è±¡ï¼‰
        
        Args:
            df: ç‰¹å¾´é‡DataFrame
        
        Returns:
            (filled_df, fill_info)
        """
        missing_info = self.classify_missing(df)
        
        df_filled = df.copy()
        
        # ä¸»è¦åˆ—ã®ã¿forward fill
        primary_mask = missing_info["primary_missing_mask"]
        if primary_mask.sum() > 0:
            df_filled[self.primary_columns] = df_filled[self.primary_columns].fillna(method='ffill')
            logger.info(
                f"ä¸»è¦åˆ—forward fillé©ç”¨: {primary_mask.sum()}è¡Œ "
                f"({missing_info['statistics']['primary_missing_rate']:.2%})"
            )
        
        # è£œåŠ©åˆ—ã¯æ¬ æã®ã¾ã¾ä¿æŒ
        aux_only_mask = missing_info["auxiliary_only_missing"]
        if aux_only_mask.sum() > 0:
            logger.debug(
                f"è£œåŠ©åˆ—æ¬ æä¿æŒï¼ˆfillä¸è¦ï¼‰: {aux_only_mask.sum()}è¡Œ "
                f"({missing_info['statistics']['auxiliary_only_missing_rate']:.2%})"
            )
        
        fill_info = {
            "filled_rows": int(primary_mask.sum()),
            "unfilled_auxiliary_rows": int(aux_only_mask.sum()),
            "statistics": missing_info["statistics"]
        }
        
        return df_filled, fill_info


# ä½¿ç”¨ä¾‹: å‰å‡¦ç†æ™‚
classifier = MissingDataClassifier({
    "primary_columns": ["open", "high", "low", "close", "volume", "rsi_14", "macd_line"],
    "primary_missing_threshold": 0.05,
    "auxiliary_missing_threshold": 0.20
})

# æ¬ æåˆ†é¡
missing_info = classifier.classify_missing(features_df)
logger.info(f"æ¬ æçµ±è¨ˆ: {missing_info['statistics']}")

# é¸æŠçš„fill
filled_df, fill_info = classifier.apply_selective_fill(features_df)

# é–¾å€¤ãƒã‚§ãƒƒã‚¯
if missing_info["statistics"]["primary_missing_rate"] > 0.05:
    logger.warning(
        f"ä¸»è¦åˆ—æ¬ æç‡éå¤§: {missing_info['statistics']['primary_missing_rate']:.2%} > 5%"
    )
```

**KPIï¼ˆé …ç›®44ï¼‰**:
- éå‰°fillå‰Šæ¸›ç‡: â‰¥50%ï¼ˆè£œåŠ©åˆ—æ¬ æã®fillå›é¿ï¼‰
- ä¸»è¦åˆ—æ¬ æç‡: <5%
- å­¦ç¿’ãƒã‚¤ã‚¢ã‚¹è»½æ¸›: è£œåŠ©åˆ—æ¬ æã®å¹³æ»‘åŒ–ãªã—

---


---

## é …ç›®45å¯¾å¿œ: é•·æœŸæ¬ æå¾Œã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†æ–­

**ç›®çš„**: å¤§ããªæ™‚é–“ã‚®ãƒ£ãƒƒãƒ—å¾Œã«ç›´ç¶šã•ã›ã‚‹ã¨ã€Œç–‘ä¼¼é«˜é€Ÿå¤‰åŒ–ã€ã‚’èª¤å­¦ç¿’

**è§£æ±ºç­–**: `gap_duration > Ï„` ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†æ–­ + å…ˆé ­warmupãƒãƒ¼ãƒã‚¹ã‚¯

```python
class LongGapSequenceSplitter:
    """é•·æœŸæ¬ æå¾Œã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†æ–­"""
    
    def __init__(self, config: dict):
        self.max_gap_duration_minutes = config.get("max_gap_duration_minutes", {
            "M1": 5,    # M1ã§5åˆ†ä»¥ä¸Šç©ºã„ãŸã‚‰åˆ†æ–­
            "M5": 15,   # M5ã§15åˆ†ä»¥ä¸Š
            "M15": 45,
            "H1": 120,
            "H4": 480
        })
        
        self.warmup_bars_after_gap = config.get("warmup_bars_after_gap", 20)
    
    def detect_long_gaps(
        self,
        timestamps: np.ndarray,
        tf_name: str
    ) -> List[int]:
        """
        é•·æœŸã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º
        
        Args:
            timestamps: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é…åˆ—ï¼ˆUnixç§’ï¼‰
            tf_name: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
        
        Returns:
            gap_indices: ã‚®ãƒ£ãƒƒãƒ—ç›´å¾Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
        """
        if len(timestamps) < 2:
            return []
        
        # æ™‚é–“å·®åˆ†ï¼ˆç§’ï¼‰
        time_diffs = np.diff(timestamps)
        
        # æœŸå¾…é–“éš”ï¼ˆç§’ï¼‰
        expected_interval = {
            "M1": 60,
            "M5": 300,
            "M15": 900,
            "H1": 3600,
            "H4": 14400
        }[tf_name]
        
        # é–¾å€¤ï¼ˆåˆ†ï¼‰â†’ç§’
        max_gap_seconds = self.max_gap_duration_minutes[tf_name] * 60
        
        # é•·æœŸã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º
        long_gap_mask = time_diffs > max_gap_seconds
        gap_indices = np.where(long_gap_mask)[0] + 1  # ã‚®ãƒ£ãƒƒãƒ—ç›´å¾Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        
        return gap_indices.tolist()
    
    def split_sequences_at_gaps(
        self,
        data: np.ndarray,
        timestamps: np.ndarray,
        tf_name: str,
        window_size: int
    ) -> List[Dict[str, Any]]:
        """
        ã‚®ãƒ£ãƒƒãƒ—ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆ†æ–­
        
        Args:
            data: ç‰¹å¾´é‡é…åˆ— (N, F)
            timestamps: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é…åˆ— (N,)
            tf_name: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
            window_size: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
        
        Returns:
            sequences: åˆ†æ–­ã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒªã‚¹ãƒˆ
        """
        gap_indices = self.detect_long_gaps(timestamps, tf_name)
        
        if len(gap_indices) == 0:
            # ã‚®ãƒ£ãƒƒãƒ—ãªã— â†’ é€šå¸¸ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–
            return self._create_normal_sequences(data, timestamps, window_size)
        
        # ã‚®ãƒ£ãƒƒãƒ—ã§åˆ†æ–­
        segments = []
        start_idx = 0
        
        for gap_idx in gap_indices:
            # ã‚®ãƒ£ãƒƒãƒ—å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
            if gap_idx - start_idx >= window_size:
                segment = {
                    "data": data[start_idx:gap_idx],
                    "timestamps": timestamps[start_idx:gap_idx],
                    "start_idx": start_idx,
                    "end_idx": gap_idx,
                    "has_gap_before": False
                }
                segments.append(segment)
            
            # ã‚®ãƒ£ãƒƒãƒ—å¾Œã¯å…ˆé ­warmupãƒãƒ¼ã‚’ãƒã‚¹ã‚¯
            start_idx = gap_idx
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        if len(data) - start_idx >= window_size:
            segment = {
                "data": data[start_idx:],
                "timestamps": timestamps[start_idx:],
                "start_idx": start_idx,
                "end_idx": len(data),
                "has_gap_before": True if start_idx > 0 else False
            }
            segments.append(segment)
        
        # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ
        all_sequences = []
        for seg in segments:
            seqs = self._create_sequences_from_segment(
                seg, window_size
            )
            all_sequences.extend(seqs)
        
        logger.info(
            f"{tf_name}: é•·æœŸã‚®ãƒ£ãƒƒãƒ— {len(gap_indices)}ç®‡æ‰€æ¤œå‡ºã€"
            f"{len(segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†æ–­ã€"
            f"{len(all_sequences)}ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ"
        )
        
        return all_sequences
    
    def _create_sequences_from_segment(
        self,
        segment: Dict[str, Any],
        window_size: int
    ) -> List[Dict[str, Any]]:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ"""
        data = segment["data"]
        timestamps = segment["timestamps"]
        N = len(data)
        
        sequences = []
        
        for i in range(N - window_size):
            seq_data = data[i:i+window_size]
            seq_timestamps = timestamps[i:i+window_size]
            
            # warmupãƒã‚¹ã‚¯ç”Ÿæˆ
            warmup_mask = np.ones(window_size, dtype=float)
            
            if segment["has_gap_before"] and i < self.warmup_bars_after_gap:
                # ã‚®ãƒ£ãƒƒãƒ—ç›´å¾Œã®å…ˆé ­warmupæœŸé–“ã¯ãƒã‚¹ã‚¯
                warmup_bars_in_window = min(
                    self.warmup_bars_after_gap - i,
                    window_size
                )
                warmup_mask[:warmup_bars_in_window] = 0.0
            
            sequences.append({
                "data": seq_data,
                "timestamps": seq_timestamps,
                "mask": warmup_mask,
                "segment_id": segment["start_idx"]
            })
        
        return sequences


# ä½¿ç”¨ä¾‹: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆæ™‚
splitter = LongGapSequenceSplitter({
    "max_gap_duration_minutes": {"M1": 5, "M5": 15, "H1": 120},
    "warmup_bars_after_gap": 20
})

# M5ãƒ‡ãƒ¼ã‚¿ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ
sequences = splitter.split_sequences_at_gaps(
    data=features_m5.values,
    timestamps=timestamps_m5,
    tf_name="M5",
    window_size=288
)

# warmupãƒã‚¹ã‚¯ã‚’å­¦ç¿’é‡ã¿ã«åæ˜ 
for seq in sequences:
    loss_weight = seq["mask"]  # 0.0=warmupé™¤å¤–, 1.0=é€šå¸¸
```

**KPIï¼ˆé …ç›®45ï¼‰**:
- ã‚®ãƒ£ãƒƒãƒ—åˆ†æ–­ç‡: è¨˜éŒ²ï¼ˆå…¸å‹çš„ã«ã¯<2%ï¼‰
- warmupæœŸé–“: ã‚®ãƒ£ãƒƒãƒ—å¾Œ20ãƒãƒ¼
- ç–‘ä¼¼é«˜é€Ÿå¤‰åŒ–èª¤å­¦ç¿’å‰Šæ¸›: å®šæ€§è©•ä¾¡

---


---

## é …ç›®106å¯¾å¿œ: é€£ç¶šã‚®ãƒ£ãƒƒãƒ—é™¤å¤–åŸºæº–

**ç›®çš„**: å¤šæ•°ã®é€£ç¶šã‚®ãƒ£ãƒƒãƒ—ã¯ç•°å¸¸åˆ†å¸ƒã‚’æ³¨å…¥ã—å­¦ç¿’ã‚’æ­ªã‚ã‚‹

**è§£æ±ºç­–**: `consecutive_gaps > K` ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é™¤å¤–

```python
class ConsecutiveGapFilter:
    """é€£ç¶šã‚®ãƒ£ãƒƒãƒ—ã«ã‚ˆã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é™¤å¤–"""
    
    def __init__(self, config: dict):
        self.max_consecutive_gaps = config.get("max_consecutive_gaps", 3)
        self.gap_detection_window = config.get("gap_detection_window", 10)
    
    def detect_consecutive_gaps(
        self,
        timestamps: np.ndarray,
        tf_name: str
    ) -> np.ndarray:
        """
        é€£ç¶šã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º
        
        Args:
            timestamps: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é…åˆ—
            tf_name: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
        
        Returns:
            consecutive_gap_mask: True=é€£ç¶šã‚®ãƒ£ãƒƒãƒ—å¤šæ•°ã€False=æ­£å¸¸
        """
        if len(timestamps) < 2:
            return np.zeros(len(timestamps), dtype=bool)
        
        # æœŸå¾…é–“éš”ï¼ˆç§’ï¼‰
        expected_interval = {
            "M1": 60,
            "M5": 300,
            "M15": 900,
            "H1": 3600,
            "H4": 14400
        }[tf_name]
        
        # æ™‚é–“å·®åˆ†
        time_diffs = np.diff(timestamps)
        
        # ã‚®ãƒ£ãƒƒãƒ—åˆ¤å®šï¼ˆæœŸå¾…é–“éš”ã®1.5å€ä»¥ä¸Šï¼‰
        is_gap = time_diffs > (expected_interval * 1.5)
        
        # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§é€£ç¶šã‚®ãƒ£ãƒƒãƒ—ã‚«ã‚¦ãƒ³ãƒˆ
        N = len(timestamps)
        consecutive_gap_mask = np.zeros(N, dtype=bool)
        
        for i in range(N - self.gap_detection_window):
            window_gaps = is_gap[i:i+self.gap_detection_window]
            gap_count = window_gaps.sum()
            
            if gap_count > self.max_consecutive_gaps:
                # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…å…¨ã¦ã‚’ãƒã‚¹ã‚¯
                consecutive_gap_mask[i:i+self.gap_detection_window+1] = True
        
        return consecutive_gap_mask
    
    def filter_sequences_by_gap_density(
        self,
        sequences: List[Dict[str, Any]],
        tf_name: str
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        é€£ç¶šã‚®ãƒ£ãƒƒãƒ—å¯†åº¦ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é™¤å¤–
        
        Args:
            sequences: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒªã‚¹ãƒˆï¼ˆå„è¦ç´ ã«"timestamps"å«ã‚€ï¼‰
            tf_name: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ å
        
        Returns:
            (filtered_sequences, filter_stats)
        """
        filtered_sequences = []
        discarded_count = 0
        
        for seq in sequences:
            timestamps = seq["timestamps"]
            
            # é€£ç¶šã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º
            gap_mask = self.detect_consecutive_gaps(timestamps, tf_name)
            
            # ã‚®ãƒ£ãƒƒãƒ—æ¯”ç‡è¨ˆç®—
            gap_ratio = gap_mask.sum() / len(gap_mask)
            
            if gap_ratio > 0.3:  # 30%ä»¥ä¸ŠãŒã‚®ãƒ£ãƒƒãƒ—å½±éŸ¿
                # é™¤å¤–
                discarded_count += 1
                logger.debug(
                    f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é™¤å¤–: ã‚®ãƒ£ãƒƒãƒ—æ¯”ç‡={gap_ratio:.2%} > 30%"
                )
            else:
                # ä¿æŒï¼ˆã‚®ãƒ£ãƒƒãƒ—ãƒã‚¹ã‚¯ã‚’ä»˜ä¸ï¼‰
                seq["gap_mask"] = gap_mask
                seq["gap_ratio"] = gap_ratio
                filtered_sequences.append(seq)
        
        filter_stats = {
            "total_sequences": len(sequences),
            "filtered_sequences": len(filtered_sequences),
            "discarded_sequences": discarded_count,
            "discard_rate": discarded_count / len(sequences) if len(sequences) > 0 else 0
        }
        
        logger.info(
            f"{tf_name} ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ•ã‚£ãƒ«ã‚¿: "
            f"{len(filtered_sequences)}/{len(sequences)}ä¿æŒã€"
            f"é™¤å¤–ç‡={filter_stats['discard_rate']:.2%}"
        )
        
        return filtered_sequences, filter_stats


# ä½¿ç”¨ä¾‹: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆå¾Œ
gap_filter = ConsecutiveGapFilter({
    "max_consecutive_gaps": 3,
    "gap_detection_window": 10
})

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰é€£ç¶šã‚®ãƒ£ãƒƒãƒ—å¤šæ•°ã®ã‚‚ã®ã‚’é™¤å¤–
filtered_sequences, stats = gap_filter.filter_sequences_by_gap_density(
    sequences=all_sequences,
    tf_name="M5"
)

# é™¤å¤–ç‡ãƒã‚§ãƒƒã‚¯
if stats["discard_rate"] > 0.05:
    logger.warning(
        f"é€£ç¶šã‚®ãƒ£ãƒƒãƒ—ã«ã‚ˆã‚‹é™¤å¤–ç‡éå¤§: {stats['discard_rate']:.2%} > 5%ã€‚"
        f"ãƒ‡ãƒ¼ã‚¿åé›†æœŸé–“ã¾ãŸã¯max_consecutive_gapsé–¾å€¤ã®è¦‹ç›´ã—æ¨å¥¨"
    )
```

**KPIï¼ˆé …ç›®106ï¼‰**:
- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é™¤å¤–ç‡: <5%
- ã‚®ãƒ£ãƒƒãƒ—å¯†åº¦é–¾å€¤: 30%
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š: ç•°å¸¸åˆ†å¸ƒæ³¨å…¥ç‡å‰Šæ¸›

---



---

## ğŸ”— é–¢é€£ä»•æ§˜

- [PREPROCESSOR_SPEC.md](../PREPROCESSOR_SPEC.md) - å‰å‡¦ç†ãƒ¡ã‚¤ãƒ³ä»•æ§˜
- [DATA_COLLECTOR_SPEC.md](../DATA_COLLECTOR_SPEC.md) - ãƒ‡ãƒ¼ã‚¿åé›†ä»•æ§˜

---

**ä½œæˆæ—¥**: 2025-10-22
