# STORAGE_POLICY_SPEC.md

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**æ›´æ–°æ—¥**: 2025-10-21  
**è²¬ä»»è€…**: core-team

---

## ğŸ“‹ ç›®çš„

å­¦ç¿’ç”¨ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’åˆ†é›¢ã—ã€ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°æˆ¦ç•¥ã«æœ€é©åŒ–ã•ã‚ŒãŸä¿æŒæœŸé–“ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å®Ÿç¾ã™ã‚‹ã€‚

---

## ğŸ¯ è¨­è¨ˆåŸå‰‡

### å•é¡Œèªè­˜

```
çŸ›ç›¾:
- å­¦ç¿’: é•·æœŸå±¥æ­´ãŒå¿…è¦ï¼ˆå­£ç¯€æ€§ã€å¤šæ§˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ï¼‰
- æ¨è«–: ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ã®ã¿å¿…è¦ï¼ˆ24æ™‚é–“ä»¥å†…ã®ä¿æœ‰æˆ¦ç•¥ï¼‰

å¾“æ¥ã®1å¹´ä¿æŒ:
âŒ ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ã«éå‰°ï¼ˆ1æ™‚é–“ä¿æœ‰ã«1å¹´åˆ†ä¸è¦ï¼‰
âŒ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è‚¥å¤§åŒ–ï¼ˆTickã¯å·¨å¤§ï¼‰
âŒ I/Oé…å»¶ï¼ˆæ¨è«–æ™‚ã«ä¸è¦ãªå¤ã„ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿ï¼‰
```

### è§£æ±ºæ–¹é‡

**2ç³»çµ±ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†**:
1. **Training Storage**: é•·æœŸä¿æŒã€ãƒãƒƒãƒæ›´æ–°ã€åœ§ç¸®æœ‰åŠ¹
2. **Inference Storage**: çŸ­æœŸä¿æŒã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã€åœ§ç¸®ç„¡åŠ¹ï¼ˆé€Ÿåº¦å„ªå…ˆï¼‰

---

## ğŸ“¦ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆ†é¡

### 1. Training Storageï¼ˆå­¦ç¿’ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ï¼‰

```yaml
training_storage:
  # ä¿æŒæœŸé–“
  retention:
    tick_data: 3 months      # å››åŠæœŸåˆ†ï¼ˆå­£ç¯€æ€§ã‚«ãƒãƒ¼ï¼‰
    bar_data: 6 months       # M1/M5/M15/H1/H4
    reason: "3ãƒ¶æœˆã§å¤šæ§˜ãªå¸‚å ´ç’°å¢ƒï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸/é«˜ä½ãƒœãƒ©ï¼‰ã‚’å­¦ç¿’å¯èƒ½"
  
  # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
  file_naming: "fx_train_{symbol}_{YYYYMMDD}_{YYYYMMDD}.h5"
  example: "fx_train_USDJPY_20250101_20250331.h5"  # 3ãƒ¶æœˆåˆ†
  
  # åœ§ç¸®è¨­å®š
  compression:
    enabled: true
    algorithm: "gzip"
    level: 4                  # åœ§ç¸®ç‡ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹
    reason: "é•·æœŸä¿å­˜ã§ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å‰Šæ¸›å„ªå…ˆ"
  
  # æ›´æ–°é »åº¦
  update_frequency: daily     # æ—¥æ¬¡ã§æ–°è¦ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
  update_time: "06:00 UTC"    # æ±äº¬å¸‚å ´é–‹å§‹å‰
  
  # ç”¨é€”
  purposes:
    - model_training          # LSTMå­¦ç¿’
    - hyperparameter_tuning   # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
    - backtesting            # å±¥æ­´æ¤œè¨¼
    - validation             # ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆè©•ä¾¡
    - feature_engineering    # æ–°ç‰¹å¾´é‡é–‹ç™º
  
  # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
  rotation:
    interval: monthly
    archive_after: 6 months   # 6ãƒ¶æœˆè¶…ã¯åˆ¥ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¸
    archive_format: "tar.gz"
    deletion_after: 2 years   # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚‚2å¹´ã§å‰Šé™¤
```

### 2. Inference Storageï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ç”¨ï¼‰

```yaml
inference_storage:
  # ä¿æŒæœŸé–“
  retention:
    tick_data: 24 hours       # ç›´è¿‘1æ—¥åˆ†ã®ã¿
    bar_data: 48 hours        # M1/M5ã¯24hã€H1/H4ã¯48h
    reason: "ã‚¹ã‚­ãƒ£ãƒ«ãƒ—/ã‚¹ã‚¤ãƒ³ã‚°ã¯æ•°æ™‚é–“ä¿æœ‰ã€24-48håˆ†ã§ååˆ†"
  
  # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
  file_naming: "fx_live_{symbol}_latest.h5"
  example: "fx_live_USDJPY_latest.h5"  # å¸¸ã«æœ€æ–°24håˆ†
  
  # åœ§ç¸®è¨­å®š
  compression:
    enabled: false            # é€Ÿåº¦å„ªå…ˆ
    reason: "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é‡è¦–ï¼ˆåœ§ç¸®å±•é–‹ã‚³ã‚¹ãƒˆå›é¿ï¼‰"
  
  # æ›´æ–°é »åº¦
  update_frequency: real_time  # Tickå—ä¿¡ã”ã¨
  write_mode: "append"         # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰
  
  # ãƒ¡ãƒ¢ãƒªè¨­è¨ˆ
  memory_structure:
    type: "ring_buffer"        # å›ºå®šã‚µã‚¤ã‚ºã®å¾ªç’°ãƒãƒƒãƒ•ã‚¡
    max_size: 
      M1: 1440 bars            # 24æ™‚é–“
      M5: 288 bars             # 24æ™‚é–“
      tick: 100000 records     # ç´„24æ™‚é–“åˆ†ï¼ˆæµå‹•æ€§ã«ã‚ˆã‚‹ï¼‰
    overflow_behavior: "overwrite_oldest"  # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ã
  
  # ç”¨é€”
  purposes:
    - real_time_prediction    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–
    - pattern_detection       # ç›´è¿‘ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
    - entry_signal_generation # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚·ã‚°ãƒŠãƒ«
    - position_monitoring     # ãƒã‚¸ã‚·ãƒ§ãƒ³ç®¡ç†
  
  # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  auto_cleanup:
    enabled: true
    check_interval: "1 hour"
    delete_older_than: "24 hours"  # 24æ™‚é–“è¶…ã®ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
```

---

## ğŸ—ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

### Training Storageéšå±¤

```
data/training/
â”œâ”€â”€ 2025Q1/
â”‚   â”œâ”€â”€ fx_train_USDJPY_20250101_20250331.h5  # 3ãƒ¶æœˆåˆ†ï¼ˆç¾åœ¨ä½¿ç”¨ä¸­ï¼‰
â”‚   â”œâ”€â”€ fx_train_EURUSD_20250101_20250331.h5
â”‚   â””â”€â”€ metadata_2025Q1.json
â”œâ”€â”€ 2024Q4/
â”‚   â”œâ”€â”€ fx_train_USDJPY_20241001_20241231.h5  # éå»å››åŠæœŸï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å€™è£œï¼‰
â”‚   â””â”€â”€ metadata_2024Q4.json
â””â”€â”€ archive/
    â”œâ”€â”€ 2024Q3.tar.gz                          # 6ãƒ¶æœˆè¶…ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
    â””â”€â”€ 2024Q2.tar.gz
```

### Inference Storageéšå±¤

```
data/inference/
â”œâ”€â”€ fx_live_USDJPY_latest.h5     # å¸¸ã«æœ€æ–°24æ™‚é–“ï¼ˆãƒªãƒ³ã‚° ãƒãƒƒãƒ•ã‚¡ï¼‰
â”œâ”€â”€ fx_live_EURUSD_latest.h5
â””â”€â”€ metadata_live.json           # æœ€çµ‚æ›´æ–°æ™‚åˆ»ã€ãƒ‡ãƒ¼ã‚¿ç¯„å›²
```

---

## ğŸ“Š HDF5å†…éƒ¨æ§‹é€ 

### Training Storage

```python
# fx_train_USDJPY_20250101_20250331.h5

/tick                          # Tickãƒ‡ãƒ¼ã‚¿ï¼ˆåœ§ç¸®æœ‰åŠ¹ï¼‰
  â””â”€â”€ data: [N, 6]             # [time, bid, ask, volume, flags, spread]
      compression: gzip level 4
      chunks: (10000, 6)        # 10kè¡Œå˜ä½

/M1, /M5, /M15, /H1, /H4       # ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿
  â””â”€â”€ data: [N, features]
      compression: gzip level 4
      chunks: (1000, features)

/metadata
  â””â”€â”€ attrs: {
       "start_date": "2025-01-01T00:00:00",
       "end_date": "2025-03-31T23:59:59",
       "total_bars_M1": 129600,
       "total_ticks": 8500000,
       "purpose": "training",
       "git_commit": "abc123",
     }
```

### Inference Storageï¼ˆRing Bufferè¨­è¨ˆï¼‰

```python
# fx_live_USDJPY_latest.h5

/tick                          # åœ§ç¸®ç„¡åŠ¹ï¼ˆé€Ÿåº¦å„ªå…ˆï¼‰
  â””â”€â”€ data: [100000, 6]        # å›ºå®šã‚µã‚¤ã‚ºï¼ˆRing Bufferï¼‰
      compression: none
      fillvalue: NaN            # æœªä½¿ç”¨ã‚¹ãƒ­ãƒƒãƒˆ
      attrs: {
        "write_index": 45123,   # ç¾åœ¨ã®æ›¸ãè¾¼ã¿ä½ç½®
        "wrap_count": 2,        # ãƒ©ãƒƒãƒ—ã‚¢ãƒ©ã‚¦ãƒ³ãƒ‰å›æ•°
        "oldest_time": "2025-10-20T12:00:00",
        "newest_time": "2025-10-21T11:59:59",
      }

/M1                            # M1ãƒãƒ¼ï¼ˆ24æ™‚é–“ = 1440æœ¬ï¼‰
  â””â”€â”€ data: [1440, features]   # å›ºå®šã‚µã‚¤ã‚º
      compression: none
      attrs: {
        "write_index": 823,
        "oldest_bar": "2025-10-20T12:00:00",
      }

/M5, /M15, /H1, /H4            # åŒæ§˜ã®æ§‹é€ 
```

**Ring Bufferã®å‹•ä½œï¼ˆé …ç›®72å¯¾å¿œ: wrapå¢ƒç•Œå†åˆæœŸåŒ–ï¼‰**:
```python
def append_tick(h5_file, new_tick):
    """Tickã‚’ring bufferã«è¿½è¨˜ï¼ˆwrapå¢ƒç•Œã§å·®åˆ†ç‰¹å¾´é‡ãƒªã‚»ãƒƒãƒˆï¼‰"""
    tick_dataset = h5_file["/tick/data"]
    write_idx = tick_dataset.attrs["write_index"]
    max_size = tick_dataset.shape[0]
    wrap_count_before = tick_dataset.attrs.get("wrap_count", 0)
    
    # å¾ªç’°æ›¸ãè¾¼ã¿
    tick_dataset[write_idx % max_size] = new_tick
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
    tick_dataset.attrs["write_index"] = write_idx + 1
    
    # wrapæ¤œå‡º
    if write_idx > 0 and (write_idx % max_size) == 0:
        wrap_count_new = wrap_count_before + 1
        tick_dataset.attrs["wrap_count"] = wrap_count_new
        
        logger.info(f"Ring Buffer wrapæ¤œå‡º: wrap_count={wrap_count_new}, "
                   f"oldest_index={write_idx % max_size}")
        
        # å·®åˆ†ç‰¹å¾´é‡ã®å†åˆæœŸåŒ–ãƒ•ãƒƒã‚¯
        _reinitialize_differential_features(h5_file, timeframe="tick")


def _reinitialize_differential_features(h5_file, timeframe: str):
    """
    Ring Buffer wrapæ™‚ã®å·®åˆ†ç‰¹å¾´é‡å†åˆæœŸåŒ–
    
    å¯¾è±¡ç‰¹å¾´é‡:
    - ATRï¼ˆAverage True Rangeï¼‰: ç§»å‹•å¹³å‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒwrapå¢ƒç•Œã‚’è·¨ã
    - EMAï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰: éå»å€¤ä¾å­˜ã§ä¸é€£ç¶š
    - ãã®ä»–ã®å·®åˆ†ç³»ç‰¹å¾´é‡ï¼ˆmomentum, ROCç­‰ï¼‰
    
    å†åˆæœŸåŒ–æ‰‹é †:
    1. wrapå¢ƒç•Œã®å‰å¾ŒNæœ¬ï¼ˆä¾‹: 100æœ¬ï¼‰ã‚’èª­ã¿è¾¼ã¿
    2. å·®åˆ†ç‰¹å¾´é‡ã‚’å†è¨ˆç®—
    3. çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆwrapå‰å¾Œã®å·®åˆ†ï¼‰
    """
    logger.info(f"å·®åˆ†ç‰¹å¾´é‡å†åˆæœŸåŒ–é–‹å§‹: {timeframe}")
    
    # wrapå¢ƒç•Œä»˜è¿‘ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå‰100æœ¬ã€å¾Œ100æœ¬ï¼‰
    tick_data = h5_file[f"/{timeframe}/data"]
    write_idx = tick_data.attrs["write_index"]
    max_size = tick_data.shape[0]
    current_pos = (write_idx - 1) % max_size
    
    # å†è¨ˆç®—å¯¾è±¡ç¯„å›²ï¼ˆwrapå‰100æœ¬ + wrapå¾Œ100æœ¬ï¼‰
    window_size = 100
    start_idx = max(0, current_pos - window_size)
    end_idx = min(max_size, current_pos + window_size)
    
    # å·®åˆ†ç‰¹å¾´é‡ã®å†è¨ˆç®—ï¼ˆä¾‹: ATRï¼‰
    # â€»å®Ÿéš›ã®å®Ÿè£…ã§ã¯ç‰¹å¾´é‡è¨ˆç®—å™¨ã‚’å‘¼ã³å‡ºã—
    high = tick_data[start_idx:end_idx, 2]  # Highä¾¡æ ¼
    low = tick_data[start_idx:end_idx, 3]   # Lowä¾¡æ ¼
    close = tick_data[start_idx:end_idx, 4] # Closeä¾¡æ ¼
    
    # ATRå†è¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    true_range = np.maximum(high - low, np.abs(high - np.roll(close, 1)))
    atr_recalc = np.mean(true_range[-14:])  # 14æœŸé–“ATR
    
    # çµ±è¨ˆãƒ­ã‚°ï¼ˆwrapå‰å¾Œã®å·®åˆ†ç¢ºèªï¼‰
    if f"/{timeframe}/features/atr" in h5_file:
        atr_old = h5_file[f"/{timeframe}/features/atr"][current_pos]
        logger.info(f"ATRå†è¨ˆç®—: old={atr_old:.6f}, new={atr_recalc:.6f}, "
                   f"delta={abs(atr_recalc - atr_old):.6f}")
        
        # é–¾å€¤ãƒã‚§ãƒƒã‚¯ï¼ˆå·®åˆ†ãŒå¤§ãã„å ´åˆã¯è­¦å‘Šï¼‰
        if abs(atr_recalc - atr_old) > atr_old * 0.1:  # 10%ä»¥ä¸Šå·®åˆ†
            logger.warning(f"ATR wrapå¢ƒç•Œã§å¤§ããªé£›ã³æ¤œå‡º: {abs(atr_recalc - atr_old):.6f}")
        
        # å†è¨ˆç®—å€¤ã§æ›´æ–°
        h5_file[f"/{timeframe}/features/atr"][current_pos] = atr_recalc
    
    # EMAã€momentumç­‰ã®ä»–ã®å·®åˆ†ç‰¹å¾´é‡ã‚‚åŒæ§˜ã«å†è¨ˆç®—
    # ...ï¼ˆçœç•¥ï¼‰
    
    # å†åˆæœŸåŒ–å®Œäº†ãƒ•ãƒ©ã‚°
    tick_data.attrs["last_reinit_wrap_count"] = tick_data.attrs.get("wrap_count", 0)
    logger.info(f"å·®åˆ†ç‰¹å¾´é‡å†åˆæœŸåŒ–å®Œäº†: {timeframe}")


# ä½¿ç”¨ä¾‹
with h5py.File("data/inference/USDJPY_inference_20251022.h5", "a") as h5:
    # Tickè¿½è¨˜ï¼ˆè‡ªå‹•wrapæ¤œå‡ºãƒ»å†åˆæœŸåŒ–ï¼‰
    append_tick(h5, new_tick)
```

**Ring Buffer wrapå†åˆæœŸåŒ–ä»•æ§˜**:
- **æ¤œå‡ºæ¡ä»¶**: `write_index % max_size == 0`
- **å¯¾è±¡ç‰¹å¾´é‡**: ATR, EMA, momentum, ROCç­‰ã®å·®åˆ†ç³»
- **å†è¨ˆç®—ç¯„å›²**: wrapå¢ƒç•Œå‰å¾Œ100æœ¬ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
- **çµ±è¨ˆãƒ­ã‚°**: wrapå‰å¾Œã®ATRå·®åˆ†ã‚’è¨˜éŒ²ï¼ˆé–¾å€¤10%ã§è­¦å‘Šï¼‰
- **æˆåŠŸæŒ‡æ¨™**: wrapå¾Œã®ç‰¹å¾´é‡é£›ã³ < 10%ï¼ˆATRåŸºæº–ï¼‰

**wrapå†åˆæœŸåŒ–ã®åŠ¹æœ**:
- è¨ˆç®—çª“å´©ã‚Œã«ã‚ˆã‚‹æŒ‡æ¨™é£›ã³é˜²æ­¢
- æ¨è«–å“è³ªã®å®‰å®šåŒ–ï¼ˆwrapå¢ƒç•Œã§ã®ç•°å¸¸ã‚·ã‚°ãƒŠãƒ«æŠ‘åˆ¶ï¼‰
- ãƒ‡ãƒãƒƒã‚°å¯èƒ½æ€§å‘ä¸Šï¼ˆwrapçµ±è¨ˆã®ãƒ­ã‚°å‡ºåŠ›ï¼‰

---

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

### å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º

```
MT5 API â†’ Data Collector â†’ Training Storage (3ãƒ¶æœˆåˆ†)
                             â†“
                        Preprocessor â†’ ç‰¹å¾´é‡HDF5
                             â†“
                        Trainer â†’ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
```

### æ¨è«–ãƒ•ã‚§ãƒ¼ã‚º

```
MT5 Tick Stream â†’ Data Collector â†’ Inference Storage (24h Ring Buffer)
                                      â†“
                                  Preprocessor (Incremental)
                                      â†“
                                  Model Inference (< 10ms)
                                      â†“
                                  Trade Signal
```

---

## âš™ï¸ å®Ÿè£…è©³ç´°

### 1. Training Storageæ›´æ–°ï¼ˆæ—¥æ¬¡ãƒãƒƒãƒï¼‰

```python
class TrainingStorageManager:
    """å­¦ç¿’ç”¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ç®¡ç†"""
    
    def __init__(self, base_dir: str = "data/training"):
        self.base_dir = Path(base_dir)
        self.current_quarter = self._get_current_quarter()
    
    def daily_update(self, symbol: str, date: datetime):
        """æ—¥æ¬¡ã§æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        # ç¾åœ¨ã®å››åŠæœŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
        h5_path = self._get_quarter_file(symbol, self.current_quarter)
        
        # MT5ã‹ã‚‰å‰æ—¥åˆ†ã‚’å–å¾—
        tick_data = fetch_mt5_ticks(symbol, date, date + timedelta(days=1))
        bar_data = fetch_mt5_bars(symbol, date, date + timedelta(days=1))
        
        # è¿½è¨˜ï¼ˆåœ§ç¸®æœ‰åŠ¹ï¼‰
        with h5py.File(h5_path, "a") as f:
            self._append_ticks(f, tick_data, compression="gzip")
            self._append_bars(f, bar_data, compression="gzip")
        
        logger.info(f"Training storage updated: {symbol} {date}")
    
    def rotate_quarterly(self):
        """å››åŠæœŸãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        old_quarters = self._get_quarters_older_than(months=6)
        
        for quarter in old_quarters:
            archive_path = self.base_dir / "archive" / f"{quarter}.tar.gz"
            self._archive_quarter(quarter, archive_path)
            logger.info(f"Archived quarter: {quarter}")
```

### 2. Inference Storageæ›´æ–°ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ï¼‰

```python
class InferenceStorageManager:
    """æ¨è«–ç”¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ç®¡ç†ï¼ˆRing Bufferï¼‰"""
    
    def __init__(self, base_dir: str = "data/inference"):
        self.base_dir = Path(base_dir)
        self.max_tick_size = 100000
        self.max_bar_sizes = {"M1": 1440, "M5": 288, "M15": 96, "H1": 24, "H4": 12}
    
    def initialize_ring_buffer(self, symbol: str):
        """Ring BufferåˆæœŸåŒ–"""
        h5_path = self.base_dir / f"fx_live_{symbol}_latest.h5"
        
        with h5py.File(h5_path, "w") as f:
            # Tickç”¨Ring Buffer
            f.create_dataset(
                "/tick/data",
                shape=(self.max_tick_size, 6),
                dtype="float32",
                fillvalue=np.nan,
                compression=None,  # åœ§ç¸®ç„¡åŠ¹
            )
            f["/tick/data"].attrs["write_index"] = 0
            f["/tick/data"].attrs["wrap_count"] = 0
            
            # å„TFç”¨Ring Buffer
            for tf, size in self.max_bar_sizes.items():
                f.create_dataset(
                    f"/{tf}/data",
                    shape=(size, 50),  # 50ç‰¹å¾´é‡ï¼ˆä¾‹ï¼‰
                    dtype="float32",
                    fillvalue=np.nan,
                    compression=None,
                )
                f[f"/{tf}/data"].attrs["write_index"] = 0
    
    def append_tick(self, symbol: str, tick: np.ndarray):
        """Tickã‚’ring bufferã«è¿½è¨˜ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ï¼‰"""
        h5_path = self.base_dir / f"fx_live_{symbol}_latest.h5"
        
        with h5py.File(h5_path, "a") as f:
            dataset = f["/tick/data"]
            write_idx = dataset.attrs["write_index"]
            max_size = dataset.shape[0]
            
            # å¾ªç’°æ›¸ãè¾¼ã¿
            dataset[write_idx % max_size] = tick
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
            dataset.attrs["write_index"] = write_idx + 1
            dataset.attrs["newest_time"] = tick[0]  # timestamp
            
            # ãƒ©ãƒƒãƒ—ã‚¢ãƒ©ã‚¦ãƒ³ãƒ‰æ¤œå‡º
            if write_idx > 0 and write_idx % max_size == 0:
                dataset.attrs["wrap_count"] += 1
                dataset.attrs["oldest_time"] = tick[0]
    
    def get_recent_data(self, symbol: str, tf: str, bars: int) -> np.ndarray:
        """ç›´è¿‘Næœ¬ã‚’å–å¾—ï¼ˆæ¨è«–ç”¨ï¼‰"""
        h5_path = self.base_dir / f"fx_live_{symbol}_latest.h5"
        
        with h5py.File(h5_path, "r") as f:
            dataset = f[f"/{tf}/data"]
            write_idx = dataset.attrs["write_index"]
            max_size = dataset.shape[0]
            
            # å¾ªç’°ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ç›´è¿‘Næœ¬ã‚’æŠ½å‡º
            if write_idx < bars:
                # ã¾ã ãƒ©ãƒƒãƒ—ã—ã¦ã„ãªã„
                return dataset[:write_idx]
            else:
                # ãƒ©ãƒƒãƒ—æ¸ˆã¿ â†’ æœ€æ–°ä½ç½®ã‹ã‚‰é€†ç®—
                start = (write_idx - bars) % max_size
                end = write_idx % max_size
                
                if start < end:
                    return dataset[start:end]
                else:
                    # ãƒãƒƒãƒ•ã‚¡æœ«å°¾ã¨å…ˆé ­ã«ã¾ãŸãŒã‚‹
                    return np.vstack([
                        dataset[start:],
                        dataset[:end]
                    ])
    
    def auto_cleanup(self, symbol: str):
        """24æ™‚é–“è¶…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
        # Ring Bufferã§ã¯è‡ªå‹•ä¸Šæ›¸ãã•ã‚Œã‚‹ãŸã‚ã€ç‰¹åˆ¥ãªå‡¦ç†ä¸è¦
        # ãŸã ã—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®oldest_timeã‚’æ›´æ–°
        h5_path = self.base_dir / f"fx_live_{symbol}_latest.h5"
        
        with h5py.File(h5_path, "a") as f:
            write_idx = f["/tick/data"].attrs["write_index"]
            max_size = f["/tick/data"].shape[0]
            
            if write_idx >= max_size:
                # ãƒ©ãƒƒãƒ—æ¸ˆã¿ â†’ å¤ã„ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«ä¸Šæ›¸ãã•ã‚Œã¦ã„ã‚‹
                oldest_idx = write_idx % max_size
                oldest_time = f["/tick/data"][oldest_idx, 0]  # timestampåˆ—
                f["/tick/data"].attrs["oldest_time"] = oldest_time
                
                logger.debug(f"Auto cleanup: oldest_time updated to {oldest_time}")
```

### 3. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åˆ‡æ›¿ãƒ­ã‚¸ãƒƒã‚¯

```python
class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ã®çµ±åˆç®¡ç†"""
    
    def __init__(self, mode: str = "training"):
        self.mode = mode
        
        if mode == "training":
            self.storage = TrainingStorageManager()
        elif mode == "inference":
            self.storage = InferenceStorageManager()
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    def collect_and_store(self, symbol: str, **kwargs):
        """ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ‡ãƒ¼ã‚¿åé›†"""
        if self.mode == "training":
            # ãƒãƒƒãƒåé›†ï¼ˆæ—¥æ¬¡æ›´æ–°ï¼‰
            self.storage.daily_update(symbol, kwargs["date"])
        
        elif self.mode == "inference":
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åé›†ï¼ˆTickå˜ä½ï¼‰
            tick_stream = subscribe_mt5_ticks(symbol)
            for tick in tick_stream:
                self.storage.append_tick(symbol, tick)
```

---

## ğŸ“ ã‚µã‚¤ã‚ºè¦‹ç©ã‚‚ã‚Š

### Training Storageï¼ˆ3ãƒ¶æœˆåˆ†ï¼‰

```
Tickï¼ˆåœ§ç¸®å¾Œï¼‰:
- 1 tick = 24 bytesï¼ˆ6 float32ï¼‰
- 1æ—¥ = ç´„100,000 ticksï¼ˆæµå‹•æ€§ã«ã‚ˆã‚‹ï¼‰
- 3ãƒ¶æœˆ = 9,000,000 ticks Ã— 24 bytes = 216 MB
- gzipåœ§ç¸® (level 4) â†’ ç´„ 60 MB

Barï¼ˆM1/M5/M15/H1/H4ã€åœ§ç¸®å¾Œï¼‰:
- M1: 129,600 bars Ã— 50 features Ã— 4 bytes = 25.9 MB â†’ åœ§ç¸®å¾Œ 10 MB
- M5: 25,920 bars Ã— 2.6 MB â†’ åœ§ç¸®å¾Œ 1 MB
- M15/H1/H4: å„ < 1 MB

åˆè¨ˆ: ç´„ 75 MB / å››åŠæœŸ / ã‚·ãƒ³ãƒœãƒ«
```

### Inference Storageï¼ˆ24æ™‚é–“åˆ†ï¼‰

```
Tickï¼ˆåœ§ç¸®ãªã—ï¼‰:
- 100,000 ticks Ã— 24 bytes = 2.4 MBï¼ˆå›ºå®šã‚µã‚¤ã‚ºï¼‰

Barï¼ˆM1=1440æœ¬ã€M5=288æœ¬ã€åœ§ç¸®ãªã—ï¼‰:
- M1: 1440 Ã— 50 Ã— 4 = 0.28 MB
- M5: 288 Ã— 50 Ã— 4 = 0.06 MB
- M15/H1/H4: å„ < 0.05 MB

åˆè¨ˆ: ç´„ 3 MB / ã‚·ãƒ³ãƒœãƒ«ï¼ˆå›ºå®šï¼‰
```

**æ¯”è¼ƒ**:
- Training: 75 MB Ã— 4å››åŠæœŸ = 300 MB/å¹´ï¼ˆåœ§ç¸®æ¸ˆã¿ï¼‰
- Inference: 3 MBï¼ˆå¸¸ã«å›ºå®šã€å¤ã„ãƒ‡ãƒ¼ã‚¿ã¯ä¸Šæ›¸ãï¼‰

---

## ğŸ”§ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

### config/data_collector.yaml

```yaml
storage_policy:
  # ãƒ¢ãƒ¼ãƒ‰é¸æŠ
  mode: training  # training | inference
  
  # Training Storageè¨­å®š
  training:
    base_dir: "data/training"
    retention:
      tick_data: 3 months
      bar_data: 6 months
    compression:
      enabled: true
      algorithm: gzip
      level: 4
    update:
      frequency: daily
      time: "06:00 UTC"
    rotation:
      interval: monthly
      archive_after: 6 months
      archive_dir: "data/training/archive"
  
  # Inference Storageè¨­å®š
  inference:
    base_dir: "data/inference"
    retention:
      tick_data: 24 hours
      bar_data:
        M1: 24 hours
        M5: 24 hours
        M15: 48 hours
        H1: 48 hours
        H4: 48 hours
    compression:
      enabled: false  # é€Ÿåº¦å„ªå…ˆ
    ring_buffer:
      max_tick_size: 100000
      max_bar_sizes:
        M1: 1440
        M5: 288
        M15: 192
        H1: 96
        H4: 48
    auto_cleanup:
      enabled: true
      check_interval: 1 hour
      delete_older_than: 24 hours
```

---

## ğŸš¨ ã‚¨ãƒ©ãƒ¼æ¡ä»¶

ä»¥ä¸‹ã®æ¡ä»¶ã§å‡¦ç†ã‚’åœæ­¢ï¼ˆERRORï¼‰:

1. **Ring Bufferæº¢ã‚Œ**: `write_index` ãŒç•°å¸¸ã«é€²ã‚“ã§ã„ã‚‹ï¼ˆ> max_size Ã— 10ï¼‰
2. **ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³**: Training StorageãŒ95%è¶…
3. **ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç•°å¸¸**: 24æ™‚é–“ä»¥ä¸Šã®å¤ã„ãƒ‡ãƒ¼ã‚¿ãŒInference Storageã«æ®‹å­˜
4. **æ›¸ãè¾¼ã¿å¤±æ•—**: HDF5ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼

### HDF5ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼å†è©¦è¡Œæˆ¦ç•¥

HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã®åŒæ™‚æ›¸ãè¾¼ã¿æ™‚ã€ä¸€éæ€§ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã§å³åœæ­¢ã‚’é¿ã‘ã‚‹ãŸã‚ã®æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤å®Ÿè£…ï¼š

```python
import time
import h5py
from functools import wraps

class HDF5LockError(Exception):
    """HDF5ãƒ­ãƒƒã‚¯é–¢é€£ã‚¨ãƒ©ãƒ¼"""
    pass


def hdf5_retry(max_attempts: int = 3, initial_delay: float = 0.1, 
               backoff_factor: float = 2.0, readonly_retry: bool = True):
    """
    HDF5ã‚¢ã‚¯ã‚»ã‚¹ã®ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    
    Args:
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3å›ï¼‰
        initial_delay: åˆå›å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        backoff_factor: é…å»¶å¢—åŠ ä¿‚æ•°ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
        readonly_retry: ãƒªãƒ¼ãƒ‰ã‚ªãƒ³ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰ã§ã®ãƒªãƒˆãƒ©ã‚¤æœ‰åŠ¹åŒ–
    
    æˆ¦ç•¥:
    - 1å›ç›®å¤±æ•—: 0.1ç§’å¾…æ©Ÿ
    - 2å›ç›®å¤±æ•—: 0.2ç§’å¾…æ©Ÿï¼ˆ0.1 * 2^1ï¼‰
    - 3å›ç›®å¤±æ•—: 0.4ç§’å¾…æ©Ÿï¼ˆ0.1 * 2^2ï¼‰
    - 3å›å¤±æ•—å¾Œ: ä¾‹å¤–ã‚’ä¸Šä½ã«ä¼æ’­
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            last_exception = None
            
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                
                except (OSError, IOError) as e:
                    # HDF5ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼æ¤œå‡º
                    if "unable to lock file" in str(e) or "Resource temporarily unavailable" in str(e):
                        last_exception = e
                        
                        if attempt < max_attempts:
                            logger.warning(
                                f"HDF5ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ [{attempt}/{max_attempts}]: {e}, "
                                f"å¾…æ©Ÿ: {delay:.2f}ç§’"
                            )
                            time.sleep(delay)
                            delay *= backoff_factor
                        else:
                            logger.error(
                                f"HDF5ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼æœ€å¤§è©¦è¡Œå›æ•°åˆ°é”: {max_attempts}å›, "
                                f"è«¦ã‚ã¾ã™"
                            )
                            raise HDF5LockError(
                                f"HDF5ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ï¼ˆ{max_attempts}å›è©¦è¡Œï¼‰: {e}"
                            ) from e
                    else:
                        # ãƒ­ãƒƒã‚¯ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«ä¼æ’­
                        raise
                
                except Exception as e:
                    # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯ãƒªãƒˆãƒ©ã‚¤ã›ãšå³åº§ã«ä¼æ’­
                    raise
            
            # åˆ°é”ã—ãªã„ãŒã€å‹ãƒã‚§ãƒƒã‚«ãƒ¼å¯¾ç­–
            if last_exception:
                raise last_exception
        
        return wrapper
    return decorator


# ä½¿ç”¨ä¾‹: æ›¸ãè¾¼ã¿æ“ä½œ
@hdf5_retry(max_attempts=3, initial_delay=0.1)
def write_inference_tick(h5_path: str, tick_data: np.ndarray):
    """æ¨è«–ç”¨Tickæ›¸ãè¾¼ã¿ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    with h5py.File(h5_path, "a") as f:
        write_idx = f["/tick/data"].attrs["write_index"]
        max_size = f["/tick/data"].shape[0]
        f["/tick/data"][write_idx % max_size] = tick_data
        f["/tick/data"].attrs["write_index"] = write_idx + 1


# ä½¿ç”¨ä¾‹: ãƒªãƒ¼ãƒ‰ã‚ªãƒ³ãƒªãƒ¼ãƒªãƒˆãƒ©ã‚¤
@hdf5_retry(max_attempts=5, initial_delay=0.05, readonly_retry=True)
def read_recent_bars(h5_path: str, n_bars: int = 480):
    """ç›´è¿‘ãƒãƒ¼èª­ã¿è¾¼ã¿ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    with h5py.File(h5_path, "r") as f:  # ãƒªãƒ¼ãƒ‰ã‚ªãƒ³ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰
        return f["/M1/data"][-n_bars:]


# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¾‹
try:
    write_inference_tick("data/inference/USDJPY.h5", new_tick)
except HDF5LockError as e:
    logger.critical(f"HDF5æ›¸ãè¾¼ã¿å¤±æ•—: {e}")
    # ä»£æ›¿å‡¦ç†ï¼ˆä¾‹: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã«ä¸€æ™‚ä¿å­˜ï¼‰
    buffer.append(new_tick)
```

**HDF5ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼å†è©¦è¡Œä»•æ§˜**:
- **max_attempts**: 3å›ï¼ˆæ›¸ãè¾¼ã¿ï¼‰/ 5å›ï¼ˆèª­ã¿è¾¼ã¿ï¼‰
- **åˆå›å¾…æ©Ÿ**: 0.1ç§’ï¼ˆæ›¸ãè¾¼ã¿ï¼‰/ 0.05ç§’ï¼ˆèª­ã¿è¾¼ã¿ï¼‰
- **ãƒãƒƒã‚¯ã‚ªãƒ•ä¿‚æ•°**: 2.0ï¼ˆæŒ‡æ•°çš„ã«å¢—åŠ ï¼‰
- **åˆè¨ˆå¾…æ©Ÿæ™‚é–“**: æœ€å¤§ 0.7ç§’ï¼ˆ3å›è©¦è¡Œ: 0.1 + 0.2 + 0.4ï¼‰
- **ãƒªãƒ¼ãƒ‰ã‚ªãƒ³ãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰**: ã‚ˆã‚Šå¤šãã®ãƒªãƒˆãƒ©ã‚¤è¨±å¯ï¼ˆ5å›ï¼‰
- **æˆåŠŸæŒ‡æ¨™**: ãƒ­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚‹å…¨ä½“åœæ­¢ = 0å›

**ä¸€éæ€§ãƒ­ãƒƒã‚¯ vs æ’ä¹…çš„ã‚¨ãƒ©ãƒ¼**:
- **ä¸€éæ€§**: ä»–ãƒ—ãƒ­ã‚»ã‚¹ã®çŸ­æœŸæ›¸ãè¾¼ã¿ â†’ ãƒªãƒˆãƒ©ã‚¤ã§è§£æ±º
- **æ’ä¹…çš„**: ãƒ•ã‚¡ã‚¤ãƒ«ç ´æã€æ¨©é™ã‚¨ãƒ©ãƒ¼ â†’ å³åº§ã«ä¾‹å¤–ä¼æ’­

---

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. æ›¸ãè¾¼ã¿æœ€é©åŒ–

```python
# Training: ãƒãƒƒãƒæ›¸ãè¾¼ã¿ï¼ˆåœ§ç¸®æœ‰åŠ¹ï¼‰
with h5py.File(path, "a") as f:
    f["/tick/data"].resize((current_size + batch_size, 6))
    f["/tick/data"][current_size:] = batch_data  # 1000è¡Œå˜ä½
    # gzipåœ§ç¸®ã§æ›¸ãè¾¼ã¿é…ã„ãŒã€å­¦ç¿’æ™‚èª­ã¿è¾¼ã¿ã¯é«˜é€Ÿ

# Inference: å˜ä¸€è¡Œæ›¸ãè¾¼ã¿ï¼ˆåœ§ç¸®ç„¡åŠ¹ï¼‰
with h5py.File(path, "a") as f:
    idx = write_index % max_size
    f["/tick/data"][idx] = single_tick  # åœ§ç¸®ãªã—ã§é«˜é€Ÿ
```

### 2. èª­ã¿è¾¼ã¿æœ€é©åŒ–

```python
# Training: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§èª­ã¿è¾¼ã¿
with h5py.File(path, "r") as f:
    for i in range(0, total_size, chunk_size):
        chunk = f["/tick/data"][i:i+chunk_size]
        process_chunk(chunk)

# Inference: ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¹ãƒ©ã‚¤ã‚¹èª­ã¿è¾¼ã¿
with h5py.File(path, "r") as f:
    recent = f["/M1/data"][-480:]  # ç›´è¿‘8æ™‚é–“åˆ†ã®ã¿
    predict(recent)  # ä¸è¦ãªå¤ã„ãƒ‡ãƒ¼ã‚¿ã¯èª­ã¾ãªã„
```

---

## ğŸ” ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### å®šæœŸãƒã‚§ãƒƒã‚¯é …ç›®

```python
def monitor_storage_health():
    """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
    
    # Training Storage
    training_size = get_directory_size("data/training")
    if training_size > 10 * 1024**3:  # 10 GBè¶…
        logger.warning(f"Training storage large: {training_size / 1024**3:.1f} GB")
    
    # Inference Storage
    for symbol in ["USDJPY", "EURUSD"]:
        h5_path = f"data/inference/fx_live_{symbol}_latest.h5"
        with h5py.File(h5_path, "r") as f:
            oldest_time = f["/tick/data"].attrs.get("oldest_time", 0)
            age_hours = (time.time() - oldest_time) / 3600
            
            if age_hours > 30:  # 30æ™‚é–“è¶…ã®å¤ã„ãƒ‡ãƒ¼ã‚¿
                logger.error(f"Inference storage stale: {symbol} oldest={age_hours:.1f}h")
    
    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
    disk_usage = shutil.disk_usage("data")
    if disk_usage.percent > 90:
        logger.critical(f"Disk usage high: {disk_usage.percent}%")
```

---

## ğŸ”® å°†æ¥æ‹¡å¼µ

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º2: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®

```yaml
adaptive_compression:
  # å¤ã„ãƒ‡ãƒ¼ã‚¿ã»ã©é«˜åœ§ç¸®
  recent_data:
    age: < 1 month
    compression: gzip level 4
  
  old_data:
    age: 1-3 months
    compression: gzip level 9  # é«˜åœ§ç¸®
  
  archive:
    age: > 3 months
    compression: lzma          # æœ€é«˜åœ§ç¸®ç‡
```

### å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º3: åˆ†æ•£ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸

```yaml
distributed_storage:
  # é«˜é€ŸSSD + å¤§å®¹é‡HDD
  hot_storage:
    location: /ssd/inference
    retention: 24 hours
    type: NVMe SSD
  
  warm_storage:
    location: /hdd/training
    retention: 3 months
    type: HDD
  
  cold_storage:
    location: /archive/backup
    retention: 2 years
    type: S3 / NAS
```

---

## ğŸ“š é–¢é€£ä»•æ§˜

- [DATA_COLLECTOR_SPEC.md](../DATA_COLLECTOR_SPEC.md) - ãƒ‡ãƒ¼ã‚¿åé›†å…¨ä½“ä»•æ§˜
- [EXECUTION_LATENCY_SPEC.md](../validator/EXECUTION_LATENCY_SPEC.md) - æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦ä»¶

---

## ğŸ“ å¤‰æ›´å±¥æ­´

- **2025-10-21**: åˆç‰ˆä½œæˆ
  - Training/Inference Storageåˆ†é›¢è¨­è¨ˆ
  - Ring Bufferå®Ÿè£…è©³ç´°
  - 3ãƒ¶æœˆ/24æ™‚é–“ã®ä¿æŒæœŸé–“å®šç¾©
  - ã‚µã‚¤ã‚ºè¦‹ç©ã‚‚ã‚Šãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
