#!/usr/bin/env python3
"""
å‰å‡¦ç†çµæœç¢ºèªãƒ„ãƒ¼ãƒ«

ä½¿ç”¨æ–¹æ³•:
    bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py
    bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py data/preprocessor.h5
"""

import sys
import json
import h5py
import argparse
import numpy as np
from pathlib import Path
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent


def format_bytes(size_bytes: int) -> str:
    """ãƒã‚¤ãƒˆã‚µã‚¤ã‚ºã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"


def inspect_preprocessor(file_path: Path) -> None:
    """å‰å‡¦ç†æ¸ˆã¿HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è¡¨ç¤º"""
    
    print("=" * 80)
    print("ğŸ” å‰å‡¦ç†çµæœç¢ºèªãƒ„ãƒ¼ãƒ«")
    print("=" * 80)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not file_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return
    
    print(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")
    print(f"   ã‚µã‚¤ã‚º: {format_bytes(file_path.stat().st_size)}")
    
    try:
        with h5py.File(file_path, 'r') as f:
            # 1. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æƒ…å ±
            print("\n" + "=" * 80)
            print("ğŸ“Š ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æƒ…å ±")
            print("=" * 80)
            
            if 'sequences' in f:
                seq_group = f['sequences']
                print(f"\nåˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ : {list(seq_group.keys())}\n")
                
                total_sequences = 0
                for tf_name in sorted(seq_group.keys()):
                    seq_data = seq_group[tf_name]
                    shape = seq_data.shape
                    total_sequences += shape[0]
                    
                    print(f"â±ï¸  {tf_name}:")
                    print(f"   Shape: {shape}")
                    print(f"   - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {shape[0]:,}")
                    print(f"   - ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {shape[1]}")
                    print(f"   - ç‰¹å¾´é‡æ•°: {shape[2]}")
                    print(f"   - ãƒ‡ãƒ¼ã‚¿å‹: {seq_data.dtype}")
                    print(f"   - ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º: {format_bytes(seq_data.size * seq_data.dtype.itemsize)}")
                    print()
                
                print(f"ğŸ“ˆ ç·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {total_sequences:,}")
                
                # NaN/Infæ¤œè¨¼
                print("\n" + "=" * 80)
                print("âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼")
                print("=" * 80)
                print("\nã€NaN/Infæ¤œæŸ»ã€‘")
                
                all_clean = True
                for tf_name in sorted(seq_group.keys()):
                    # ã‚µãƒ³ãƒ—ãƒ«ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®100ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼‰
                    sample_size = min(100, seq_group[tf_name].shape[0])
                    sample_data = seq_group[tf_name][:sample_size]
                    
                    nan_count = np.isnan(sample_data).sum()
                    inf_count = np.isinf(sample_data).sum()
                    total_elements = sample_data.size
                    
                    if nan_count > 0 or inf_count > 0:
                        all_clean = False
                        print(f"âš ï¸  {tf_name}: NaN={nan_count}, Inf={inf_count} / {total_elements:,} ({(nan_count+inf_count)/total_elements*100:.2f}%)")
                    else:
                        print(f"âœ… {tf_name}: ã‚¯ãƒªãƒ¼ãƒ³ï¼ˆå…ˆé ­{sample_size}ã‚µãƒ³ãƒ—ãƒ«ï¼‰")
                
                if all_clean:
                    print(f"\nâœ… å…¨ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã§NaN/Inf ãªã—ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ï¼‰")
                else:
                    print(f"\nâš ï¸  NaN/Infæ¤œå‡ºã‚ã‚Š - ãƒ‡ãƒ¼ã‚¿å“è³ªã«å•é¡Œ")
            else:
                print("âš ï¸  ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # 2. æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            print("\n" + "=" * 80)
            print("ğŸ¯ æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
            print("=" * 80)
            
            if 'scaler_params' in f:
                scaler_params = json.loads(f['scaler_params'][()])
                
                print(f"\næ­£è¦åŒ–æ–¹æ³•: {scaler_params.get('method', 'unknown')}")
                print(f"ç‰¹å¾´é‡æ•°: {len(scaler_params.get('feature_names', []))}")
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°è¡¨ç¤º
                if scaler_params.get('method') == 'robust':
                    print(f"\nã€RobustScaler ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
                    print(f"å››åˆ†ä½ç¯„å›²: {scaler_params.get('quantile_range', [])}")
                    
                    center = scaler_params.get('center_', [])
                    scale = scaler_params.get('scale_', [])
                    
                    if center and scale:
                        print(f"\nCenterï¼ˆå…ˆé ­5å€‹ï¼‰: {center[:5]}")
                        print(f"Scaleï¼ˆå…ˆé ­5å€‹ï¼‰: {scale[:5]}")
                        print(f"\nCenterçµ±è¨ˆ:")
                        print(f"  - æœ€å°: {min(center):.6f}")
                        print(f"  - æœ€å¤§: {max(center):.6f}")
                        print(f"  - å¹³å‡: {sum(center)/len(center):.6f}")
                        print(f"\nScaleçµ±è¨ˆ:")
                        print(f"  - æœ€å°: {min(scale):.6f}")
                        print(f"  - æœ€å¤§: {max(scale):.6f}")
                        print(f"  - å¹³å‡: {sum(scale)/len(scale):.6f}")
                
                elif scaler_params.get('method') == 'standard':
                    print(f"\nã€StandardScaler ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
                    
                    mean = scaler_params.get('mean_', [])
                    scale = scaler_params.get('scale_', [])
                    
                    if mean and scale:
                        print(f"\nMeanï¼ˆå…ˆé ­5å€‹ï¼‰: {mean[:5]}")
                        print(f"Scaleï¼ˆå…ˆé ­5å€‹ï¼‰: {scale[:5]}")
                
                elif scaler_params.get('method') == 'minmax':
                    print(f"\nã€MinMaxScaler ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
                    
                    data_min = scaler_params.get('data_min_', [])
                    data_max = scaler_params.get('data_max_', [])
                    
                    if data_min and data_max:
                        print(f"\nData Minï¼ˆå…ˆé ­5å€‹ï¼‰: {data_min[:5]}")
                        print(f"Data Maxï¼ˆå…ˆé ­5å€‹ï¼‰: {data_max[:5]}")
                
                # ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
                feature_names = scaler_params.get('feature_names', [])
                if feature_names:
                    print(f"\nã€ç‰¹å¾´é‡åãƒªã‚¹ãƒˆã€‘ï¼ˆå…¨{len(feature_names)}å€‹ï¼‰")
                    for i, name in enumerate(feature_names, 1):
                        print(f"  {i:2d}. {name}")
            else:
                print("\nâš ï¸  æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # 3. ç‰¹å¾´é‡åï¼ˆscaler_paramsã‹ã‚‰å–å¾—ã§ããªã„å ´åˆã®äºˆå‚™ï¼‰
            if 'feature_names' in f and 'scaler_params' not in f:
                print("\n" + "=" * 80)
                print("ğŸ“‹ ç‰¹å¾´é‡å")
                print("=" * 80)
                
                feature_names = [name.decode('utf-8') if isinstance(name, bytes) else name 
                               for name in f['feature_names'][:]]
                print(f"\nç‰¹å¾´é‡æ•°: {len(feature_names)}")
                print("\nç‰¹å¾´é‡ãƒªã‚¹ãƒˆ:")
                for i, name in enumerate(feature_names, 1):
                    print(f"  {i:2d}. {name}")
            
            # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            print("\n" + "=" * 80)
            print("ğŸ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
            print("=" * 80)
            
            if 'metadata' in f:
                metadata = json.loads(f['metadata'][()])
                
                print(f"\nç”Ÿæˆæ—¥æ™‚: {metadata.get('processing_timestamp', 'N/A')}")
                print(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {metadata.get('input_file', 'N/A')}")
                
                if 'filter_stats' in metadata:
                    stats = metadata['filter_stats']
                    print(f"\nãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆ:")
                    print(f"  - åˆæœŸç‰¹å¾´é‡æ•°: {stats.get('initial', 'N/A')}")
                    print(f"  - ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {stats.get('final', 'N/A')}")
                    print(f"  - é™¤å¤–æ•°: {stats.get('initial', 0) - stats.get('final', 0)}")
                
                if 'config' in metadata:
                    print(f"\nè¨­å®šæƒ…å ±:")
                    config = metadata['config']
                    
                    # å“è³ªãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
                    if 'quality_filter' in config:
                        qf = config['quality_filter']
                        print(f"  å“è³ªãƒ•ã‚£ãƒ«ã‚¿:")
                        print(f"    - NaNæ¯”ç‡ä¸Šé™: {qf.get('max_nan_ratio', 'N/A')}")
                        print(f"    - æœ€å°IQR: {qf.get('min_iqr', 'N/A')}")
                        print(f"    - ç›¸é–¢é–¾å€¤: {qf.get('max_correlation', 'N/A')}")
                    
                    # æ­£è¦åŒ–è¨­å®š
                    if 'normalization' in config:
                        norm = config['normalization']
                        print(f"  æ­£è¦åŒ–:")
                        print(f"    - æ–¹æ³•: {norm.get('method', 'N/A')}")
                        print(f"    - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜: {norm.get('save_params', 'N/A')}")
            else:
                print("\nâš ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§
            print("\n" + "=" * 80)
            print("ğŸ—‚ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§")
            print("=" * 80)
            
            def print_tree(group, prefix=""):
                """HDF5ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ„ãƒªãƒ¼è¡¨ç¤º"""
                items = list(group.items())
                for i, (name, item) in enumerate(items):
                    is_last = (i == len(items) - 1)
                    connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                    
                    if isinstance(item, h5py.Group):
                        print(f"{prefix}{connector}{name}/ (Group)")
                        extension = "    " if is_last else "â”‚   "
                        print_tree(item, prefix + extension)
                    else:
                        shape_str = f"{item.shape}" if hasattr(item, 'shape') else ""
                        dtype_str = f"[{item.dtype}]" if hasattr(item, 'dtype') else ""
                        print(f"{prefix}{connector}{name} {shape_str} {dtype_str}")
            
            print()
            print_tree(f)
    
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 80)


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='å‰å‡¦ç†çµæœç¢ºèªãƒ„ãƒ¼ãƒ«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py
  bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py data/preprocessor.h5
  bash ./docker_run.sh python3 tools/preprocessor/inspect_preprocessor.py data/20251023_143045_preprocessor.h5
        """
    )
    
    parser.add_argument(
        'file',
        nargs='?',
        default='data/preprocessor.h5',
        help='ç¢ºèªã™ã‚‹HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/preprocessor.h5ï¼‰'
    )
    
    args = parser.parse_args()
    
    # ãƒ‘ã‚¹ã‚’Pathã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    if Path(args.file).is_absolute():
        file_path = Path(args.file)
    else:
        file_path = PROJECT_ROOT / args.file
    
    inspect_preprocessor(file_path)


if __name__ == "__main__":
    main()
