"""
ç‰¹å¾´é‡è¨ˆç®—çµ±åˆã‚¯ãƒ©ã‚¹

å„ã‚«ãƒ†ã‚´ãƒªè¨ˆç®—å™¨ã‚’ç®¡ç†ã—ã€ç‰¹å¾´é‡è¨ˆç®—ã‚’çµ±åˆçš„ã«å®Ÿè¡Œ
"""

from typing import Dict, List
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import time
import h5py
import json

from .base_calculator import BaseCalculator

logger = logging.getLogger(__name__)


class FeatureCalculatorIntegrator:
    """
    ç‰¹å¾´é‡è¨ˆç®—ã®çµ±åˆã‚¯ãƒ©ã‚¹
    
    å½¹å‰²:
    - å„ã‚«ãƒ†ã‚´ãƒªè¨ˆç®—å™¨ã®å®Ÿè¡Œç®¡ç†
    - ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»å¢—åˆ†æ›´æ–°ï¼‰
    - æ®µéšçš„æ¤œè¨¼ã®å®Ÿæ–½
    - çµ±åˆHDF5å‡ºåŠ›
    """
    
    def __init__(self, config: dict, project_root: Path = None):
        """
        åˆæœŸåŒ–
        
        Args:
            config: è¨­å®šè¾æ›¸
            project_root: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹
        """
        self.config = config
        self.calculators: List[BaseCalculator] = []
        self.category_results = {}
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
        if project_root is None:
            project_root = Path(__file__).resolve().parent.parent.parent
        self.project_root = project_root
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.category_dir = self.project_root / "data" / "feature_calculator"
        self.category_dir.mkdir(parents=True, exist_ok=True)
    
    def register_calculator(self, calculator: BaseCalculator):
        """
        è¨ˆç®—å™¨ã‚’ç™»éŒ²
        
        Args:
            calculator: è¨ˆç®—å™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        category_name = calculator.name
        
        # ã‚«ãƒ†ã‚´ãƒªãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not self.config['enable_categories'].get(category_name, False):
            logger.info(f"â­ï¸  {category_name}: ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return
        
        self.calculators.append(calculator)
        logger.info(f"âœ… {category_name} ã‚’ç™»éŒ²")
    
    def calculate(
        self, 
        raw_data: Dict[str, pd.DataFrame]
    ) -> pd.DataFrame:
        """
        å…¨ã‚«ãƒ†ã‚´ãƒªã®ç‰¹å¾´é‡ã‚’è¨ˆç®—
        
        ãƒ•ãƒ­ãƒ¼:
        1. å„ã‚«ãƒ†ã‚´ãƒªã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Š â†’ èª­ã¿è¾¼ã¿
        3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã— â†’ è¨ˆç®— â†’ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        4. å…¨ã‚«ãƒ†ã‚´ãƒªã‚’çµ±åˆ
        
        Args:
            raw_data: ãƒãƒ«ãƒTFç”Ÿãƒ‡ãƒ¼ã‚¿
                {
                    'M1': DataFrame(N, 6),
                    'M5': DataFrame(N, 6),
                    'M15': DataFrame(N, 6),
                    'H1': DataFrame(N, 6),
                    'H4': DataFrame(N, 6),
                }
        
        Returns:
            DataFrame(N, K): Kåˆ—ã®çµ±åˆç‰¹å¾´é‡
        """
        all_features = []
        
        # å†è¨ˆç®—å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªï¼ˆè¨­å®šã§æŒ‡å®šã€ãªã‘ã‚Œã°å…¨ã¦å†è¨ˆç®—ï¼‰
        recalculate_categories = self.config.get('recalculate_categories', None)
        
        for calculator in self.calculators:
            category_name = calculator.name
            category_file = self.category_dir / f"{category_name}.h5"
            
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ãƒªãƒãƒ¼ãƒ ï¼ˆä»•æ§˜æ›¸: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã€JSTæ—¥æ™‚ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã§ãƒªãƒãƒ¼ãƒ é€€é¿ï¼‰
            if category_file.exists():
                from datetime import datetime, timezone, timedelta
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆæ—¥æ™‚ã‚’å–å¾—
                file_mtime = category_file.stat().st_mtime
                file_dt = datetime.fromtimestamp(file_mtime, tz=timezone(timedelta(hours=9)))
                timestamp_str = file_dt.strftime('%Y%m%d_%H%M%S')
                backup_file = self.category_dir / f"{timestamp_str}_{category_name}.h5"
                category_file.rename(backup_file)
                logger.info(f"ğŸ’¾ {category_name} æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªãƒãƒ¼ãƒ : {backup_file.name}")
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨åˆ¤å®š
                use_cache = recalculate_categories is not None and category_name not in recalculate_categories
                
                if use_cache:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
                    logger.info(f"ğŸ’¾ {category_name} ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨")
                    
                    start_time = time.time()
                    
                    try:
                        with h5py.File(backup_file, 'r') as f:
                            data_array = f['features'][:]
                            feature_names_bytes = f['feature_names'][:]
                            feature_names = [name.decode('utf-8') for name in feature_names_bytes]
                            cat_features = pd.DataFrame(data_array, columns=feature_names)
                            metadata = json.loads(f['metadata'][()].decode('utf-8'))
                        
                        calc_time = time.time() - start_time
                        self.category_results[category_name] = {
                            'count': len(cat_features.columns),
                            'calculation_time_sec': round(calc_time, 2),
                            'columns': cat_features.columns.tolist(),
                            'nan_ratio': metadata.get('nan_ratio', 0.0),
                            'inf_count': metadata.get('inf_count', 0),
                            'cached': True
                        }
                        
                        all_features.append(cat_features)
                        logger.info(f"   â†’ {len(cat_features.columns)}åˆ—èª­ã¿è¾¼ã¿ ({calc_time:.1f}ç§’)")
                        continue
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸  {category_name} ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å¤±æ•—: {e}\n   â†’ å†è¨ˆç®—ã—ã¾ã™")
            
            # è¨ˆç®—å®Ÿè¡Œ
            logger.info(f"ğŸ§® {category_name} è¨ˆç®—é–‹å§‹")
            
            start_time = time.time()
            
            try:
                # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡è¨ˆç®—
                cat_features = calculator.compute(raw_data)
                
                # æ¤œè¨¼
                validation = calculator.validate(cat_features)
                
                if not validation['valid']:
                    logger.warning(
                        f"âš ï¸  {category_name} æ¤œè¨¼å¤±æ•—: "
                        f"{', '.join(validation['warnings'])}"
                    )
                
                # çµæœã‚’è¨˜éŒ²
                calc_time = time.time() - start_time
                self.category_results[category_name] = {
                    'count': len(cat_features.columns),
                    'calculation_time_sec': round(calc_time, 2),
                    'columns': cat_features.columns.tolist(),
                    'nan_ratio': validation['nan_ratio'],
                    'inf_count': validation['inf_count'],
                    'cached': False
                }
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥HDF5ä¿å­˜
                self._save_category_features(
                    category_name,
                    cat_features,
                    validation
                )
                
                all_features.append(cat_features)
                
                logger.info(
                    f"   â†’ {len(cat_features.columns)}åˆ—ç”Ÿæˆ "
                    f"({calc_time:.1f}ç§’)"
                )
                
            except Exception as e:
                logger.error(f"âŒ {category_name} è¨ˆç®—å¤±æ•—: {e}")
                raise
        
        # å…¨ç‰¹å¾´é‡ã‚’çµåˆ
        if not all_features:
            raise ValueError("è¨ˆç®—ã•ã‚ŒãŸç‰¹å¾´é‡ãŒã‚ã‚Šã¾ã›ã‚“")
        
        features = pd.concat(all_features, axis=1)
        
        logger.info(f"âœ… ç‰¹å¾´é‡è¨ˆç®—å®Œäº†: {len(features.columns)}åˆ—")
        
        return features
    
    def get_category_info(self) -> Dict:
        """
        ã‚«ãƒ†ã‚´ãƒªåˆ¥æƒ…å ±ã‚’å–å¾—
        
        Returns:
            dict: ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆæƒ…å ±
        """
        return self.category_results
    
    def _save_category_features(
        self,
        category_name: str,
        features: pd.DataFrame,
        validation: Dict
    ):
        """
        ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’å€‹åˆ¥HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            category_name: ã‚«ãƒ†ã‚´ãƒªå
            features: ç‰¹å¾´é‡DataFrame
            validation: æ¤œè¨¼çµæœ
        """
        category_file = self.category_dir / f"{category_name}.h5"
        
        # ãƒªãƒãƒ¼ãƒ ã¯ calculate() ã§å®Ÿè¡Œæ¸ˆã¿
        try:
            # numpyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›
            def convert_to_serializable(obj):
                """numpyå‹ã‚’JSONäº’æ›å‹ã«å¤‰æ›"""
                if isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
                    return int(obj)
                elif isinstance(obj, (np.float64, np.float32, np.float16)):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_to_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_to_serializable(item) for item in obj]
                return obj
            
            # HDF5ä¿å­˜ï¼ˆh5pyå½¢å¼ï¼‰
            with h5py.File(category_file, 'w') as f:
                # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
                f.create_dataset(
                    'features',
                    data=features.values,
                    dtype='float32',
                    compression='gzip',
                    compression_opts=5
                )
                
                # ç‰¹å¾´é‡å
                feature_names = [name.encode('utf-8') for name in features.columns]
                f.create_dataset('feature_names', data=feature_names)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                metadata = {
                    'category_name': category_name,
                    'num_features': len(features.columns),
                    'num_samples': len(features),
                    'nan_ratio': validation['nan_ratio'],
                    'inf_count': validation['inf_count'],
                    'feature_names': features.columns.tolist()
                }
                metadata_serializable = convert_to_serializable(metadata)
                metadata_json = json.dumps(metadata_serializable, ensure_ascii=False).encode('utf-8')
                f.create_dataset('metadata', data=metadata_json)
            
            logger.info(f"   ğŸ’¾ ä¿å­˜: {category_file.name}")
            
        except Exception as e:
            logger.warning(f"âš ï¸  {category_name} ä¿å­˜å¤±æ•—: {e}")
            # ä¿å­˜å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã§æ¬¡å›å†è¨ˆç®—ï¼‰
