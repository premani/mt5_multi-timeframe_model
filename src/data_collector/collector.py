"""
ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import numpy as np
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Tuple
from pathlib import Path
import json

from .api_client import MT5APIClient
from .validator import DataValidator
from .hdf5_writer import HDF5Writer


class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®æœŸå¾…é–“éš”ï¼ˆç§’ï¼‰
    TF_INTERVALS = {
        'M1': 60,
        'M5': 300,
        'M15': 900,
        'H1': 3600,
        'H4': 14400
    }
    
    def __init__(
        self,
        config,
        logger
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            config: ConfigManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            logger: LoggingManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.config = config
        self.logger = logger
        
        # APIè¨­å®šæ¤œè¨¼
        config.validate_api_config()
        
        # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.api_client = MT5APIClient(
            endpoint=config.get_required('api.endpoint'),
            api_key=config.get_required('api.api_key'),
            timeout=config.get('api.timeout', 60),
            logger=logger
        )
        
        # ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        self.validator = DataValidator(logger=logger)
        
        # HDF5ãƒ©ã‚¤ã‚¿ãƒ¼åˆæœŸåŒ–
        output_dir = config.get('output.data_dir', 'data')
        base_name = config.get('output.base_name', 'data_collector')
        output_path = Path(output_dir) / f"{base_name}.h5"
        
        compression = config.get('output.hdf5.compression')
        self.hdf5_writer = HDF5Writer(
            output_path=str(output_path),
            compression=compression,
            logger=logger
        )
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'timeframes': {},
            'ticks': {},
            'quality': {},
            'performance': {}
        }
    
    def collect(self):
        """ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        self.logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if self.config.get('output.backup.enabled', True):
            timestamp_format = self.config.get('output.backup.timestamp_format', '%Y%m%d_%H%M%S')
            self.hdf5_writer.backup_existing(timestamp_format)
        
        # åé›†è¨­å®šå–å¾—
        symbols = self.config.get_required('data_collection.symbols')
        timeframes = self.config.get_required('data_collection.timeframes')
        period = self.config.get_required('data_collection.period')
        
        # æ—¥æ™‚ã‚’ISO8601å½¢å¼ã«å¤‰æ›
        start_dt = datetime.fromisoformat(period['start']).replace(tzinfo=timezone.utc)
        end_dt = datetime.fromisoformat(period['end']).replace(tzinfo=timezone.utc, hour=23, minute=59, second=59)
        
        start_iso = start_dt.isoformat()
        end_iso = end_dt.isoformat()
        
        # å„é€šè²¨ãƒšã‚¢ã«ã¤ã„ã¦å‡¦ç†
        for symbol in symbols:
            self.logger.info(f"ğŸ“Š {symbol} ãƒ‡ãƒ¼ã‚¿åé›†")
            
            # ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†
            for tf in timeframes:
                self._collect_bars(symbol, tf, start_iso, end_iso)
            
            # Tickãƒ‡ãƒ¼ã‚¿åé›†
            if self.config.get('data_collection.ticks.enabled', False):
                self._collect_ticks(symbol, start_iso, end_iso)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self._save_metadata(symbols[0], start_iso, end_iso)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_reports()
        
        # APIçµ±è¨ˆå‡ºåŠ›
        api_stats = self.api_client.get_stats()
        self.logger.info(
            f"âš™ï¸  APIçµ±è¨ˆ: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ{api_stats['total_requests']}å›, "
            f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹{api_stats['avg_response_time_ms']}ms"
        )
        
        self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
    
    def _collect_bars(
        self,
        symbol: str,
        timeframe: str,
        start: str,
        end: str
    ):
        """
        ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†
        
        Args:
            symbol: é€šè²¨ãƒšã‚¢
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            start: é–‹å§‹æ—¥æ™‚ï¼ˆISO8601ï¼‰
            end: çµ‚äº†æ—¥æ™‚ï¼ˆISO8601ï¼‰
        """
        self.logger.info(f"   ğŸ“‚ {timeframe}ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        
        # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        bars = self.api_client.fetch_bars(
            symbol=symbol,
            timeframe=timeframe,
            start=start,
            end=end
        )
        
        if not bars:
            self.logger.warning(f"   âš ï¸  {timeframe}: ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # numpyé…åˆ—ã«å¤‰æ›
        bar_array = self._convert_bars_to_array(bars)
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
        timestamps = bar_array[:, 0].astype(np.int64)
        
        # å“è³ªæ¤œè¨¼
        self._validate_bars(timeframe, timestamps, bar_array)
        
        # HDF5ä¿å­˜
        self.hdf5_writer.write_bar_data(timeframe, bar_array)
        
        # çµ±è¨ˆè¨˜éŒ²
        self.stats['timeframes'][timeframe] = {
            'bars': len(bars),
            'period': {'start': start, 'end': end}
        }
        
        self.logger.info(f"   âœ… {timeframe}: {len(bars)}ä»¶å–å¾—ãƒ»ä¿å­˜å®Œäº†")
    
    def _collect_ticks(
        self,
        symbol: str,
        start: str,
        end: str
    ):
        """
        Tickãƒ‡ãƒ¼ã‚¿åé›†
        
        Args:
            symbol: é€šè²¨ãƒšã‚¢
            start: é–‹å§‹æ—¥æ™‚ï¼ˆISO8601ï¼‰
            end: çµ‚äº†æ—¥æ™‚ï¼ˆISO8601ï¼‰
        """
        self.logger.info(f"   ğŸ“‚ Tickãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        
        # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        ticks = self.api_client.fetch_ticks(
            symbol=symbol,
            start=start,
            end=end
        )
        
        if not ticks:
            self.logger.warning(f"   âš ï¸  Tickãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼
        timestamps = np.array([tick['time'] for tick in ticks])
        self.validator.check_monotonic(timestamps, name="Tick")
        self.validator.check_duplicates(timestamps, name="Tick")
        
        # HDF5ä¿å­˜
        self.hdf5_writer.write_tick_data(ticks)
        
        # çµ±è¨ˆè¨˜éŒ²
        self.stats['ticks'] = {
            'count': len(ticks),
            'period': {'start': start, 'end': end}
        }
        
        self.logger.info(f"   âœ… Tick: {len(ticks)}ä»¶å–å¾—ãƒ»ä¿å­˜å®Œäº†")
    
    def _convert_bars_to_array(self, bars: List[Dict]) -> np.ndarray:
        """
        ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
        
        Args:
            bars: ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        
        Returns:
            numpyé…åˆ— (N, 8) [time, open, high, low, close, tick_volume, spread, real_volume]
        """
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¯int64ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã¯float32ã§ä¿å­˜
        time_col = np.array([bar['time'] for bar in bars], dtype=np.int64).reshape(-1, 1)
        price_cols = np.array([
            [
                bar['open'],
                bar['high'],
                bar['low'],
                bar['close'],
                bar['tick_volume'],
                bar['spread'],
                bar.get('real_volume', 0)
            ]
            for bar in bars
        ], dtype=np.float32)
        
        # çµåˆ (N, 8)
        return np.hstack([time_col, price_cols])
    
    def _validate_bars(
        self,
        timeframe: str,
        timestamps: np.ndarray,
        bar_array: np.ndarray
    ):
        """
        ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼
        
        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            timestamps: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é…åˆ—
            bar_array: ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿é…åˆ—
        """
        # å˜èª¿æ€§ãƒã‚§ãƒƒã‚¯
        if not self.validator.check_monotonic(timestamps, name=timeframe):
            raise RuntimeError(f"{timeframe}: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å˜èª¿æ€§é•å")
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        is_valid, dup_count = self.validator.check_duplicates(timestamps, name=timeframe)
        if not is_valid:
            raise RuntimeError(f"{timeframe}: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é‡è¤‡æ¤œå‡º ({dup_count}ä»¶)")
        
        # æ¬ æç‡ãƒã‚§ãƒƒã‚¯
        expected_interval = self.TF_INTERVALS[timeframe]
        max_gap_ratio = self.config.get('data_collection.quality_thresholds.max_gap_ratio', 0.005)
        
        is_valid, gap_ratio, missing_count = self.validator.check_gap_ratio(
            timestamps,
            expected_interval,
            max_gap_ratio,
            name=timeframe
        )
        
        self.stats['timeframes'][timeframe] = {
            'missing_count': missing_count,
            'missing_rate': gap_ratio
        }
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒã‚§ãƒƒã‚¯
        spreads = bar_array[:, 6]  # spreadåˆ—
        is_valid, neg_count = self.validator.check_spread_validity(spreads, name=timeframe)
        if not is_valid:
            raise RuntimeError(f"{timeframe}: è² ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰æ¤œå‡º ({neg_count}ä»¶)")
        
        # tick_volumeé€£ç¶šã‚¼ãƒ­ãƒã‚§ãƒƒã‚¯
        tick_volumes = bar_array[:, 5]
        max_zero_streak = self.config.get('data_collection.quality_thresholds.max_zero_streak', 120)
        self.validator.check_zero_streak(tick_volumes, max_zero_streak, name=timeframe)
    
    def _save_metadata(
        self,
        symbol: str,
        start: str,
        end: str
    ):
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        
        Args:
            symbol: é€šè²¨ãƒšã‚¢
            start: é–‹å§‹æ—¥æ™‚
            end: çµ‚äº†æ—¥æ™‚
        """
        metadata = {
            'symbol': symbol,
            'start_date': start,
            'end_date': end,
            'api_endpoint': self.config.get('api.endpoint'),
            'collection_time': datetime.now(timezone(timedelta(hours=9))).isoformat(),
            'bar_counts': {
                tf: self.stats['timeframes'].get(tf, {}).get('bars', 0)
                for tf in self.config.get('data_collection.timeframes', [])
            },
            'tick_count': self.stats['ticks'].get('count', 0),
            'file_size_mb': self.hdf5_writer.get_file_size_mb()
        }
        
        self.hdf5_writer.write_metadata(metadata)
    
    def _generate_reports(self):
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        output_dir = Path(self.config.get('output.data_dir', 'data'))
        base_name = self.config.get('output.base_name', 'data_collector')
        
        # JSON ãƒ¬ãƒãƒ¼ãƒˆ
        if self.config.get('output.reports.json', True):
            json_path = output_dir / f"{base_name}_report.json"
            self._generate_json_report(json_path)
        
        # Markdown ãƒ¬ãƒãƒ¼ãƒˆ
        if self.config.get('output.reports.markdown', True):
            md_path = output_dir / f"{base_name}_report.md"
            self._generate_markdown_report(md_path)
    
    def _generate_json_report(self, output_path: Path):
        """JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            'timestamp': datetime.now(timezone(timedelta(hours=9))).isoformat(),
            'process': 'data_collector',
            'version': '1.0',
            'timeframes': self.stats['timeframes'],
            'ticks': self.stats['ticks'],
            'quality': self.validator.get_results(),
            'performance': self.api_client.get_stats()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“„ JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_path.name}")
    
    def _generate_markdown_report(self, output_path: Path):
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        now_jst = datetime.now(timezone(timedelta(hours=9)))
        
        lines = [
            "# ãƒ‡ãƒ¼ã‚¿åé›† å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ\n",
            f"**å®Ÿè¡Œæ—¥æ™‚**: {now_jst.strftime('%Y-%m-%d %H:%M:%S JST')}  \n",
            "**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0\n",
            "\n## ğŸ“Š ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥çµ±è¨ˆ\n",
            "\n| TF | ãƒãƒ¼æ•° | æ¬ ææ•° | æ¬ æç‡ |",
            "\n|----|--------|--------|--------|"
        ]
        
        for tf, stats in self.stats['timeframes'].items():
            bars = stats.get('bars', 0)
            missing = stats.get('missing_count', 0)
            missing_rate = stats.get('missing_rate', 0.0)
            lines.append(
                f"\n| {tf} | {bars:,} | {missing} | {missing_rate*100:.2f}% |"
            )
        
        if self.stats['ticks']:
            lines.extend([
                "\n\n## ğŸ“ˆ Tickãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ\n",
                f"\n- **Tickæ•°**: {self.stats['ticks'].get('count', 0):,}"
            ])
        
        lines.append("\n\n## âœ… æ¤œè¨¼çµæœ\n")
        for key, value in self.validator.get_results().items():
            lines.append(f"\n- {key}: {value}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(''.join(lines))
        
        self.logger.info(f"ğŸ“„ Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_path.name}")
