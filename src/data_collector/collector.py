"""
ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import numpy as np
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Tuple
from pathlib import Path
import json

from .api_client import MT5APIClient
from .validator import DataValidator
from .hdf5_writer import HDF5Writer


class DataCollector:
    """ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®æœŸå¾…é–“éš”ï¼ˆç§’ï¼‰
    TF_INTERVALS = {
        'M1': 60,
        'M5': 300,
        'M15': 900,
        'H1': 3600,
        'H4': 14400
    }

    # ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©
    # é…åˆ—æ§‹é€ : [time, open, high, low, close, tick_volume, spread, real_volume]
    BAR_COLUMNS = {
        'time': 0,
        'open': 1,
        'high': 2,
        'low': 3,
        'close': 4,
        'tick_volume': 5,
        'spread': 6,
        'real_volume': 7
    }

    # ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç·ã‚«ãƒ©ãƒ æ•°
    BAR_COLUMN_COUNT = 8
    
    def __init__(
        self,
        config,
        logger
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            config: ConfigManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            logger: LoggingManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.config = config
        self.logger = logger

        # å…¨è¨­å®šã®åŒ…æ‹¬çš„ãªæ¤œè¨¼
        self.logger.info("ğŸ” è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ä¸­...")
        config.validate_all()
        self.logger.info("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å®Œäº†")
        
        # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.api_client = MT5APIClient(
            endpoint=config.get_required('api.endpoint'),
            api_key=config.get_required('api.api_key'),
            timeout=config.get('api.timeout'),  # Noneè¨±å¯
            logger=logger
        )
        
        # ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        self.validator = DataValidator(logger=logger)
        
        # HDF5ãƒ©ã‚¤ã‚¿ãƒ¼åˆæœŸåŒ–
        output_dir = config.get('output.data_dir', 'data')
        base_name = config.get('output.base_name', 'data_collector')
        output_path = Path(output_dir) / f"{base_name}.h5"
        
        compression = config.get('output.hdf5.compression')
        self.hdf5_writer = HDF5Writer(
            output_path=str(output_path),
            compression=compression,
            logger=logger
        )
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'timeframes': {},
            'ticks': {},
            'quality': {},
            'performance': {}
        }
    
    def collect(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        self.logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if self.config.get('output.backup.enabled', True):
            timestamp_format = self.config.get('output.backup.timestamp_format', '%Y%m%d_%H%M%S')
            self.hdf5_writer.backup_existing(timestamp_format)
        
        # æ—¢å­˜Tickãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ï¼ˆãƒãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†å‰ã«å®Ÿè¡Œï¼‰
        self.hdf5_writer.clear_tick_data()
        
        # åé›†è¨­å®šå–å¾—
        symbols = self.config.get_required('data_collection.symbols')
        timeframes = self.config.get_required('data_collection.timeframes')
        period = self.config.get_required('data_collection.period')
        
        # MT5 API Serverã¯YYYY-MM-DDå½¢å¼ã‚’æœŸå¾…
        start_date = period['start']
        end_date = period['end']
        
        # å„é€šè²¨ãƒšã‚¢ã«ã¤ã„ã¦å‡¦ç†
        for symbol in symbols:
            self.logger.info(f"ğŸ“Š {symbol} ãƒ‡ãƒ¼ã‚¿åé›†")
            
            # ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†
            for tf in timeframes:
                self._collect_bars(symbol, tf, start_date, end_date)
            
            # Tickãƒ‡ãƒ¼ã‚¿åé›†
            if self.config.get('data_collection.ticks.enabled', False):
                self._collect_ticks(symbol, start_date, end_date)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self._save_metadata(symbols[0], start_date, end_date)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_reports()
        
        # APIçµ±è¨ˆå‡ºåŠ›
        api_stats = self.api_client.get_stats()
        self.logger.info(
            f"âš™ï¸  APIçµ±è¨ˆ: ãƒªã‚¯ã‚¨ã‚¹ãƒˆ{api_stats['total_requests']}å›, "
            f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹{api_stats['avg_response_time_ms']}ms"
        )
        
        self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
    
    def _collect_bars(
        self,
        symbol: str,
        timeframe: str,
        start: str,
        end: str
    ) -> None:
        """
        ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†

        Args:
            symbol: é€šè²¨ãƒšã‚¢
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            start: é–‹å§‹æ—¥æ™‚ï¼ˆISO8601ï¼‰
            end: çµ‚äº†æ—¥æ™‚ï¼ˆISO8601ï¼‰
        """
        self.logger.info(f"   ğŸ“‚ {timeframe}ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        
        # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        bars = self.api_client.fetch_bars(
            symbol=symbol,
            timeframe=timeframe,
            start=start,
            end=end
        )
        
        if not bars:
            self.logger.warning(f"   âš ï¸  {timeframe}: ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # numpyé…åˆ—ã«å¤‰æ›
        bar_array = self._convert_bars_to_array(bars)

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
        timestamps = bar_array[:, self.BAR_COLUMNS['time']].astype(np.int64)
        
        # å“è³ªæ¤œè¨¼
        self._validate_bars(timeframe, timestamps, bar_array)
        
        # HDF5ä¿å­˜
        self.hdf5_writer.write_bar_data(timeframe, bar_array)
        
        # çµ±è¨ˆè¨˜éŒ²ï¼ˆåˆå› or æ›´æ–°ï¼‰
        if timeframe not in self.stats['timeframes']:
            self.stats['timeframes'][timeframe] = {}
        
        self.stats['timeframes'][timeframe].update({
            'bars': len(bars),
            'period': {'start': start, 'end': end}
        })
        
        self.logger.info(f"   âœ… {timeframe}: {len(bars)}ä»¶å–å¾—ãƒ»ä¿å­˜å®Œäº†")
    
    def _collect_ticks(
        self,
        symbol: str,
        start: str,
        end: str
    ) -> None:
        """
        Tickãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæœˆå˜ä½ã§åˆ†å‰²å–å¾—ï¼‰

        Args:
            symbol: é€šè²¨ãƒšã‚¢
            start: é–‹å§‹æ—¥æ™‚ï¼ˆISO8601: YYYY-MM-DDï¼‰
            end: çµ‚äº†æ—¥æ™‚ï¼ˆISO8601: YYYY-MM-DDï¼‰
        """
        self.logger.info(f"   ğŸ“‚ Tickãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ï¼ˆæœˆåˆ†å‰²ï¼‰...")
        
        # æœˆç¯„å›²ã‚’ç”Ÿæˆ
        month_ranges = self._generate_month_ranges(start, end)
        
        if not month_ranges:
            self.logger.warning(f"   âš ï¸  æœˆç¯„å›²ãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        self.logger.info(f"   ğŸ“… å–å¾—å¯¾è±¡: {len(month_ranges)}ãƒ¶æœˆåˆ†")
        
        total_ticks = 0
        
        # æœˆã”ã¨ã«å–å¾—ãƒ»ä¿å­˜
        for i, (month_start, month_end) in enumerate(month_ranges, 1):
            self.logger.info(f"   ğŸ”„ [{i}/{len(month_ranges)}] {month_start} ~ {month_end}")
            
            # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            ticks = self.api_client.fetch_ticks(
                symbol=symbol,
                start=month_start,
                end=month_end
            )
            
            if not ticks:
                self.logger.warning(f"      âš ï¸  ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                continue
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ¤œè¨¼ï¼ˆãƒŸãƒªç§’ç²¾åº¦ï¼‰
            timestamps_msc = np.array([tick['time_msc'] for tick in ticks])
            self.validator.check_monotonic(timestamps_msc, name=f"Tick[{month_start}]")
            self.validator.check_duplicates(timestamps_msc, name=f"Tick[{month_start}]")
            
            # HDF5è¿½è¨˜ä¿å­˜
            self.hdf5_writer.append_tick_data(ticks)
            
            total_ticks += len(ticks)
            self.logger.info(f"      âœ… {len(ticks):,}ä»¶ä¿å­˜ï¼ˆç´¯è¨ˆ: {total_ticks:,}ä»¶ï¼‰")
        
        # çµ±è¨ˆè¨˜éŒ²
        self.stats['ticks'] = {
            'count': total_ticks,
            'months': len(month_ranges),
            'period': {'start': start, 'end': end}
        }
        
        self.logger.info(f"   âœ… Tick: å…¨{total_ticks:,}ä»¶å–å¾—ãƒ»ä¿å­˜å®Œäº†")
    
    def _generate_month_ranges(
        self,
        start: str,
        end: str
    ) -> List[Tuple[str, str]]:
        """
        æœˆå˜ä½ã®ç¯„å›²ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            start: é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDï¼‰
            end: çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDï¼‰
        
        Returns:
            [(month_start, month_end), ...] ã®ãƒªã‚¹ãƒˆ
        """
        from datetime import datetime, timedelta
        from dateutil.relativedelta import relativedelta
        
        try:
            start_dt = datetime.strptime(start, '%Y-%m-%d')
            end_dt = datetime.strptime(end, '%Y-%m-%d')
        except ValueError as e:
            self.logger.error(f"æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
        if start_dt > end_dt:
            self.logger.error(f"é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã§ã™: {start} > {end}")
            return []
        
        ranges = []
        current = start_dt
        
        while current <= end_dt:
            # æœˆåˆ
            month_start = current.replace(day=1)
            
            # ç¿Œæœˆåˆ - 1æ—¥ = æœˆæœ«
            next_month = month_start + relativedelta(months=1)
            month_end = next_month - timedelta(days=1)
            
            # æœ€çµ‚æœˆã¯æŒ‡å®šçµ‚äº†æ—¥ã§ã‚¯ãƒªãƒƒãƒ—
            if month_end > end_dt:
                month_end = end_dt
            
            # ISO8601å½¢å¼ã«å¤‰æ›
            ranges.append((
                month_start.strftime('%Y-%m-%d'),
                month_end.strftime('%Y-%m-%d')
            ))
            
            # æ¬¡ã®æœˆã¸
            current = next_month
        
        return ranges
    
    def _convert_bars_to_array(self, bars: List[Dict]) -> np.ndarray:
        """
        ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
        
        Args:
            bars: ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        
        Returns:
            numpyé…åˆ— (N, 8) [time, open, high, low, close, tick_volume, spread, real_volume]
        """
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¯int64ã€ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜ã¯float32ã§ä¿å­˜
        time_col = np.array([bar['time'] for bar in bars], dtype=np.int64).reshape(-1, 1)
        price_cols = np.array([
            [
                bar['open'],
                bar['high'],
                bar['low'],
                bar['close'],
                bar['tick_volume'],
                bar['spread'],
                bar.get('real_volume', 0)
            ]
            for bar in bars
        ], dtype=np.float32)
        
        # çµåˆ (N, 8)
        return np.hstack([time_col, price_cols])
    
    def _validate_bars(
        self,
        timeframe: str,
        timestamps: np.ndarray,
        bar_array: np.ndarray
    ) -> None:
        """
        ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼

        Args:
            timeframe: ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ 
            timestamps: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é…åˆ—
            bar_array: ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿é…åˆ—
        """
        # å˜èª¿æ€§ãƒã‚§ãƒƒã‚¯
        if not self.validator.check_monotonic(timestamps, name=timeframe):
            # validatorã«è©³ç´°æƒ…å ±ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
            violation_info = self.validator.validation_results.get(f'{timeframe}_monotonic_violations', {})
            raise RuntimeError(
                f"{timeframe}: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å˜èª¿æ€§é•å - "
                f"è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ (é•åæ•°: {violation_info.get('count', 'ä¸æ˜')}ä»¶)"
            )

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        is_valid, dup_count = self.validator.check_duplicates(timestamps, name=timeframe)
        if not is_valid:
            # validatorã«è©³ç´°æƒ…å ±ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
            dup_info = self.validator.validation_results.get(f'{timeframe}_duplicates', {})
            if isinstance(dup_info, dict):
                raise RuntimeError(
                    f"{timeframe}: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é‡è¤‡æ¤œå‡º - "
                    f"é‡è¤‡å€¤{dup_info.get('unique_duplicate_values', '?')}å€‹, "
                    f"ç·é‡è¤‡æ•°{dup_info.get('count', dup_count)}ä»¶"
                )
            else:
                raise RuntimeError(f"{timeframe}: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é‡è¤‡æ¤œå‡º ({dup_count}ä»¶)")
        
        # æ¬ æç‡ãƒã‚§ãƒƒã‚¯
        expected_interval = self.TF_INTERVALS[timeframe]
        max_gap_ratio = self.config.get('data_collection.quality_thresholds.max_gap_ratio', 0.005)
        
        is_valid, gap_ratio, missing_count = self.validator.check_gap_ratio(
            timestamps,
            expected_interval,
            max_gap_ratio,
            name=timeframe
        )
        
        # çµ±è¨ˆè¨˜éŒ²ï¼ˆåˆå› or æ›´æ–°ï¼‰
        if timeframe not in self.stats['timeframes']:
            self.stats['timeframes'][timeframe] = {}
        
        self.stats['timeframes'][timeframe].update({
            'missing_count': missing_count,
            'missing_rate': gap_ratio
        })
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ãƒã‚§ãƒƒã‚¯
        spreads = bar_array[:, self.BAR_COLUMNS['spread']]
        is_valid, neg_count = self.validator.check_spread_validity(spreads, name=timeframe)
        if not is_valid:
            # validatorã«è©³ç´°æƒ…å ±ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
            spread_info = self.validator.validation_results.get(f'{timeframe}_negative_spread', {})
            if isinstance(spread_info, dict):
                raise RuntimeError(
                    f"{timeframe}: è² ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰æ¤œå‡º - "
                    f"ç·æ•°{spread_info.get('count', neg_count)}ä»¶, "
                    f"æœ€åˆã®è² å€¤index={spread_info.get('first_negative_index', '?')}"
                )
            else:
                raise RuntimeError(f"{timeframe}: è² ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰æ¤œå‡º ({neg_count}ä»¶)")

        # tick_volumeé€£ç¶šã‚¼ãƒ­ãƒã‚§ãƒƒã‚¯
        tick_volumes = bar_array[:, self.BAR_COLUMNS['tick_volume']]
        max_zero_streak = self.config.get('data_collection.quality_thresholds.max_zero_streak', 120)
        self.validator.check_zero_streak(tick_volumes, max_zero_streak, name=timeframe)
    
    def _save_metadata(
        self,
        symbol: str,
        start: str,
        end: str
    ) -> None:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜

        Args:
            symbol: é€šè²¨ãƒšã‚¢
            start: é–‹å§‹æ—¥æ™‚
            end: çµ‚äº†æ—¥æ™‚
        """
        metadata = {
            'symbol': symbol,
            'start_date': start,
            'end_date': end,
            'api_endpoint': self.config.get('api.endpoint'),
            'collection_time': datetime.now(timezone(timedelta(hours=9))).isoformat(),
            'bar_counts': {
                tf: self.stats['timeframes'].get(tf, {}).get('bars', 0)
                for tf in self.config.get('data_collection.timeframes', [])
            },
            'tick_count': self.stats['ticks'].get('count', 0),
            'file_size_mb': self.hdf5_writer.get_file_size_mb()
        }
        
        self.hdf5_writer.write_metadata(metadata)
    
    def _generate_reports(self) -> None:
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        output_dir = Path(self.config.get('output.data_dir', 'data'))
        base_name = self.config.get('output.base_name', 'data_collector')
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
        backup_enabled = self.config.get('output.backup.enabled', True)
        timestamp_format = self.config.get('output.backup.timestamp_format', '%Y%m%d_%H%M%S')
        
        # JSON ãƒ¬ãƒãƒ¼ãƒˆ
        if self.config.get('output.reports.json', True):
            json_path = output_dir / f"{base_name}_report.json"
            if backup_enabled:
                self._backup_report_file(json_path, timestamp_format)
            self._generate_json_report(json_path)
        
        # Markdown ãƒ¬ãƒãƒ¼ãƒˆ
        if self.config.get('output.reports.markdown', True):
            md_path = output_dir / f"{base_name}_report.md"
            if backup_enabled:
                self._backup_report_file(md_path, timestamp_format)
            self._generate_markdown_report(md_path)
    
    def _backup_report_file(self, file_path: Path, timestamp_format: str) -> None:
        """
        æ—¢å­˜ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

        Args:
            file_path: ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            timestamp_format: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        if not file_path.exists():
            return
        
        # JSTæ—¥æ™‚ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ
        jst_tz = timezone(timedelta(hours=9))
        mtime = datetime.fromtimestamp(
            file_path.stat().st_mtime,
            tz=jst_tz
        )
        timestamp_str = mtime.strftime(timestamp_format)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        backup_name = f"{timestamp_str}_{file_path.name}"
        backup_path = file_path.parent / backup_name
        
        # ãƒªãƒãƒ¼ãƒ 
        file_path.rename(backup_path)
        self.logger.info(f"ğŸ“¦ æ—¢å­˜ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_name}")
    
    def _generate_json_report(self, output_path: Path) -> None:
        """JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            'timestamp': datetime.now(timezone(timedelta(hours=9))).isoformat(),
            'process': 'data_collector',
            'version': '1.0',
            'timeframes': self.stats['timeframes'],
            'ticks': self.stats['ticks'],
            'quality': self.validator.get_results(),
            'performance': self.api_client.get_stats()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ“„ JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_path.name}")
    
    def _generate_markdown_report(self, output_path: Path) -> None:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        now_jst = datetime.now(timezone(timedelta(hours=9)))
        
        lines = [
            "# ãƒ‡ãƒ¼ã‚¿åé›† å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ\n",
            f"**å®Ÿè¡Œæ—¥æ™‚**: {now_jst.strftime('%Y-%m-%d %H:%M:%S JST')}  \n",
            "**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0\n",
            "\n## ğŸ“Š ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥çµ±è¨ˆ\n",
            "\n| TF | ãƒãƒ¼æ•° | æ¬ ææ•° | æ¬ æç‡ |",
            "\n|----|--------|--------|--------|"
        ]
        
        for tf, stats in self.stats['timeframes'].items():
            bars = stats.get('bars', 0)
            missing = stats.get('missing_count', 0)
            missing_rate = stats.get('missing_rate', 0.0)
            lines.append(
                f"\n| {tf} | {bars:,} | {missing} | {missing_rate*100:.2f}% |"
            )
        
        if self.stats['ticks']:
            lines.extend([
                "\n\n## ğŸ“ˆ Tickãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ\n",
                f"\n- **Tickæ•°**: {self.stats['ticks'].get('count', 0):,}"
            ])
        
        lines.append("\n\n## âœ… æ¤œè¨¼çµæœ\n")
        for key, value in self.validator.get_results().items():
            lines.append(f"\n- {key}: {value}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(''.join(lines))
        
        self.logger.info(f"ğŸ“„ Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {output_path.name}")
