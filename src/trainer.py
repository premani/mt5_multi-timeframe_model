"""
trainer.py
Phase 0: å­¦ç¿’å‡¦ç†ï¼ˆæœ€å°æ§‹æˆï¼‰

ç›®çš„: ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ç‰¹å¾´é‡ã‹ã‚‰ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
å…¥åŠ›: data/preprocessor.h5ï¼ˆå‰å‡¦ç†æ¸ˆã¿ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼‰
å‡ºåŠ›: models/trainer_model.pthï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼‰
"""

import os
import sys
import time
import json
import yaml
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple, List

import numpy as np
import h5py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_absolute_error, mean_squared_error, r2_score

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).parent))
from utils.logging_manager import LoggingManager


class MultiTFDataset(Dataset):
    """ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, sequences: Dict[str, np.ndarray], labels: Dict[str, np.ndarray]):
        """
        Args:
            sequences: {TFå: (N, seq_len, features)}
            labels: {"direction": (N,), "magnitude": (N,)}
        """
        self.sequences = sequences
        self.labels = labels
        self.n_samples = len(labels["direction"])
        
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å–å¾—
        seq_dict = {
            tf: torch.FloatTensor(seq[idx])
            for tf, seq in self.sequences.items()
        }
        
        # ãƒ©ãƒ™ãƒ«å–å¾—
        label_dict = {
            "direction": torch.LongTensor([self.labels["direction"][idx]])[0],
            "magnitude": torch.FloatTensor([self.labels["magnitude"][idx]])[0]
        }
        
        return seq_dict, label_dict


class TFEncoder(nn.Module):
    """ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆLSTMï¼‰"""
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, seq_len, features)
        Returns:
            output: (batch, seq_len, hidden_size)
        """
        output, _ = self.lstm(x)
        return output


class AttentionFusion(nn.Module):
    """ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³èåˆå±¤"""
    
    def __init__(self, hidden_size: int, num_heads: int, dropout: float):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.norm = nn.LayerNorm(hidden_size)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (batch, seq_len, hidden_size)
        Returns:
            output: (batch, hidden_size)
        """
        # Self-attention
        attn_out, _ = self.attention(x, x, x)
        x = self.norm(x + attn_out)
        
        # æœ€çµ‚çŠ¶æ…‹ã‚’å–å¾—ï¼ˆæœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        return x[:, -1, :]


class MultiTFModel(nn.Module):
    """ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ LSTMãƒ¢ãƒ‡ãƒ«ï¼ˆPhase 0: è¶…ç°¡ç•¥ç‰ˆï¼‰"""
    
    def __init__(self, config: Dict):
        super().__init__()
        
        # è¨­å®š
        lstm_cfg = config["model"]["lstm"]
        
        hidden_size = lstm_cfg["hidden_size"]
        num_layers = lstm_cfg["num_layers"]
        dropout = lstm_cfg["dropout"]
        
        # TFåˆ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆç‰¹å¾´é‡æ•°ã¯å‹•çš„ã«è¨­å®šï¼‰
        self.encoders = nn.ModuleDict()
        
        # TFé‡ã¿ï¼ˆPhase 0: å‡ç­‰ï¼‰
        self.tf_weights = nn.Parameter(torch.ones(5) / 5, requires_grad=False)
        
        # å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
        self.direction_head = nn.Sequential(
            nn.Linear(hidden_size, 3),  # ç›´æ¥å‡ºåŠ›
        )
        
        self.magnitude_head = nn.Sequential(
            nn.Linear(hidden_size, 1),  # ç›´æ¥å‡ºåŠ›
            nn.Sigmoid()  # 0-1ã«æ­£è¦åŒ–å¾Œã€ã‚¹ã‚±ãƒ¼ãƒ«
        )
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯åˆæœŸåŒ–æ™‚ã«å‹•çš„ä½œæˆ
        self._encoder_config = {
            "hidden_size": hidden_size,
            "num_layers": num_layers,
            "dropout": dropout
        }
        
    def add_encoder(self, tf_name: str, input_size: int):
        """TFåˆ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’å‹•çš„è¿½åŠ """
        self.encoders[tf_name] = TFEncoder(
            input_size=input_size,
            hidden_size=self._encoder_config["hidden_size"],
            num_layers=self._encoder_config["num_layers"],
            dropout=self._encoder_config["dropout"]
        )
        # XavieråˆæœŸåŒ–
        for name, param in self.encoders[tf_name].named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)
        
    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Args:
            x: {TFå: (batch, seq_len, features)}
        Returns:
            output: {"direction": (batch, 3), "magnitude": (batch, 1)}
        """
        # TFåˆ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€çµ‚éš ã‚ŒçŠ¶æ…‹ã®ã¿å–å¾—ï¼‰
        encoded_states = []
        for tf_name in ["M1", "M5", "M15", "H1", "H4"]:  # é †åºå›ºå®š
            if tf_name in x and tf_name in self.encoders:
                seq = x[tf_name]
                # LSTMã®å…¨å‡ºåŠ›ã‚’å–å¾—
                encoded = self.encoders[tf_name](seq)  # (batch, seq_len, hidden)
                # æœ€çµ‚ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿
                final_state = encoded[:, -1, :]  # (batch, hidden)
                encoded_states.append(final_state)
        
        # Phase 0: å˜ç´”ãªå¹³å‡èåˆï¼ˆé‡ã¿ä»˜ãï¼‰
        stacked = torch.stack(encoded_states, dim=0)  # (num_tf, batch, hidden)
        weights = self.tf_weights.view(-1, 1, 1)  # (num_tf, 1, 1)
        fused = (stacked * weights).sum(dim=0)  # (batch, hidden)
        
        # å‡ºåŠ›
        direction_logits = self.direction_head(fused)  # (batch, 3)
        magnitude_raw = self.magnitude_head(fused)  # (batch, 1), range [0, 1]
        magnitude = 0.5 + magnitude_raw.squeeze(-1) * 4.5  # [0.5, 5.0] pips
        
        return {
            "direction": direction_logits,
            "magnitude": magnitude
        }


class Trainer:
    """å­¦ç¿’ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)
        
        # ãƒ­ã‚°åˆæœŸåŒ–
        self.logger = LoggingManager(
            name="trainer",
            log_dir="logs",
            level=self.config["logging"]["level"]
        )
        
        self.logger.info("ğŸš€ å­¦ç¿’å‡¦ç†é–‹å§‹ï¼ˆPhase 0ï¼‰")
        self.logger.info(f"   è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {config_path}")
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        self.device = self._setup_device()
        
        # å†ç¾æ€§è¨­å®š
        self._setup_reproducibility()
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.data = self._load_data()
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        self.model = self._build_model()
        
        # æå¤±é–¢æ•°ãƒ»æœ€é©åŒ–è¨­å®š
        self.criterion = self._setup_loss()
        self.optimizer = self._setup_optimizer()
        self.scheduler = self._setup_scheduler()
        
        # å­¦ç¿’çŠ¶æ…‹
        self.best_val_loss = float("inf")
        self.patience_counter = 0
        
    def _setup_device(self) -> torch.device:
        """ãƒ‡ãƒã‚¤ã‚¹è¨­å®š"""
        if self.config["device"]["use_cuda"] and torch.cuda.is_available():
            device_id = self.config["device"]["device_id"]
            device = torch.device(f"cuda:{device_id}")
            self.logger.info(f"   ãƒ‡ãƒã‚¤ã‚¹: CUDA (GPU {device_id})")
            self.logger.info(f"   GPUå: {torch.cuda.get_device_name(device_id)}")
        else:
            device = torch.device("cpu")
            self.logger.info("   ãƒ‡ãƒã‚¤ã‚¹: CPU")
        return device
    
    def _setup_reproducibility(self):
        """å†ç¾æ€§è¨­å®š"""
        seed = self.config["reproducibility"]["seed"]
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        if self.config["reproducibility"]["deterministic"]:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            self.logger.info(f"   å†ç¾æ€§: æœ‰åŠ¹ï¼ˆseed={seed}ï¼‰")
    
    def _load_data(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        self.logger.info("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
        input_file = self.config["io"]["input_file"]
        
        with h5py.File(input_file, "r") as f:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            metadata = json.loads(f["metadata"][()])
            self.logger.info(f"   å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_file}")
            self.logger.info(f"   ç”Ÿæˆæ—¥æ™‚: {metadata['processing_timestamp']}")
            self.logger.info(f"   ç‰¹å¾´é‡æ•°: {metadata['filter_stats']['initial']} â†’ {metadata['filter_stats']['final']}")
            
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            sequences = {}
            for tf_name in ["M1", "M5", "M15", "H1", "H4"]:
                if f"sequences/{tf_name}" in f:
                    sequences[tf_name] = f[f"sequences/{tf_name}"][:]
                    self.logger.info(f"   {tf_name}: {sequences[tf_name].shape}")
            
            # æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæ¨è«–æ™‚å¿…è¦ï¼‰
            scaler_params = json.loads(f["scaler_params"][()])
            
            # Phase 0: ãƒ€ãƒŸãƒ¼ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆä»®å®Ÿè£…ï¼‰
            # TODO: å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
            # å„TFã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒç•°ãªã‚‹ãŸã‚ã€æœ€å°æ•°ã«æƒãˆã‚‹
            min_samples = min(len(seq) for seq in sequences.values())
            
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã«æƒãˆã‚‹
            sequences = {tf: seq[:min_samples] for tf, seq in sequences.items()}
            
            n_samples = min_samples
            labels = {
                "direction": np.random.randint(0, 3, n_samples),  # UP/DOWN/NEUTRAL
                "magnitude": np.random.uniform(0.5, 5.0, n_samples)  # pips
            }
            self.logger.warning("âš ï¸  Phase 0: ãƒ€ãƒŸãƒ¼ãƒ©ãƒ™ãƒ«ä½¿ç”¨ä¸­ï¼ˆå®Ÿè£…å¾…ã¡ï¼‰")
            self.logger.info(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ {min_samples} ã«çµ±ä¸€")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        train_data, val_data, test_data = self._split_data(sequences, labels)
        
        return {
            "train": train_data,
            "val": val_data,
            "test": test_data,
            "scaler_params": scaler_params,
            "metadata": metadata
        }
    
    def _split_data(self, sequences: Dict, labels: Dict) -> Tuple:
        """ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆæ™‚ç³»åˆ—é †åºç¶­æŒï¼‰"""
        n_samples = len(labels["direction"])
        train_ratio = self.config["data_split"]["train_ratio"]
        val_ratio = self.config["data_split"]["val_ratio"]
        
        train_end = int(n_samples * train_ratio)
        val_end = int(n_samples * (train_ratio + val_ratio))
        
        # Train
        train_sequences = {tf: seq[:train_end] for tf, seq in sequences.items()}
        train_labels = {k: v[:train_end] for k, v in labels.items()}
        
        # Val
        val_sequences = {tf: seq[train_end:val_end] for tf, seq in sequences.items()}
        val_labels = {k: v[train_end:val_end] for k, v in labels.items()}
        
        # Test
        test_sequences = {tf: seq[val_end:] for tf, seq in sequences.items()}
        test_labels = {k: v[val_end:] for k, v in labels.items()}
        
        self.logger.info(f"   Train: {len(train_labels['direction'])} samples")
        self.logger.info(f"   Val:   {len(val_labels['direction'])} samples")
        self.logger.info(f"   Test:  {len(test_labels['direction'])} samples")
        
        return (train_sequences, train_labels), (val_sequences, val_labels), (test_sequences, test_labels)
    
    def _build_model(self) -> nn.Module:
        """ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        self.logger.info("ğŸ—ï¸  ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰")
        
        model = MultiTFModel(self.config)
        
        # TFåˆ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’å‹•çš„è¿½åŠ 
        train_sequences = self.data["train"][0]
        for tf_name, seq in train_sequences.items():
            input_size = seq.shape[2]  # ç‰¹å¾´é‡æ•°
            model.add_encoder(tf_name, input_size)
            self.logger.info(f"   {tf_name} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€: input_size={input_size}")
        
        # ãƒ‡ãƒã‚¤ã‚¹ã¸ç§»å‹•
        model = model.to(self.device)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.logger.info(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        self.logger.info(f"   å­¦ç¿’å¯èƒ½: {trainable_params:,}")
        
        return model
    
    def _setup_loss(self) -> Dict:
        """æå¤±é–¢æ•°è¨­å®š"""
        self.logger.info("âš™ï¸  æå¤±é–¢æ•°è¨­å®š")
        
        # Direction: CrossEntropy
        direction_criterion = nn.CrossEntropyLoss()
        
        # Magnitude: Huber Loss
        huber_delta = self.config["loss"]["huber_delta"]
        magnitude_criterion = nn.HuberLoss(delta=huber_delta)
        
        self.logger.info(f"   Direction: CrossEntropyLoss")
        self.logger.info(f"   Magnitude: HuberLoss (Î´={huber_delta})")
        
        return {
            "direction": direction_criterion,
            "magnitude": magnitude_criterion
        }
    
    def _setup_optimizer(self) -> optim.Optimizer:
        """æœ€é©åŒ–è¨­å®š"""
        self.logger.info("âš™ï¸  æœ€é©åŒ–è¨­å®š")
        
        optimizer_name = self.config["training"]["optimizer"].lower()
        lr = self.config["training"]["learning_rate"]
        weight_decay = self.config["training"]["weight_decay"]
        
        if optimizer_name == "adam":
            optimizer = optim.Adam(
                self.model.parameters(),
                lr=lr,
                weight_decay=weight_decay
            )
        elif optimizer_name == "adamw":
            optimizer = optim.AdamW(
                self.model.parameters(),
                lr=lr,
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"æœªå¯¾å¿œã®æœ€é©åŒ–æ‰‹æ³•: {optimizer_name}")
        
        self.logger.info(f"   Optimizer: {optimizer_name.upper()}")
        self.logger.info(f"   Learning Rate: {lr}")
        self.logger.info(f"   Weight Decay: {weight_decay}")
        
        return optimizer
    
    def _setup_scheduler(self):
        """å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®š"""
        if not self.config["training"]["lr_scheduler"]["enabled"]:
            return None
        
        self.logger.info("âš™ï¸  å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©è¨­å®š")
        
        scheduler_type = self.config["training"]["lr_scheduler"]["type"]
        
        if scheduler_type == "reduce_on_plateau":
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode="min",
                factor=self.config["training"]["lr_scheduler"]["factor"],
                patience=self.config["training"]["lr_scheduler"]["patience"]
            )
            self.logger.info(f"   Type: ReduceLROnPlateau")
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©: {scheduler_type}")
        
        return scheduler
    
    def train(self):
        """å­¦ç¿’å®Ÿè¡Œ"""
        self.logger.info("ğŸ”„ å­¦ç¿’é–‹å§‹")
        
        start_time = time.time()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_dataset = MultiTFDataset(*self.data["train"])
        val_dataset = MultiTFDataset(*self.data["val"])
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config["training"]["batch_size"],
            shuffle=not self.config["data_split"]["shuffle"],  # Phase 0: æ™‚ç³»åˆ—é †åºç¶­æŒ
            num_workers=self.config["dataloader"]["num_workers"],
            pin_memory=self.config["dataloader"]["pin_memory"]
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config["training"]["batch_size"],
            shuffle=False,
            num_workers=self.config["dataloader"]["num_workers"],
            pin_memory=self.config["dataloader"]["pin_memory"]
        )
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—
        epochs = self.config["training"]["epochs"]
        for epoch in range(1, epochs + 1):
            self.logger.info(f"\nğŸ“Š Epoch {epoch}/{epochs}")
            
            # å­¦ç¿’
            train_loss, train_metrics = self._train_epoch(train_loader)
            
            # æ¤œè¨¼
            val_loss, val_metrics = self._validate_epoch(val_loader)
            
            # ãƒ­ã‚°å‡ºåŠ›
            self._log_epoch_results(epoch, train_loss, train_metrics, val_loss, val_metrics)
            
            # å­¦ç¿’ç‡èª¿æ•´
            if self.scheduler:
                self.scheduler.step(val_loss)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆãƒ™ã‚¹ãƒˆã®ã¿ï¼‰
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self._save_model(epoch, val_loss, val_metrics)
            else:
                self.patience_counter += 1
            
            # æ—©æœŸåœæ­¢
            if self.patience_counter >= self.config["training"]["early_stopping_patience"]:
                self.logger.info(f"â¹ï¸  æ—©æœŸåœæ­¢ï¼ˆæ”¹å–„ãªã—é€£ç¶š {self.patience_counter} ã‚¨ãƒãƒƒã‚¯ï¼‰")
                break
        
        elapsed = time.time() - start_time
        self.logger.info(f"\nâœ… å­¦ç¿’å®Œäº†ï¼ˆ{elapsed:.2f}ç§’ï¼‰")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_report()
    
    def _train_epoch(self, loader: DataLoader) -> Tuple[float, Dict]:
        """1ã‚¨ãƒãƒƒã‚¯å­¦ç¿’"""
        self.model.train()
        
        total_loss = 0.0
        all_direction_preds = []
        all_direction_labels = []
        all_magnitude_preds = []
        all_magnitude_labels = []
        
        loss_weights = self.config["loss"]["weights"]
        
        for batch_idx, (sequences, labels) in enumerate(loader):
            # ãƒ‡ãƒã‚¤ã‚¹ã¸ç§»å‹•
            sequences = {tf: seq.to(self.device) for tf, seq in sequences.items()}
            labels = {k: v.to(self.device) for k, v in labels.items()}
            
            # Forward
            outputs = self.model(sequences)
            
            # Lossè¨ˆç®—
            direction_loss = self.criterion["direction"](outputs["direction"], labels["direction"])
            magnitude_loss = self.criterion["magnitude"](outputs["magnitude"], labels["magnitude"])
            
            loss = (
                loss_weights["direction"] * direction_loss +
                loss_weights["magnitude"] * magnitude_loss
            )
            
            # Backward
            self.optimizer.zero_grad()
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            if self.config["training"]["gradient_clipping"]["enabled"]:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config["training"]["gradient_clipping"]["max_norm"]
                )
            
            self.optimizer.step()
            
            # çµ±è¨ˆ
            total_loss += loss.item()
            
            direction_preds = outputs["direction"].argmax(dim=1).cpu().numpy()
            all_direction_preds.extend(direction_preds)
            all_direction_labels.extend(labels["direction"].cpu().numpy())
            
            all_magnitude_preds.extend(outputs["magnitude"].cpu().detach().numpy())
            all_magnitude_labels.extend(labels["magnitude"].cpu().numpy())
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        avg_loss = total_loss / len(loader)
        
        # NaNãƒã‚§ãƒƒã‚¯
        magnitude_preds_array = np.array(all_magnitude_preds)
        if np.isnan(magnitude_preds_array).any():
            self.logger.warning(f"   âš ï¸  Magnitudeäºˆæ¸¬ã«NaNæ¤œå‡º: {np.isnan(magnitude_preds_array).sum()} / {len(magnitude_preds_array)}")
        metrics = self._compute_metrics(
            np.array(all_direction_preds),
            np.array(all_direction_labels),
            np.array(all_magnitude_preds),
            np.array(all_magnitude_labels)
        )
        
        return avg_loss, metrics
    
    def _validate_epoch(self, loader: DataLoader) -> Tuple[float, Dict]:
        """1ã‚¨ãƒãƒƒã‚¯æ¤œè¨¼"""
        self.model.eval()
        
        total_loss = 0.0
        all_direction_preds = []
        all_direction_labels = []
        all_magnitude_preds = []
        all_magnitude_labels = []
        
        loss_weights = self.config["loss"]["weights"]
        
        with torch.no_grad():
            for sequences, labels in loader:
                # ãƒ‡ãƒã‚¤ã‚¹ã¸ç§»å‹•
                sequences = {tf: seq.to(self.device) for tf, seq in sequences.items()}
                labels = {k: v.to(self.device) for k, v in labels.items()}
                
                # Forward
                outputs = self.model(sequences)
                
                # Lossè¨ˆç®—
                direction_loss = self.criterion["direction"](outputs["direction"], labels["direction"])
                magnitude_loss = self.criterion["magnitude"](outputs["magnitude"], labels["magnitude"])
                
                loss = (
                    loss_weights["direction"] * direction_loss +
                    loss_weights["magnitude"] * magnitude_loss
                )
                
                # çµ±è¨ˆ
                total_loss += loss.item()
                
                direction_preds = outputs["direction"].argmax(dim=1).cpu().numpy()
                all_direction_preds.extend(direction_preds)
                all_direction_labels.extend(labels["direction"].cpu().numpy())
                
                all_magnitude_preds.extend(outputs["magnitude"].cpu().numpy())
                all_magnitude_labels.extend(labels["magnitude"].cpu().numpy())
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        avg_loss = total_loss / len(loader)
        metrics = self._compute_metrics(
            np.array(all_direction_preds),
            np.array(all_direction_labels),
            np.array(all_magnitude_preds),
            np.array(all_magnitude_labels)
        )
        
        return avg_loss, metrics
    
    def _compute_metrics(
        self,
        direction_preds: np.ndarray,
        direction_labels: np.ndarray,
        magnitude_preds: np.ndarray,
        magnitude_labels: np.ndarray
    ) -> Dict:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        # Direction
        direction_acc = accuracy_score(direction_labels, direction_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            direction_labels, direction_preds, average="macro", zero_division=0
        )
        
        # Magnitudeï¼ˆNaNé™¤å¤–ï¼‰
        nan_mask = ~np.isnan(magnitude_preds)
        if nan_mask.sum() > 0:
            clean_preds = magnitude_preds[nan_mask]
            clean_labels = magnitude_labels[nan_mask]
            
            mae = mean_absolute_error(clean_labels, clean_preds)
            rmse = np.sqrt(mean_squared_error(clean_labels, clean_preds))
            r2 = r2_score(clean_labels, clean_preds)
        else:
            # å…¨ã¦NaNã®å ´åˆ
            mae = float('nan')
            rmse = float('nan')
            r2 = float('nan')
        
        return {
            "direction_accuracy": direction_acc,
            "direction_precision": precision,
            "direction_recall": recall,
            "direction_f1": f1,
            "magnitude_mae": mae,
            "magnitude_rmse": rmse,
            "magnitude_r2": r2
        }
    
    def _log_epoch_results(
        self,
        epoch: int,
        train_loss: float,
        train_metrics: Dict,
        val_loss: float,
        val_metrics: Dict
    ):
        """ã‚¨ãƒãƒƒã‚¯çµæœãƒ­ã‚°"""
        self.logger.info(f"   Train Loss: {train_loss:.4f}")
        self.logger.info(f"      Direction Acc: {train_metrics['direction_accuracy']:.4f}")
        self.logger.info(f"      Magnitude MAE: {train_metrics['magnitude_mae']:.4f} pips")
        
        self.logger.info(f"   Val Loss: {val_loss:.4f}")
        self.logger.info(f"      Direction Acc: {val_metrics['direction_accuracy']:.4f}")
        self.logger.info(f"      Magnitude MAE: {val_metrics['magnitude_mae']:.4f} pips")
        
        if val_loss < self.best_val_loss:
            self.logger.info(f"   ğŸ’¾ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°")
    
    def _save_model(self, epoch: int, val_loss: float, val_metrics: Dict):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        output_path = self.config["io"]["output_model"]
        
        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿
        save_dict = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "val_loss": val_loss,
            "val_metrics": val_metrics,
            "config": self.config,
            "scaler_params": self.data["scaler_params"]
        }
        
        # ä¿å­˜
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(save_dict, output_path)
    
    def _generate_report(self):
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.logger.info("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        
        # TODO: è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆå®Ÿè£…
        self.logger.info("   âš ï¸  Phase 0: ç°¡æ˜“ãƒ¬ãƒãƒ¼ãƒˆï¼ˆè©³ç´°å®Ÿè£…å¾…ã¡ï¼‰")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    config_path = "config/trainer.yaml"
    
    if not os.path.exists(config_path):
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        print("   config/trainer.template.yaml ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„")
        return
    
    trainer = Trainer(config_path)
    trainer.train()


if __name__ == "__main__":
    main()
