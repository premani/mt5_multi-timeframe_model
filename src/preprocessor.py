#!/usr/bin/env python3
"""
å‰å‡¦ç†ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç¬¬3æ®µéš: å‰å‡¦ç†ï¼ˆæ­£è¦åŒ–ãƒ»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ï¼‰
- å…¥åŠ›: data/feature_calculator.h5ï¼ˆè¨ˆç®—æ¸ˆã¿ç‰¹å¾´é‡ï¼‰
- å‡ºåŠ›: data/preprocessor.h5ï¼ˆå­¦ç¿’å¯èƒ½å½¢å¼ï¼‰
- ãƒ¬ãƒãƒ¼ãƒˆ: data/preprocessor_report.{json,md}
"""

import sys
import time
import json
import h5py
import yaml
import hashlib
import logging
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Tuple

import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))


def setup_logging(config: Dict[str, Any]) -> logging.Logger:
    """ãƒ­ã‚°è¨­å®šï¼ˆJSTè¡¨ç¤ºï¼‰"""
    # JSTç”¨ã®ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
    class JSTFormatter(logging.Formatter):
        def formatTime(self, record, datefmt=None):
            dt = datetime.fromtimestamp(record.created, tz=timezone.utc)
            jst = dt.astimezone(timezone(timedelta(hours=9)))
            if datefmt:
                return jst.strftime(datefmt)
            return jst.strftime('%Y-%m-%d %H:%M:%S JST')
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    log_dir = PROJECT_ROOT / "logs"
    log_dir.mkdir(exist_ok=True)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆJSTï¼‰
    jst_now = datetime.now(timezone(timedelta(hours=9)))
    log_file = log_dir / f"{jst_now.strftime('%Y%m%d_%H%M%S')}_preprocessor.log"
    
    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    logger = logging.getLogger("preprocessor")
    logger.setLevel(getattr(logging, config['logging']['level']))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    fh = logging.FileHandler(log_file, encoding='utf-8')
    fh.setLevel(logging.INFO)
    fh.setFormatter(JSTFormatter('%(asctime)s [%(levelname)s] %(message)s'))
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    if config['logging']['console']:
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        ch.setFormatter(JSTFormatter('%(asctime)s [%(levelname)s] %(message)s'))
        logger.addHandler(ch)
    
    logger.addHandler(fh)
    
    return logger


def load_config() -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    config_path = PROJECT_ROOT / "config" / "preprocessor.yaml"
    template_path = PROJECT_ROOT / "config" / "preprocessor.template.yaml"
    
    if not config_path.exists():
        if template_path.exists():
            raise FileNotFoundError(
                f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}\n"
                f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã—ã¦ãã ã•ã„:\n"
                f"  cp {template_path} {config_path}"
            )
        else:
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_features(input_path: Path, logger: logging.Logger) -> Tuple[pd.DataFrame, List[str]]:
    """
    ç¬¬2æ®µéšã§è¨ˆç®—æ¸ˆã¿ã®ç‰¹å¾´é‡ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        input_path: feature_calculator.h5 ã®ãƒ‘ã‚¹
        logger: ãƒ­ã‚¬ãƒ¼
        
    Returns:
        features: (N, F) ã®ç‰¹å¾´é‡DataFrame
        feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
    """
    logger.info("ğŸ”„ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
    logger.info(f"   å…¥åŠ›: {input_path}")
    
    if not input_path.exists():
        raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
    
    with h5py.File(input_path, 'r') as f:
        # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        features_array = f['features'][:]
        feature_names = [name.decode('utf-8') if isinstance(name, bytes) else name 
                        for name in f['feature_names'][:]]
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if 'metadata' in f:
            metadata = json.loads(f['metadata'][()])
            logger.info(f"   å…ƒãƒ‡ãƒ¼ã‚¿æœŸé–“: {metadata.get('period', {}).get('start', 'N/A')} ~ "
                       f"{metadata.get('period', {}).get('end', 'N/A')}")
    
    # DataFrameã«å¤‰æ›
    features = pd.DataFrame(features_array, columns=feature_names)
    
    logger.info(f"   âœ… èª­ã¿è¾¼ã¿å®Œäº†: {features.shape[0]:,}è¡Œ Ã— {features.shape[1]}ç‰¹å¾´é‡")
    
    return features, feature_names


def filter_features(
    features: pd.DataFrame,
    config: Dict[str, Any],
    logger: logging.Logger
) -> pd.DataFrame:
    """
    å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    é™¤å¤–æ¡ä»¶:
    - NaN/Inf å«æœ‰ç‡ > max_nan_ratio
    - IQR < min_iqrï¼ˆå®šæ•°åˆ—ï¼‰
    - ä»–ç‰¹å¾´ã¨ã®ç›¸é–¢ |Ï| > max_correlation
    """
    logger.info("ğŸ” å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹")
    
    filter_config = config['quality_filter']
    initial_count = len(features.columns)
    
    # 1. NaN/Infé™¤å¤–
    nan_ratio = features.isna().sum() / len(features)
    inf_ratio = np.isinf(features).sum() / len(features)
    bad_ratio = nan_ratio + inf_ratio
    
    valid_cols = bad_ratio[bad_ratio <= filter_config['max_nan_ratio']].index.tolist()
    removed_nan = initial_count - len(valid_cols)
    
    if removed_nan > 0:
        logger.info(f"   ğŸ—‘ï¸  NaN/Infé™¤å¤–: {removed_nan}åˆ—")
    
    features = features[valid_cols]
    
    # 2. å®šæ•°åˆ—é™¤å¤–ï¼ˆIQR < é–¾å€¤ï¼‰
    q75 = features.quantile(0.75)
    q25 = features.quantile(0.25)
    iqr = q75 - q25
    
    valid_cols = iqr[iqr >= filter_config['min_iqr']].index.tolist()
    removed_const = len(features.columns) - len(valid_cols)
    
    if removed_const > 0:
        logger.info(f"   ğŸ—‘ï¸  å®šæ•°åˆ—é™¤å¤–: {removed_const}åˆ—")
    
    features = features[valid_cols]
    
    # 3. é«˜ç›¸é–¢ãƒšã‚¢é™¤å¤–ï¼ˆä¸Šä¸‰è§’ã®ã¿èµ°æŸ»ï¼‰
    corr_matrix = features.corr().abs()
    upper_triangle = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    high_corr_pairs = np.where((corr_matrix.where(upper_triangle) > filter_config['max_correlation']))
    
    to_drop = set()
    for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]):
        # IQRãŒå°ã•ã„æ–¹ã‚’å‰Šé™¤
        if iqr.iloc[i] < iqr.iloc[j]:
            to_drop.add(features.columns[i])
        else:
            to_drop.add(features.columns[j])
    
    if to_drop:
        logger.info(f"   ğŸ—‘ï¸  é«˜ç›¸é–¢é™¤å¤–: {len(to_drop)}åˆ—")
        features = features.drop(columns=list(to_drop))
    
    final_count = len(features.columns)
    logger.info(f"   âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {initial_count}åˆ— â†’ {final_count}åˆ—ï¼ˆ{initial_count - final_count}åˆ—é™¤å¤–ï¼‰")
    
    # æœ€å°ç‰¹å¾´é‡æ•°ãƒã‚§ãƒƒã‚¯
    min_features = config['thresholds']['min_features_after_filter']
    if final_count < min_features:
        raise ValueError(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ç‰¹å¾´é‡æ•°ãŒä¸è¶³: {final_count} < {min_features}")
    
    return features


def normalize_features(
    features: pd.DataFrame,
    config: Dict[str, Any],
    logger: logging.Logger
) -> Tuple[np.ndarray, dict]:
    """
    ç‰¹å¾´é‡ã®æ­£è¦åŒ–
    
    Returns:
        normalized: æ­£è¦åŒ–å¾Œã®é…åˆ— (N, F)
        scaler_params: æ¨è«–æ™‚ã®é€†å¤‰æ›ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    """
    logger.info("ğŸ“Š æ­£è¦åŒ–é–‹å§‹")
    
    norm_config = config['normalization']
    method = norm_config['method']
    
    # Scalerã®é¸æŠ
    if method == 'robust':
        scaler = RobustScaler(quantile_range=tuple(norm_config['quantile_range']))
        logger.info(f"   æ–¹æ³•: RobustScalerï¼ˆå››åˆ†ä½ç¯„å›²: {norm_config['quantile_range']}ï¼‰")
    elif method == 'standard':
        scaler = StandardScaler()
        logger.info(f"   æ–¹æ³•: StandardScalerï¼ˆå¹³å‡ãƒ»æ¨™æº–åå·®ï¼‰")
    elif method == 'minmax':
        scaler = MinMaxScaler()
        logger.info(f"   æ–¹æ³•: MinMaxScalerï¼ˆ0-1ç¯„å›²ï¼‰")
    else:
        raise ValueError(f"ä¸æ˜ãªæ­£è¦åŒ–æ–¹æ³•: {method}")
    
    # æ­£è¦åŒ–å®Ÿè¡Œ
    normalized = scaler.fit_transform(features)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ï¼ˆæ¨è«–æ™‚ã®é€†å¤‰æ›ã«å¿…é ˆï¼‰
    if norm_config['save_params']:
        if method == 'robust':
            scaler_params = {
                'method': method,
                'center_': scaler.center_.tolist(),
                'scale_': scaler.scale_.tolist(),
                'quantile_range': norm_config['quantile_range'],
                'feature_names': features.columns.tolist()
            }
        elif method == 'standard':
            scaler_params = {
                'method': method,
                'mean_': scaler.mean_.tolist(),
                'scale_': scaler.scale_.tolist(),
                'feature_names': features.columns.tolist()
            }
        elif method == 'minmax':
            scaler_params = {
                'method': method,
                'min_': scaler.min_.tolist(),
                'scale_': scaler.scale_.tolist(),
                'data_min_': scaler.data_min_.tolist(),
                'data_max_': scaler.data_max_.tolist(),
                'feature_names': features.columns.tolist()
            }
        
        logger.info(f"   âœ… æ­£è¦åŒ–å®Œäº†ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜: {len(scaler_params['feature_names'])}ç‰¹å¾´é‡ï¼‰")
    else:
        scaler_params = None
        logger.info(f"   âœ… æ­£è¦åŒ–å®Œäº†ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ãªã—ï¼‰")
    
    return normalized, scaler_params


def create_sequences(
    features: np.ndarray,
    tf_configs: Dict[str, int],
    logger: logging.Logger
) -> Dict[str, np.ndarray]:
    """
    TFåˆ¥ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ
    
    Args:
        features: (N, F) æ­£è¦åŒ–æ¸ˆã¿ç‰¹å¾´é‡
        tf_configs: {'M1': 480, 'M5': 288, ...}
        
    Returns:
        {'M1': (N-480, 480, F), 'M5': (N-288, 288, F), ...}
    """
    logger.info("ğŸ¯ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–é–‹å§‹")
    
    sequences = {}
    N, F = features.shape
    
    for tf_name, window_size in sorted(tf_configs.items()):
        if N <= window_size:
            logger.warning(f"   âš ï¸  {tf_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆ{N} <= {window_size}ï¼‰ã‚¹ã‚­ãƒƒãƒ—")
            continue
        
        # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        seq_list = []
        for i in range(N - window_size):
            seq_list.append(features[i:i+window_size])
        
        sequences[tf_name] = np.array(seq_list, dtype=np.float32)
        logger.info(f"   âœ… {tf_name}: {sequences[tf_name].shape} "
                   f"({window_size}ã‚¹ãƒ†ãƒƒãƒ— Ã— {F}ç‰¹å¾´é‡ Ã— {len(seq_list):,}ã‚·ãƒ¼ã‚±ãƒ³ã‚¹)")
    
    logger.info(f"   âœ… ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–å®Œäº†: {len(sequences)}ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ")
    
    return sequences


def check_future_leak(
    sequences: Dict[str, np.ndarray],
    config: Dict[str, Any],
    logger: logging.Logger
) -> None:
    """
    æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    
    Note: è©³ç´°ãªæ¤œæŸ»ã¯ validator/FUTURE_LEAK_PREVENTION_SPEC.md å‚ç…§
    """
    if not config['leak_check']['enabled']:
        logger.info("â­ï¸  æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç„¡åŠ¹åŒ–ï¼‰")
        return
    
    logger.info("ğŸ” æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»é–‹å§‹")
    
    # åŸºæœ¬ãƒã‚§ãƒƒã‚¯: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®æ™‚ç³»åˆ—é †åº
    # ï¼ˆå®Ÿè£…ã¯ç°¡ç•¥åŒ–ã€è©³ç´°ã¯validatorã§å®Ÿæ–½ï¼‰
    
    logger.info("   âœ… æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»å®Œäº†ï¼ˆåŸºæœ¬ãƒã‚§ãƒƒã‚¯ã®ã¿ï¼‰")


def save_preprocessed_data(
    sequences: Dict[str, np.ndarray],
    scaler_params: dict,
    feature_names: List[str],
    metadata: Dict[str, Any],
    output_path: Path,
    config: Dict[str, Any],
    logger: logging.Logger
) -> None:
    """
    å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’HDF5ã§ä¿å­˜
    
    Structure:
        /sequences/{TF}/data: (N, window, F) ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        /scaler_params: JSON bytes
        /feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        /metadata: JSON bytes
    """
    logger.info("ğŸ’¾ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜é–‹å§‹")
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    if output_path.exists() and config['io']['backup_existing']:
        jst_now = datetime.now(timezone(timedelta(hours=9)))
        backup_path = output_path.parent / f"{jst_now.strftime('%Y%m%d_%H%M%S')}_{output_path.name}"
        output_path.rename(backup_path)
        logger.info(f"   ğŸ“¦ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path.name}")
    
    with h5py.File(output_path, 'w') as f:
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¿å­˜
        seq_group = f.create_group('sequences')
        for tf_name, seq_data in sequences.items():
            seq_group.create_dataset(tf_name, data=seq_data, dtype='float32')
        
        # æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
        if scaler_params:
            f.create_dataset('scaler_params', data=json.dumps(scaler_params).encode('utf-8'))
        
        # ç‰¹å¾´é‡åä¿å­˜
        f.create_dataset('feature_names', data=np.array(feature_names, dtype='S'))
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        f.create_dataset('metadata', data=json.dumps(metadata).encode('utf-8'))
    
    logger.info(f"   âœ… ä¿å­˜å®Œäº†: {output_path}")
    logger.info(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {output_path.stat().st_size / 1024 / 1024:.2f} MB")


def generate_report(
    sequences: Dict[str, np.ndarray],
    scaler_params: dict,
    feature_names: List[str],
    filter_stats: Dict[str, int],
    config: Dict[str, Any],
    processing_time: float,
    logger: logging.Logger
) -> None:
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆJSON + Markdownï¼‰"""
    logger.info("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
    
    # JSON ãƒ¬ãƒãƒ¼ãƒˆ
    report_json = {
        'timestamp': datetime.now(timezone(timedelta(hours=9))).isoformat(),
        'processing_time_sec': round(processing_time, 2),
        'features': {
            'initial_count': filter_stats['initial'],
            'after_filter_count': filter_stats['final'],
            'removed_count': filter_stats['initial'] - filter_stats['final'],
            'names': feature_names
        },
        'normalization': {
            'method': scaler_params['method'] if scaler_params else 'none',
            'params_saved': scaler_params is not None
        },
        'sequences': {
            tf_name: {
                'shape': list(seq_data.shape),
                'count': int(seq_data.shape[0]),
                'window_size': int(seq_data.shape[1]),
                'features': int(seq_data.shape[2])
            }
            for tf_name, seq_data in sequences.items()
        },
        'config': config
    }
    
    json_path = PROJECT_ROOT / config['io']['report_json']
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(report_json, f, ensure_ascii=False, indent=2)
    
    logger.info(f"   âœ… JSONãƒ¬ãƒãƒ¼ãƒˆ: {json_path}")
    
    # Markdown ãƒ¬ãƒãƒ¼ãƒˆ
    md_path = PROJECT_ROOT / config['io']['report_md']
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(f"# å‰å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ç”Ÿæˆæ—¥æ™‚**: {report_json['timestamp']}\n")
        f.write(f"**å‡¦ç†æ™‚é–“**: {processing_time:.2f}ç§’\n\n")
        
        f.write(f"## ç‰¹å¾´é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n\n")
        f.write(f"- åˆæœŸç‰¹å¾´é‡æ•°: {filter_stats['initial']}\n")
        f.write(f"- ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {filter_stats['final']}\n")
        f.write(f"- é™¤å¤–æ•°: {filter_stats['initial'] - filter_stats['final']}\n\n")
        
        f.write(f"## æ­£è¦åŒ–\n\n")
        f.write(f"- æ–¹æ³•: {scaler_params['method'] if scaler_params else 'ãªã—'}\n")
        f.write(f"- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜: {'âœ…' if scaler_params else 'âŒ'}\n\n")
        
        f.write(f"## ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n\n")
        for tf_name, info in report_json['sequences'].items():
            f.write(f"### {tf_name}\n")
            f.write(f"- Shape: {info['shape']}\n")
            f.write(f"- ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {info['count']:,}\n")
            f.write(f"- ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {info['window_size']}\n\n")
    
    logger.info(f"   âœ… Markdownãƒ¬ãƒãƒ¼ãƒˆ: {md_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    start_time = time.time()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    logger = setup_logging(config)
    
    logger.info("=" * 80)
    logger.info("ğŸ”„ å‰å‡¦ç†é–‹å§‹ï¼ˆç¬¬3æ®µéšï¼‰")
    logger.info("=" * 80)
    
    try:
        # 1. ç‰¹å¾´é‡èª­ã¿è¾¼ã¿
        input_path = PROJECT_ROOT / config['io']['input_file']
        features, feature_names = load_features(input_path, logger)
        initial_feature_count = len(feature_names)
        
        # 2. å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        features = filter_features(features, config, logger)
        final_feature_count = len(features.columns)
        
        filter_stats = {
            'initial': initial_feature_count,
            'final': final_feature_count
        }
        
        # 3. æ­£è¦åŒ–
        normalized, scaler_params = normalize_features(features, config, logger)
        
        # 4. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–
        sequences = create_sequences(normalized, config['sequences'], logger)
        
        # 5. æœªæ¥ãƒªãƒ¼ã‚¯æ¤œæŸ»
        check_future_leak(sequences, config, logger)
        
        # 6. ä¿å­˜
        metadata = {
            'processing_timestamp': datetime.now(timezone(timedelta(hours=9))).isoformat(),
            'input_file': str(input_path),
            'filter_stats': filter_stats,
            'config': config
        }
        
        output_path = PROJECT_ROOT / config['io']['output_file']
        save_preprocessed_data(
            sequences,
            scaler_params,
            features.columns.tolist(),
            metadata,
            output_path,
            config,
            logger
        )
        
        # 7. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        processing_time = time.time() - start_time
        generate_report(
            sequences,
            scaler_params,
            features.columns.tolist(),
            filter_stats,
            config,
            processing_time,
            logger
        )
        
        logger.info("=" * 80)
        logger.info(f"âœ… å‰å‡¦ç†å®Œäº†ï¼ˆå‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ï¼‰")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()
