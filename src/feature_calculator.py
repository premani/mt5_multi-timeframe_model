#!/usr/bin/env python3
"""
ç‰¹å¾´é‡è¨ˆç®—ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç¬¬2æ®µéš: ç‰¹å¾´é‡è¨ˆç®—
- å…¥åŠ›: data/data_collector.h5ï¼ˆãƒãƒ«ãƒTFç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
- å‡ºåŠ›: data/feature_calculator.h5ï¼ˆ50-80ç‰¹å¾´é‡ï¼‰
- ãƒ¬ãƒãƒ¼ãƒˆ: data/feature_calculator_report.{json,md}
"""

import sys
import time
import json
import h5py
import yaml
import hashlib
import logging
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, Any, List

import pandas as pd
import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from feature_calculator import BaseCalculator, BasicMultiTFCalculator, SessionTimeCalculator, LabelGenerator
from feature_calculator.integrator import FeatureCalculatorIntegrator


def setup_logging() -> logging.Logger:
    """ãƒ­ã‚°è¨­å®šï¼ˆJSTè¡¨ç¤ºï¼‰"""
    # JSTç”¨ã®ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
    class JSTFormatter(logging.Formatter):
        def formatTime(self, record, datefmt=None):
            dt = datetime.fromtimestamp(record.created, tz=timezone.utc)
            jst = dt.astimezone(timezone(timedelta(hours=9)))
            if datefmt:
                return jst.strftime(datefmt)
            return jst.strftime('%Y-%m-%d %H:%M:%S JST')
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    log_dir = PROJECT_ROOT / "logs"
    log_dir.mkdir(exist_ok=True)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆJSTï¼‰
    jst_now = datetime.now(timezone(timedelta(hours=9)))
    log_file = log_dir / f"{jst_now.strftime('%Y%m%d_%H%M%S')}_feature_calculator.log"
    
    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    logger = logging.getLogger("feature_calculator")
    logger.setLevel(logging.INFO)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    fh = logging.FileHandler(log_file, encoding='utf-8')
    fh.setLevel(logging.INFO)
    fh.setFormatter(JSTFormatter('%(asctime)s [%(levelname)s] %(message)s'))
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(JSTFormatter('%(asctime)s [%(levelname)s] %(message)s'))
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger


from datetime import timedelta, timezone


def load_config() -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    config_path = PROJECT_ROOT / "config" / "feature_calculator.yaml"
    template_path = PROJECT_ROOT / "config" / "feature_calculator.template.yaml"
    
    if not config_path.exists():
        if template_path.exists():
            raise FileNotFoundError(
                f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}\n"
                f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã—ã¦ãã ã•ã„:\n"
                f"  cp {template_path} {config_path}"
            )
        else:
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_raw_data(logger: logging.Logger) -> Dict[str, pd.DataFrame]:
    """
    ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆç¬¬1æ®µéš: data_collector.h5ï¼‰
    
    HDF5æ§‹é€ : /M1/data, /M5/data, ..., /ticks/data, /metadata
    
    Returns:
        Dict[str, pd.DataFrame]: {TF: DataFrame} ã®è¾æ›¸
    """
    logger.info("ğŸ”„ ç¬¬1æ®µéšãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
    
    data_path = PROJECT_ROOT / "data" / "data_collector.h5"
    if not data_path.exists():
        raise FileNotFoundError(f"ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
    
    raw_data = {}
    with h5py.File(data_path, 'r') as f:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        if 'metadata' in f:
            metadata = json.loads(f['metadata'][()])
            logger.info(f"   å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {data_path.name}")
            logger.info(f"   åé›†æœŸé–“: {metadata.get('start_date')} ï½ {metadata.get('end_date')}")
            logger.info(f"   é€šè²¨ãƒšã‚¢: {metadata.get('symbol')}")
        
        # å„TFã®ãƒãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        timeframes = ['M1', 'M5', 'M15', 'H1', 'H4']
        for tf in timeframes:
            if tf in f and 'data' in f[tf]:
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
                data_array = f[tf]['data'][:]
                
                # ã‚«ãƒ©ãƒ åå®šç¾©ï¼ˆDATA_COLLECTOR_SPEC.mdã«æº–æ‹ ï¼‰
                columns = ['time', 'open', 'high', 'low', 'close', 'tick_volume', 'spread', 'real_volume']
                
                # DataFrameã«å¤‰æ›
                df = pd.DataFrame(data_array, columns=columns)
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’int64ã¨ã—ã¦å–å¾—ã—ã€datetime64ã«å¤‰æ›
                df['time'] = pd.to_datetime(df['time'].astype('int64'), unit='s', utc=True)
                
                raw_data[tf] = df
                logger.info(f"   {tf}: {len(df):,} è¡Œèª­ã¿è¾¼ã¿å®Œäº†")
            else:
                logger.warning(f"   {tf}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    
    if not raw_data:
        raise ValueError("æœ‰åŠ¹ãªã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    logger.info("âœ… ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    return raw_data


def register_calculators(config: Dict[str, Any], logger: logging.Logger) -> FeatureCalculatorIntegrator:
    """è¨ˆç®—å™¨ç™»éŒ²"""
    logger.info("ğŸ”„ è¨ˆç®—å™¨ç™»éŒ²é–‹å§‹")
    
    integrator = FeatureCalculatorIntegrator(config)
    
    # Phase 1-1: å¿…é ˆã‚«ãƒ†ã‚´ãƒª
    integrator.register_calculator(BasicMultiTFCalculator(config))
    integrator.register_calculator(SessionTimeCalculator(config))
    
    logger.info(f"   ç™»éŒ²æ¸ˆã¿è¨ˆç®—å™¨: {len(integrator.calculators)} å€‹")
    for calc in integrator.calculators:
        logger.info(f"      - {calc.name}")
    
    logger.info("âœ… è¨ˆç®—å™¨ç™»éŒ²å®Œäº†")
    return integrator


def calculate_features(
    integrator: FeatureCalculatorIntegrator,
    raw_data: Dict[str, pd.DataFrame],
    logger: logging.Logger
) -> pd.DataFrame:
    """ç‰¹å¾´é‡è¨ˆç®—å®Ÿè¡Œ"""
    logger.info("ğŸ”„ ç‰¹å¾´é‡è¨ˆç®—é–‹å§‹")
    start_time = time.time()
    
    try:
        features = integrator.calculate(raw_data)
        
        elapsed = time.time() - start_time
        logger.info(f"   è¨ˆç®—æ™‚é–“: {elapsed:.2f} ç§’")
        logger.info(f"   å‡ºåŠ›shape: {features.shape}")
        logger.info(f"   ç‰¹å¾´é‡æ•°: {len(features.columns)} åˆ—")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        category_info = integrator.get_category_info()
        logger.info("   ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
        for cat_name, info in category_info.items():
            logger.info(f"      - {cat_name}: {info['count']} åˆ—, "
                       f"{info['calculation_time_sec']:.2f}ç§’, "
                       f"NaNæ¯”ç‡={info['nan_ratio']:.4f}")
        
        logger.info("âœ… ç‰¹å¾´é‡è¨ˆç®—å®Œäº†")
        return features
    
    except Exception as e:
        logger.error(f"âŒ ç‰¹å¾´é‡è¨ˆç®—å¤±æ•—: {e}", exc_info=True)
        raise


def save_features(
    features: pd.DataFrame,
    category_info: Dict[str, Any],
    config: Dict[str, Any],
    logger: logging.Logger,
    integrator: 'FeatureCalculatorIntegrator',
    labels: Dict[str, np.ndarray] = None
) -> Path:
    """ç‰¹å¾´é‡ã‚’HDF5å½¢å¼ã§ä¿å­˜ï¼ˆãƒ©ãƒ™ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
    logger.info("ğŸ”„ ç‰¹å¾´é‡ä¿å­˜é–‹å§‹")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæ—¢å­˜æ™‚ã®ã¿ãƒªãƒãƒ¼ãƒ ï¼‰
    base_output_file = PROJECT_ROOT / "data" / "feature_calculator.h5"
    
    if base_output_file.exists():
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆæ—¥æ™‚ã‚’å–å¾—ã—ã¦ãƒªãƒãƒ¼ãƒ 
        file_mtime = base_output_file.stat().st_mtime
        file_dt = datetime.fromtimestamp(file_mtime, tz=timezone(timedelta(hours=9)))
        timestamp_str = file_dt.strftime('%Y%m%d_%H%M%S')
        backup_file = PROJECT_ROOT / "data" / f"{timestamp_str}_feature_calculator.h5"
        base_output_file.rename(backup_file)
        logger.info(f"   æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒªãƒãƒ¼ãƒ : {backup_file.name}")
    
    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¸¸ã«base_output_fileãƒ‘ã‚¹ã§ä¿å­˜
    output_file = base_output_file
    
    # ç¾åœ¨æ™‚åˆ»ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
    jst_now = datetime.now(timezone(timedelta(hours=9)))
    
    # numpyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›
    def convert_to_serializable(obj):
        """numpyå‹ã‚’JSONäº’æ›å‹ã«å¤‰æ›"""
        if isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        return obj
    
    category_info_serializable = convert_to_serializable(category_info)
    config_serializable = convert_to_serializable(config)
    
    with h5py.File(output_file, 'w') as f:
        # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        f.create_dataset('features', data=features.values, dtype='float32', compression='gzip')
        
        # ç‰¹å¾´é‡å
        feature_names = [name.encode('utf-8') for name in features.columns]
        f.create_dataset('feature_names', data=feature_names)
        
        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±
        category_info_json = json.dumps(category_info_serializable, ensure_ascii=False).encode('utf-8')
        f.create_dataset('category_info', data=category_info_json)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            'created_at': jst_now.isoformat(),
            'num_samples': int(len(features)),
            'num_features': int(len(features.columns)),
            'phase': 'feature_calculator',
            'config_hash': hashlib.sha256(json.dumps(config_serializable, sort_keys=True).encode()).hexdigest()[:8]
        }
        metadata_json = json.dumps(metadata, ensure_ascii=False).encode('utf-8')
        f.create_dataset('metadata', data=metadata_json)
        
        # ãƒ©ãƒ™ãƒ«ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if labels is not None:
            logger.info("   ğŸ“Š ãƒ©ãƒ™ãƒ«ä¿å­˜")
            labels_group = f.create_group('labels')
            labels_group.create_dataset('direction', data=labels['direction'], dtype='int64')
            labels_group.create_dataset('magnitude', data=labels['magnitude'], dtype='float32')
            logger.info(f"      Direction: {labels['direction'].shape}")
            logger.info(f"      Magnitude: {labels['magnitude'].shape}")
    
    logger.info(f"   å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file.name}")
    logger.info(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {output_file.stat().st_size / 1024 / 1024:.2f} MB")
    logger.info("âœ… ç‰¹å¾´é‡ä¿å­˜å®Œäº†")
    
    return output_file


def generate_report(
    features: pd.DataFrame,
    category_info: Dict[str, Any],
    config: Dict[str, Any],
    output_file: Path,
    integrator: FeatureCalculatorIntegrator,
    logger: logging.Logger
):
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆJSON + Markdownï¼‰"""
    logger.info("ğŸ”„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
    
    jst_now = datetime.now(timezone(timedelta(hours=9)))
    
    # numpyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›
    def convert_to_serializable(obj):
        """numpyå‹ã‚’JSONäº’æ›å‹ã«å¤‰æ›"""
        if isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        return obj
    
    category_info_serializable = convert_to_serializable(category_info)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æƒ…å ±
    category_files = {}
    for cat_name in category_info.keys():
        cat_file = integrator.category_dir / f"{cat_name}.h5"
        if cat_file.exists():
            category_files[cat_name] = str(cat_file.relative_to(PROJECT_ROOT))
    
    # JSON ãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¢å­˜æ™‚ã®ã¿ãƒªãƒãƒ¼ãƒ ï¼‰
    base_json_path = PROJECT_ROOT / "data" / "feature_calculator_report.json"
    if base_json_path.exists():
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆæ—¥æ™‚ã‚’å–å¾—ã—ã¦ãƒªãƒãƒ¼ãƒ 
        json_mtime = base_json_path.stat().st_mtime
        json_dt = datetime.fromtimestamp(json_mtime, tz=timezone(timedelta(hours=9)))
        backup_json = PROJECT_ROOT / "data" / f"{json_dt.strftime('%Y%m%d_%H%M%S')}_feature_calculator_report.json"
        base_json_path.rename(backup_json)
        logger.info(f"   æ—¢å­˜JSONãƒ¬ãƒãƒ¼ãƒˆãƒªãƒãƒ¼ãƒ : {backup_json.name}")
    
    json_path = base_json_path
    
    report = {
        'created_at': jst_now.isoformat(),
        'phase': 'feature_calculator',
        'summary': {
            'num_samples': int(len(features)),
            'num_features': int(len(features.columns)),
            'output_file': output_file.name,
            'file_size_mb': round(output_file.stat().st_size / 1024 / 1024, 2)
        },
        'category_files': category_files,
        'categories': category_info_serializable,
        'feature_names': features.columns.tolist()
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    logger.info(f"   JSONãƒ¬ãƒãƒ¼ãƒˆ: {json_path.name}")
    
    # Markdown ãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¢å­˜æ™‚ã®ã¿ãƒªãƒãƒ¼ãƒ ï¼‰
    base_md_path = PROJECT_ROOT / "data" / "feature_calculator_report.md"
    if base_md_path.exists():
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆæ—¥æ™‚ã‚’å–å¾—ã—ã¦ãƒªãƒãƒ¼ãƒ 
        md_mtime = base_md_path.stat().st_mtime
        md_dt = datetime.fromtimestamp(md_mtime, tz=timezone(timedelta(hours=9)))
        backup_md = PROJECT_ROOT / "data" / f"{md_dt.strftime('%Y%m%d_%H%M%S')}_feature_calculator_report.md"
        base_md_path.rename(backup_md)
        logger.info(f"   æ—¢å­˜MDãƒ¬ãƒãƒ¼ãƒˆãƒªãƒãƒ¼ãƒ : {backup_md.name}")
    
    md_path = base_md_path
    
    md_content = f"""# ç‰¹å¾´é‡è¨ˆç®—ãƒ¬ãƒãƒ¼ãƒˆ

## æ¦‚è¦
- **ä½œæˆæ—¥æ™‚**: {jst_now.strftime('%Y-%m-%d %H:%M:%S JST')}
- **å‡¦ç†æ®µéš**: ç¬¬2æ®µéš: ç‰¹å¾´é‡è¨ˆç®—
- **ã‚µãƒ³ãƒ—ãƒ«æ•°**: {len(features):,}
- **ç‰¹å¾´é‡æ•°**: {len(features.columns)} åˆ—
- **å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«**: `{output_file.name}`
- **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: {output_file.stat().st_size / 1024 / 1024:.2f} MB

## ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«

"""
    
    for cat_name, cat_path in category_files.items():
        md_content += f"- **{cat_name}**: `{cat_path}`\n"
    
    md_content += """
## ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ

| ã‚«ãƒ†ã‚´ãƒª | ç‰¹å¾´é‡æ•° | è¨ˆç®—æ™‚é–“ (ç§’) | NaNæ¯”ç‡ | çŠ¶æ…‹ |
|---------|---------|--------------|---------|------|
"""
    
    for cat_name, info in category_info.items():
        status = "ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥" if info.get('cached', False) else "âœ… è¨ˆç®—"
        md_content += f"| {cat_name} | {info['count']} | {info['calculation_time_sec']:.2f} | {info['nan_ratio']:.4f} | {status} |\n"
    
    md_content += f"""
## ç‰¹å¾´é‡ä¸€è¦§

åˆè¨ˆ {len(features.columns)} åˆ—:

"""
    
    for i, col in enumerate(features.columns, 1):
        md_content += f"{i}. `{col}`\n"
    
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    logger.info(f"   Markdownãƒ¬ãƒãƒ¼ãƒˆ: {md_path.name}")
    
    logger.info("âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger = setup_logging()
    logger.info("=" * 80)
    logger.info("ğŸš€ ç‰¹å¾´é‡è¨ˆç®—é–‹å§‹")
    logger.info("=" * 80)
    
    try:
        # è¨­å®šèª­ã¿è¾¼ã¿
        config = load_config()
        logger.info("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        raw_data = load_raw_data(logger)
        
        # è¨ˆç®—å™¨ç™»éŒ²
        integrator = register_calculators(config, logger)
        
        # ç‰¹å¾´é‡è¨ˆç®—
        features = calculate_features(integrator, raw_data, logger)
        
        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±å–å¾—
        category_info = integrator.get_category_info()
        
        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        labels = None
        if config.get('label_generation', {}).get('enabled', False):
            logger.info("=" * 80)
            logger.info("ğŸ·ï¸  ãƒ©ãƒ™ãƒ«ç”Ÿæˆé–‹å§‹")
            logger.info("=" * 80)
            
            label_gen_config = config['label_generation']
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            cache_dir = PROJECT_ROOT / "data" / "feature_calculator"
            cache_dir.mkdir(exist_ok=True)
            cache_file = cache_dir / "labels.h5"
            
            # å†è¨ˆç®—åˆ¤å®š
            recalculate_categories = config.get('recalculate_categories')
            should_recalculate = recalculate_categories is None or 'labels' in recalculate_categories
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            if cache_file.exists() and not should_recalculate:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨
                logger.info(f"ğŸ’¾ labels ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨")
                
                try:
                    with h5py.File(cache_file, 'r') as f:
                        labels = {
                            'direction': f['direction'][:],
                            'magnitude': f['magnitude'][:]
                        }
                        metadata = json.loads(f['metadata'][()].decode('utf-8'))
                        
                        logger.info(f"   â†’ Direction: {labels['direction'].shape}")
                        logger.info(f"   â†’ Magnitude: {labels['magnitude'].shape}")
                        logger.info(f"   â†’ ç”Ÿæˆæ—¥æ™‚: {metadata.get('created_at', 'N/A')}")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸  labels ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                    logger.warning("   â†’ å†è¨ˆç®—ã—ã¾ã™")
                    labels = None
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„ã€ã¾ãŸã¯å†è¨ˆç®—ãŒå¿…è¦ãªå ´åˆ
            if labels is None:
                # æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªãƒãƒ¼ãƒ 
                if cache_file.exists():
                    from datetime import datetime, timezone, timedelta
                    file_mtime = cache_file.stat().st_mtime
                    file_dt = datetime.fromtimestamp(file_mtime, tz=timezone(timedelta(hours=9)))
                    timestamp_str = file_dt.strftime('%Y%m%d_%H%M%S')
                    backup_file = cache_dir / f"{timestamp_str}_labels.h5"
                    cache_file.rename(backup_file)
                    logger.info(f"ğŸ’¾ labels æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªãƒãƒ¼ãƒ : {backup_file.name}")
                
                # ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
                label_generator = LabelGenerator(
                    k_spread=label_gen_config.get('k_spread', 1.0),
                    k_atr=label_gen_config.get('k_atr', 0.3),
                    spread_default=label_gen_config.get('spread_default', 1.2),
                    atr_period=label_gen_config.get('atr_period', 14),
                    pip_value=label_gen_config.get('pip_value', 0.01)
                )
                
                # ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
                collector_path = PROJECT_ROOT / "data" / "data_collector.h5"
                
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°è¨ˆç®—ï¼ˆprediction_horizonåˆ†å¼•ãï¼‰
                n_sequences = len(features) - label_gen_config.get('prediction_horizon', 36)
                if n_sequences <= 0:
                    logger.warning(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ä¸è¶³: features={len(features)}, horizon={label_gen_config.get('prediction_horizon', 36)}")
                    logger.warning("   ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                else:
                    # ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
                    logger.info(f"ğŸ§® labels è¨ˆç®—é–‹å§‹")
                    label_result = label_generator.generate_labels(
                        preprocessor_path=None,  # æœªä¿å­˜ãªã®ã§None
                        collector_path=collector_path,
                        prediction_horizon=label_gen_config.get('prediction_horizon', 36),
                        n_sequences=n_sequences
                    )
                    
                    # ãƒ©ãƒ™ãƒ«å“è³ªæ¤œè¨¼
                    label_generator.validate_labels(label_result, logger)
                    
                    # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª
                    n_valid = np.sum(label_result['valid_mask'])
                    valid_ratio = n_valid / len(label_result['valid_mask'])
                    
                    min_valid_ratio = label_gen_config.get('min_valid_samples_ratio', 0.9)
                    if valid_ratio < min_valid_ratio:
                        logger.warning(f"âš ï¸  æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«æ¯”ç‡ãŒä½ã™ãã¾ã™: {valid_ratio:.2%} < {min_valid_ratio:.0%}")
                        logger.warning("   ãƒ©ãƒ™ãƒ«ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    else:
                        # æœ‰åŠ¹ã‚µãƒ³ãƒ—ãƒ«ã®ã¿æŠ½å‡º
                        valid_mask = label_result['valid_mask']
                        labels = {
                            'direction': label_result['direction'][valid_mask],
                            'magnitude': label_result['magnitude'][valid_mask]
                        }
                        
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                        try:
                            from datetime import datetime, timezone, timedelta
                            jst_now = datetime.now(timezone(timedelta(hours=9)))
                            
                            with h5py.File(cache_file, 'w') as f:
                                f.create_dataset('direction', data=labels['direction'], dtype='int64')
                                f.create_dataset('magnitude', data=labels['magnitude'], dtype='float32')
                                
                                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                                metadata = {
                                    'created_at': jst_now.isoformat(),
                                    'n_samples': len(labels['direction']),
                                    'prediction_horizon': label_gen_config.get('prediction_horizon', 36),
                                    'k_spread': label_gen_config.get('k_spread', 1.0),
                                    'k_atr': label_gen_config.get('k_atr', 0.3)
                                }
                                metadata_json = json.dumps(metadata, ensure_ascii=False).encode('utf-8')
                                f.create_dataset('metadata', data=metadata_json)
                            
                            logger.info(f"   ğŸ’¾ ä¿å­˜: labels.h5")
                        except Exception as e:
                            logger.warning(f"âš ï¸  labels ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å¤±æ•—: {e}")
                        
                        # ç‰¹å¾´é‡ã‚‚åŒæ§˜ã«ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæœ€åˆã®n_sequencesè¡Œã‚’ä½¿ç”¨ï¼‰
                        features = features.iloc[:n_sequences][valid_mask].reset_index(drop=True)
                        
                        logger.info(f"   â†’ {len(labels['direction'])}åˆ—ç”Ÿæˆ")
                        logger.info(f"âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†: {len(labels['direction'])} ã‚µãƒ³ãƒ—ãƒ«")
        
        # ç‰¹å¾´é‡ä¿å­˜ï¼ˆãƒ©ãƒ™ãƒ«å«ã‚€ï¼‰
        output_file = save_features(features, category_info, config, logger, integrator, labels)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        generate_report(features, category_info, config, output_file, integrator, logger)
        
        logger.info("=" * 80)
        logger.info("âœ… ç‰¹å¾´é‡è¨ˆç®—ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"âŒ ç‰¹å¾´é‡è¨ˆç®—ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}")
        logger.error("=" * 80)
        raise


if __name__ == "__main__":
    main()
