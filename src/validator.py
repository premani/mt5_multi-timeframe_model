#!/usr/bin/env python3
"""
æ¤œè¨¼å‡¦ç†ï¼ˆç¬¬5æ®µéšï¼‰

ç›®çš„:
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡

å…¥åŠ›:
    - preprocessed.h5: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆï¼‰
    - training.pt: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«

å‡ºåŠ›:
    - validation_report.json: è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆ
    - confusion_matrix.png: æ··åŒè¡Œåˆ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

å®Ÿè¡Œ:
    bash ./docker_run.sh python3 src/validator.py
"""

import sys
from pathlib import Path
import yaml
import h5py
import torch
import torch.nn as nn
import numpy as np
import json
from datetime import datetime
from typing import Dict, Tuple, Any
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    mean_absolute_error, mean_squared_error, r2_score,
    confusion_matrix, classification_report
)

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logging_manager import LoggingManager

# trainer.pyã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å®šç¾©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import importlib.util
trainer_spec = importlib.util.spec_from_file_location("trainer_module", Path(__file__).parent / "trainer.py")
trainer_module = importlib.util.module_from_spec(trainer_spec)
trainer_spec.loader.exec_module(trainer_module)
MultiTFModel = trainer_module.MultiTFModel
TFEncoder = trainer_module.TFEncoder


class Validator:
    """æ¤œè¨¼å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
        self.logger = LoggingManager(
            name="validator",
            log_dir="logs",
            level=config.get("logging", {}).get("level", "INFO")
        )
        
        self.device = torch.device(config['batch']['device'] if torch.cuda.is_available() else 'cpu')
        
        self.logger.info(f"ğŸ¯ æ¤œè¨¼å‡¦ç†é–‹å§‹")
        self.logger.info(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
    
    def load_data(self) -> Tuple[Dict[str, torch.Tensor], torch.Tensor, torch.Tensor]:
        """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œè¨¼ç”¨ã‚’æŠ½å‡ºï¼‰"""
        input_path = Path(self.config['input']['preprocessed_file'])
        
        self.logger.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {input_path.name}")
        
        sequences = {}
        with h5py.File(input_path, 'r') as f:
            # å…¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹èª­ã¿è¾¼ã¿ï¼ˆãƒãƒ«ãƒTFï¼‰
            for tf in f['sequences'].keys():
                sequences[tf] = f[f'sequences/{tf}'][:]
                self.logger.info(f"   {tf}: {sequences[tf].shape}")
            
            # ãƒ©ãƒ™ãƒ«èª­ã¿è¾¼ã¿
            all_direction = f['labels/direction'][:]
            all_magnitude = f['labels/magnitude'][:]
            
            self.logger.info(f"   Direction: {all_direction.shape}")
            self.logger.info(f"   Magnitude: {all_magnitude.shape}")
        
        # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆå…¨TFã§å…±é€šã®é•·ã•ï¼‰
        min_samples = min(len(all_direction), len(all_magnitude), 
                         min(seq.shape[0] for seq in sequences.values()))
        
        # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå¾ŒåŠ20%ã‚’ä½¿ç”¨ï¼‰
        test_size = int(min_samples * 0.2)
        test_start = min_samples - test_size
        
        self.logger.info(f"   ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå¾Œ: {min_samples} ã‚µãƒ³ãƒ—ãƒ«")
        self.logger.info(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {test_start}ã€œ{min_samples} ({test_size} ã‚µãƒ³ãƒ—ãƒ«)")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        test_sequences = {
            tf: torch.from_numpy(seq[test_start:min_samples]).float()
            for tf, seq in sequences.items()
        }
        test_direction = torch.from_numpy(all_direction[test_start:min_samples]).long()
        test_magnitude = torch.from_numpy(all_magnitude[test_start:min_samples]).float()
        
        return test_sequences, test_direction, test_magnitude
    
    def load_model(self) -> MultiTFModel:
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        model_path = Path(self.config['input']['model_file'])
        
        self.logger.info(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path.name}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ï¼ˆPyTorch 2.8å¯¾å¿œï¼‰
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆè¨­å®šã‹ã‚‰å¾©å…ƒï¼‰
        model_config = checkpoint.get('config', self.config)
        model = MultiTFModel(model_config)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’å‹•çš„è¿½åŠ ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ§‹é€ ã‚’å¾©å…ƒï¼‰
        state_dict = checkpoint['model_state_dict']
        for key in state_dict.keys():
            if key.startswith('encoders.'):
                tf_name = key.split('.')[1]
                # æœ€åˆã®é‡ã¿ã‹ã‚‰input_sizeã‚’æ¨å®š
                weight_key = f'encoders.{tf_name}.lstm.weight_ih_l0'
                if weight_key in state_dict:
                    input_size = state_dict[weight_key].shape[1]
                    if tf_name not in model.encoders:
                        model.add_encoder(tf_name, input_size)
        
        # é‡ã¿èª­ã¿è¾¼ã¿
        model.load_state_dict(state_dict)
        model.to(self.device)
        model.eval()
        
        self.logger.info(f"   ã‚¨ãƒãƒƒã‚¯: {checkpoint['epoch']}")
        self.logger.info(f"   å­¦ç¿’ç²¾åº¦: {checkpoint.get('train_accuracy', 'N/A')}")
        
        return model
    
    def predict(
        self,
        model: MultiTFModel,
        sequences: Dict[str, torch.Tensor],
        batch_size: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """æ¨è«–å®Ÿè¡Œ"""
        self.logger.info(f"ğŸ”„ æ¨è«–å®Ÿè¡Œä¸­...")
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°å–å¾—ï¼ˆå…¨TFå…±é€šï¼‰
        n_samples = len(next(iter(sequences.values())))
        
        all_direction_preds = []
        all_magnitude_preds = []
        
        with torch.no_grad():
            for i in range(0, n_samples, batch_size):
                # ãƒãƒƒãƒä½œæˆ
                batch = {
                    tf: seq[i:i+batch_size].to(self.device)
                    for tf, seq in sequences.items()
                }
                
                # æ¨è«–
                output = model(batch)
                direction_logits = output["direction"]
                magnitude_pred = output["magnitude"]
                
                # Direction: argmax
                direction_pred = torch.argmax(direction_logits, dim=1).cpu().numpy()
                all_direction_preds.append(direction_pred)
                
                # Magnitude
                magnitude_pred = magnitude_pred.cpu().numpy()
                all_magnitude_preds.append(magnitude_pred)
        
        # çµåˆ
        direction_preds = np.concatenate(all_direction_preds)
        magnitude_preds = np.concatenate(all_magnitude_preds)
        
        self.logger.info(f"   æ¨è«–å®Œäº†: {len(direction_preds)} ã‚µãƒ³ãƒ—ãƒ«")
        
        return direction_preds, magnitude_preds
    
    def evaluate_direction(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray
    ) -> Dict[str, Any]:
        """æ–¹å‘äºˆæ¸¬è©•ä¾¡"""
        self.logger.info(f"ğŸ“Š æ–¹å‘äºˆæ¸¬è©•ä¾¡")
        
        # å…¨ä½“ç²¾åº¦
        accuracy = accuracy_score(y_true, y_pred)
        self.logger.info(f"   Accuracy: {accuracy:.4f}")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥æŒ‡æ¨™
        precision = precision_score(y_true, y_pred, average=None, zero_division=0)
        recall = recall_score(y_true, y_pred, average=None, zero_division=0)
        f1 = f1_score(y_true, y_pred, average=None, zero_division=0)
        
        class_names = ['DOWN', 'NEUTRAL', 'UP']
        for i, name in enumerate(class_names):
            self.logger.info(f"   {name:8s}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}")
        
        # æ··åŒè¡Œåˆ—
        cm = confusion_matrix(y_true, y_pred)
        self.logger.info(f"   æ··åŒè¡Œåˆ—:\n{cm}")
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)
        
        return {
            'accuracy': float(accuracy),
            'precision': precision.tolist(),
            'recall': recall.tolist(),
            'f1_score': f1.tolist(),
            'confusion_matrix': cm.tolist(),
            'classification_report': report
        }
    
    def evaluate_magnitude(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray
    ) -> Dict[str, float]:
        """ä¾¡æ ¼å¹…äºˆæ¸¬è©•ä¾¡"""
        self.logger.info(f"ğŸ“Š ä¾¡æ ¼å¹…äºˆæ¸¬è©•ä¾¡")
        
        # MAE
        mae = mean_absolute_error(y_true, y_pred)
        self.logger.info(f"   MAE: {mae:.4f} pips")
        
        # RMSE
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        self.logger.info(f"   RMSE: {rmse:.4f} pips")
        
        # RÂ²
        r2 = r2_score(y_true, y_pred)
        self.logger.info(f"   RÂ²: {r2:.4f}")
        
        return {
            'mae': float(mae),
            'rmse': float(rmse),
            'r2': float(r2)
        }
    
    def save_report(self, report: Dict[str, Any], output_dir: Path):
        """ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ï¼ˆJSON + Markdownï¼‰
        
        å‘½åè¦ç´„:
        - åŸºæœ¬: validator_report.json / validator_report.md
        - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ validator_YYYYMMDD_HHMMSS_report.* ã«ãƒªãƒãƒ¼ãƒ 
        """
        import os
        
        # åŸºæœ¬ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã—ï¼‰
        json_path = output_dir / "validator_report.json"
        md_path = output_dir / "validator_report.md"
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        for path in [json_path, md_path]:
            if path.exists():
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´æ™‚åˆ»ã‚’å–å¾—
                mtime = os.path.getmtime(path)
                timestamp = datetime.fromtimestamp(mtime).strftime("%Y%m%d_%H%M%S")
                
                # ãƒªãƒãƒ¼ãƒ å…ˆ
                backup_name = f"validator_{timestamp}_report{path.suffix}"
                backup_path = path.parent / backup_name
                
                # ãƒªãƒãƒ¼ãƒ 
                path.rename(backup_path)
                self.logger.info(f"ğŸ“¦ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_name}")
        
        # JSONãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        self.logger.info(f"ğŸ’¾ JSONãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {json_path.name}")
        
        # Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        self._save_markdown_report(report, md_path)
        self.logger.info(f"ğŸ’¾ Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {md_path.name}")
    
    def _save_markdown_report(self, report: Dict[str, Any], md_path: Path):
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        lines = [
            "# æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ",
            "",
            f"**æ¤œè¨¼æ—¥æ™‚**: {report['timestamp']}",
            f"**ãƒ¢ãƒ‡ãƒ«**: {Path(report['model_file']).name}",
            f"**ãƒ‡ãƒ¼ã‚¿**: {Path(report['preprocessed_file']).name}",
            f"**ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°**: {report['test_samples']:,}",
            "",
            "---",
            "",
            "## ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ",
            "",
            "| ã‚¯ãƒ©ã‚¹ | ã‚µãƒ³ãƒ—ãƒ«æ•° | å‰²åˆ |",
            "|--------|-----------|------|"
        ]
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        class_names = ['DOWN', 'NEUTRAL', 'UP']
        for name in class_names:
            key = name.lower()
            count = report['class_distribution'][key]['count']
            ratio = report['class_distribution'][key]['ratio']
            lines.append(f"| {name:8s} | {count:6,d} | {ratio:6.2%} |")
        
        lines.extend([
            "",
            "---",
            "",
            "## ğŸ¯ æ–¹å‘äºˆæ¸¬è©•ä¾¡",
            "",
            f"**Accuracy**: {report['direction_metrics']['accuracy']:.4f}",
            "",
            "### ã‚¯ãƒ©ã‚¹åˆ¥æŒ‡æ¨™",
            "",
            "| ã‚¯ãƒ©ã‚¹ | Precision | Recall | F1-Score |",
            "|--------|-----------|--------|----------|"
        ])
        
        for i, name in enumerate(class_names):
            precision = report['direction_metrics']['precision'][i]
            recall = report['direction_metrics']['recall'][i]
            f1 = report['direction_metrics']['f1_score'][i]
            lines.append(f"| {name:8s} | {precision:.4f} | {recall:.4f} | {f1:.4f} |")
        
        lines.extend([
            "",
            "### æ··åŒè¡Œåˆ—",
            "",
            "|         | DOWN | NEUTRAL | UP   |",
            "|---------|------|---------|------|"
        ])
        
        cm = report['direction_metrics']['confusion_matrix']
        for i, name in enumerate(class_names):
            lines.append(f"| {name:7s} | {cm[i][0]:4d} | {cm[i][1]:7d} | {cm[i][2]:4d} |")
        
        lines.extend([
            "",
            "---",
            "",
            "## ğŸ” äºˆæ¸¬ä¿¡é ¼åº¦",
            "",
            f"- **å¹³å‡ä¿¡é ¼åº¦**: {report['confidence_stats']['mean']:.4f}",
            f"- **ä¸­å¤®å€¤**: {report['confidence_stats']['median']:.4f}",
            f"- **æ¨™æº–åå·®**: {report['confidence_stats']['std']:.4f}",
            f"- **ç¯„å›²**: [{report['confidence_stats']['min']:.4f}, {report['confidence_stats']['max']:.4f}]",
            f"- **å››åˆ†ä½ç¯„å›²**: [{report['confidence_stats']['q25']:.4f}, {report['confidence_stats']['q75']:.4f}]",
            "",
            "---",
            "",
            "## ğŸ“Š ä¾¡æ ¼å¹…äºˆæ¸¬è©•ä¾¡",
            "",
            f"### èª¤å·®æŒ‡æ¨™",
            "",
            f"- **MAE**: {report['magnitude_metrics']['mae']:.4f} pips",
            f"- **RMSE**: {report['magnitude_metrics']['rmse']:.4f} pips",
            f"- **RÂ²**: {report['magnitude_metrics']['r2']:.4f}",
            "",
            "### å®Ÿéš›å€¤ã®åˆ†å¸ƒ",
            "",
            f"- **å¹³å‡**: {report['magnitude_distribution']['true']['mean']:.4f} pips",
            f"- **ä¸­å¤®å€¤**: {report['magnitude_distribution']['true']['median']:.4f} pips",
            f"- **æ¨™æº–åå·®**: {report['magnitude_distribution']['true']['std']:.4f} pips",
            f"- **ç¯„å›²**: [{report['magnitude_distribution']['true']['min']:.4f}, {report['magnitude_distribution']['true']['max']:.4f}] pips",
            "",
            "### äºˆæ¸¬å€¤ã®åˆ†å¸ƒ",
            "",
            f"- **å¹³å‡**: {report['magnitude_distribution']['pred']['mean']:.4f} pips",
            f"- **ä¸­å¤®å€¤**: {report['magnitude_distribution']['pred']['median']:.4f} pips",
            f"- **æ¨™æº–åå·®**: {report['magnitude_distribution']['pred']['std']:.4f} pips",
            f"- **ç¯„å›²**: [{report['magnitude_distribution']['pred']['min']:.4f}, {report['magnitude_distribution']['pred']['max']:.4f}] pips",
            ""
        ])
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
    
    def analyze_class_distribution(self, y_true: np.ndarray) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹åˆ†å¸ƒåˆ†æ"""
        self.logger.info(f"ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒåˆ†æ")
        
        class_names = ['DOWN', 'NEUTRAL', 'UP']
        total = len(y_true)
        distribution = {}
        
        for i, name in enumerate(class_names):
            count = np.sum(y_true == i)
            ratio = count / total
            distribution[name.lower()] = {
                'count': int(count),
                'ratio': float(ratio)
            }
            self.logger.info(f"   {name:8s}: {count:5d} ({ratio:6.2%})")
        
        return distribution
    
    def analyze_prediction_confidence(
        self,
        model: MultiTFModel,
        sequences: Dict[str, torch.Tensor],
        batch_size: int
    ) -> Dict[str, Any]:
        """äºˆæ¸¬ä¿¡é ¼åº¦åˆ†æ"""
        self.logger.info(f"ğŸ” äºˆæ¸¬ä¿¡é ¼åº¦åˆ†æ")
        
        n_samples = len(next(iter(sequences.values())))
        all_probs = []
        
        with torch.no_grad():
            for i in range(0, n_samples, batch_size):
                batch = {
                    tf: seq[i:i+batch_size].to(self.device)
                    for tf, seq in sequences.items()
                }
                
                output = model(batch)
                direction_logits = output["direction"]
                probs = torch.softmax(direction_logits, dim=1)
                all_probs.append(probs.cpu().numpy())
        
        all_probs = np.concatenate(all_probs)
        max_probs = np.max(all_probs, axis=1)
        
        confidence_stats = {
            'mean': float(np.mean(max_probs)),
            'median': float(np.median(max_probs)),
            'std': float(np.std(max_probs)),
            'min': float(np.min(max_probs)),
            'max': float(np.max(max_probs)),
            'q25': float(np.percentile(max_probs, 25)),
            'q75': float(np.percentile(max_probs, 75))
        }
        
        self.logger.info(f"   å¹³å‡ä¿¡é ¼åº¦: {confidence_stats['mean']:.4f}")
        self.logger.info(f"   ä¸­å¤®å€¤: {confidence_stats['median']:.4f}")
        self.logger.info(f"   æ¨™æº–åå·®: {confidence_stats['std']:.4f}")
        
        return confidence_stats
    
    def analyze_magnitude_distribution(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray
    ) -> Dict[str, Any]:
        """ä¾¡æ ¼å¹…åˆ†å¸ƒåˆ†æ"""
        self.logger.info(f"ğŸ“Š ä¾¡æ ¼å¹…åˆ†å¸ƒåˆ†æ")
        
        true_stats = {
            'mean': float(np.mean(y_true)),
            'median': float(np.median(y_true)),
            'std': float(np.std(y_true)),
            'min': float(np.min(y_true)),
            'max': float(np.max(y_true)),
            'q25': float(np.percentile(y_true, 25)),
            'q75': float(np.percentile(y_true, 75))
        }
        
        pred_stats = {
            'mean': float(np.mean(y_pred)),
            'median': float(np.median(y_pred)),
            'std': float(np.std(y_pred)),
            'min': float(np.min(y_pred)),
            'max': float(np.max(y_pred)),
            'q25': float(np.percentile(y_pred, 25)),
            'q75': float(np.percentile(y_pred, 75))
        }
        
        self.logger.info(f"   å®Ÿéš›å€¤ - å¹³å‡: {true_stats['mean']:.4f} pips, ç¯„å›²: [{true_stats['min']:.4f}, {true_stats['max']:.4f}]")
        self.logger.info(f"   äºˆæ¸¬å€¤ - å¹³å‡: {pred_stats['mean']:.4f} pips, ç¯„å›²: [{pred_stats['min']:.4f}, {pred_stats['max']:.4f}]")
        
        return {
            'true': true_stats,
            'pred': pred_stats
        }
    
    def run(self):
        """æ¤œè¨¼å®Ÿè¡Œ"""
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            test_sequences, test_direction, test_magnitude = self.load_data()
            
            # ã‚µãƒ³ãƒ—ãƒ«æ•°å–å¾—ï¼ˆãƒãƒ«ãƒTFå¯¾å¿œï¼‰
            n_samples = len(test_direction)
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model = self.load_model()
            
            # æ¨è«–
            batch_size = self.config['batch']['size']
            direction_preds, magnitude_preds = self.predict(model, test_sequences, batch_size)
            
            # è©•ä¾¡
            direction_metrics = self.evaluate_direction(
                test_direction.numpy(),
                direction_preds
            )
            
            magnitude_metrics = self.evaluate_magnitude(
                test_magnitude.numpy(),
                magnitude_preds
            )
            
            # è¿½åŠ åˆ†æ
            class_distribution = self.analyze_class_distribution(test_direction.numpy())
            confidence_stats = self.analyze_prediction_confidence(model, test_sequences, batch_size)
            magnitude_distribution = self.analyze_magnitude_distribution(
                test_magnitude.numpy(),
                magnitude_preds
            )
            
            # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            report = {
                'timestamp': datetime.now().isoformat(),
                'model_file': self.config['input']['model_file'],
                'preprocessed_file': self.config['input']['preprocessed_file'],
                'test_samples': n_samples,
                'class_distribution': class_distribution,
                'direction_metrics': direction_metrics,
                'magnitude_metrics': magnitude_metrics,
                'confidence_stats': confidence_stats,
                'magnitude_distribution': magnitude_distribution
            }
            
            # ä¿å­˜
            output_dir = Path(self.config['output']['report_dir'])
            output_dir.mkdir(parents=True, exist_ok=True)
            self.save_report(report, output_dir)
            
            self.logger.info(f"âœ… æ¤œè¨¼å®Œäº†")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¤œè¨¼å¤±æ•—: {e}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # è¨­å®šèª­ã¿è¾¼ã¿
    config_path = Path("config/validator.yaml")
    if not config_path.exists():
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        print(f"   ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„:")
        print(f"   cp config/validator.template.yaml config/validator.yaml")
        sys.exit(1)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æ¤œè¨¼å®Ÿè¡Œ
    validator = Validator(config)
    validator.run()


if __name__ == "__main__":
    main()
